# -*- coding: utf-8 -*-
"""å›¾è¡¨æ•°æ®API - å‰åç«¯åˆ†ç¦»æ¶æ„"""

from fastapi import APIRouter, Depends, HTTPException, Query
from typing import Dict, Any, List
from datetime import datetime
import json
import pandas as pd
import hashlib

from app.core.sync_redis_client import get_sync_redis_client
from app.api.dependencies import verify_token
from app.core.logging import logger
from app.trading.strategies import apply_strategy

router = APIRouter(tags=["å›¾è¡¨æ•°æ®"])

# Redisç¼“å­˜é…ç½®
CACHE_TTL_SECONDS = 60  # 1åˆ†é’Ÿç¼“å­˜ï¼ˆé€‚åˆå®æ—¶æ•°æ®ï¼‰
CACHE_PREFIX = "chart_data"


def _generate_cache_key(stock_code: str, strategy: str) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    return f"{CACHE_PREFIX}:{stock_code}:{strategy}"


def _serialize_dataframe(df: pd.DataFrame) -> List[Dict]:
    """å°†DataFrameåºåˆ—åŒ–ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
    # è½¬æ¢æ—¥æœŸä¸ºå­—ç¬¦ä¸²
    df_copy = df.copy()
    if 'date' in df_copy.columns:
        df_copy['date'] = df_copy['date'].astype(str)
    
    # æ›¿æ¢NaNä¸ºNone
    return df_copy.where(pd.notnull(df_copy), None).to_dict('records')


@router.get(
    "/api/stocks/{stock_code}/chart-data",
    summary="è·å–å›¾è¡¨æ•°æ®ï¼ˆçº¯JSONï¼‰",
    dependencies=[Depends(verify_token)]
)
async def get_chart_data(
    stock_code: str,
    strategy: str = Query("volume_wave", description="ç­–ç•¥ç±»å‹"),
    force_refresh: bool = Query(False, description="å¼ºåˆ¶åˆ·æ–°ç¼“å­˜")
) -> Dict[str, Any]:
    """
    è·å–è‚¡ç¥¨å›¾è¡¨æ•°æ®ï¼ˆKçº¿+æŒ‡æ ‡+ä¿¡å·ï¼‰
    
    ä¼˜åŠ¿ï¼š
    - å‰åç«¯åˆ†ç¦»ï¼Œ1ä¸ªHTMLæ¨¡æ¿æœåŠ¡æ‰€æœ‰è‚¡ç¥¨
    - Redisç¼“å­˜æŒ‡æ ‡è®¡ç®—ç»“æœï¼Œ1åˆ†é’ŸTTL
    - æ”¯æŒ1000+å¹¶å‘ï¼Œæ— HTMLæ–‡ä»¶å †ç§¯
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç 
        strategy: ç­–ç•¥ç±»å‹
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
        
    Returns:
        {
            "stock": {"code": "000001", "name": "å¹³å®‰é“¶è¡Œ"},
            "kline_data": [...],  # Kçº¿æ•°æ®
            "indicators": {...},  # æŒ‡æ ‡æ•°æ®
            "signals": [...],     # ä¹°å–ä¿¡å·
            "strategy": "volume_wave",
            "cached": true,       # æ˜¯å¦æ¥è‡ªç¼“å­˜
            "generated_time": "2025-12-24T10:30:00"
        }
    """
    if strategy not in ["volume_wave", "volume_wave_enhanced", "volatility_conservation"]:
        raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„ç­–ç•¥: {strategy}")
    
    try:
        redis_client = get_sync_redis_client()
        cache_key = _generate_cache_key(stock_code, strategy)
        
        # 1. å°è¯•ä»ç¼“å­˜è·å–
        if not force_refresh:
            cached_data = redis_client.get(cache_key)
            if cached_data:
                logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®: {stock_code} ({strategy})")
                result = json.loads(cached_data)
                result['cached'] = True
                return result
        
        # 2. è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
        stock_codes_key = "stocks:codes:all"
        stock_codes_data = redis_client.get(stock_codes_key)
        
        if not stock_codes_data:
            raise HTTPException(status_code=500, detail="è‚¡ç¥¨ä»£ç æ•°æ®ä¸å¯ç”¨")
        
        stock_codes = json.loads(stock_codes_data)
        stock_info = None
        ts_code = None
        
        # æŸ¥æ‰¾è‚¡ç¥¨ä¿¡æ¯
        for stock in stock_codes:
            if (stock.get('ts_code') == stock_code or 
                stock.get('symbol') == stock_code or 
                stock.get('ts_code', '').split('.')[0] == stock_code):
                stock_info = stock
                ts_code = stock.get('ts_code')
                break
        
        if not stock_info or not ts_code:
            raise HTTPException(status_code=404, detail=f"è‚¡ç¥¨ {stock_code} ä¸å­˜åœ¨")
        
        # 3. è·å–Kçº¿æ•°æ®ï¼ˆå¸¦è‡ªåŠ¨è¡¥å¿ï¼‰
        kline_key = f"stock_trend:{ts_code}"
        kline_data = redis_client.get(kline_key)
        
        if not kline_data:
            # ğŸ”§ è‡ªåŠ¨æ•°æ®è¡¥å¿æœºåˆ¶ï¼šå¦‚æœRedisä¸­æ²¡æœ‰æ•°æ®ï¼Œç«‹å³ä»Tushareè·å–
            logger.warning(f"Redisä¸­æ²¡æœ‰è‚¡ç¥¨ {stock_code} çš„å†å²æ•°æ®")
            logger.info(f"ğŸš€ å¯åŠ¨è‡ªåŠ¨æ•°æ®è¡¥å¿ï¼šä»Tushareè·å–è‚¡ç¥¨ {ts_code} çš„å†å²æ•°æ®...")
            
            try:
                from app.services.stock.unified_data_service import UnifiedDataService
                import tushare as ts
                from datetime import datetime
                
                unified_service = UnifiedDataService()
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºETF
                is_etf = stock_info.get('market', '') == 'ETF' or ts_code.startswith(('51', '15', '16', '56'))
                
                # è·å–180å¤©æ•°æ®
                logger.info(f"æ­£åœ¨è·å– {ts_code} çš„180å¤©Kçº¿æ•°æ®ï¼ˆ{'ETF' if is_etf else 'è‚¡ç¥¨'}ï¼‰...")
                kline_list = unified_service.fetch_historical_data(
                    ts_code=ts_code,
                    days=180,
                    is_etf=is_etf
                )
                
                if not kline_list or len(kline_list) < 20:
                    logger.error(f"âŒ ä»Tushareè·å–çš„æ•°æ®ä¸è¶³: {len(kline_list) if kline_list else 0} æ¡")
                    raise HTTPException(
                        status_code=404,
                        detail=f"è‚¡ç¥¨ {stock_code} å†å²æ•°æ®ä¸è¶³ï¼ˆè·å–åˆ°{len(kline_list) if kline_list else 0}æ¡ï¼‰"
                    )
                
                # å­˜å‚¨åˆ°Redis
                trend_data_to_store = {
                    'ts_code': ts_code,
                    'data': kline_list,
                    'updated_at': datetime.now().isoformat(),
                    'data_count': len(kline_list),
                    'source': 'tushare_è¡¥å¿'
                }
                
                redis_client.set(kline_key, json.dumps(trend_data_to_store, default=str))
                logger.info(f"âœ… æ•°æ®è¡¥å¿æˆåŠŸ: {ts_code}ï¼Œå·²å­˜å‚¨ {len(kline_list)} æ¡Kçº¿æ•°æ®")
                
                # é‡æ–°è¯»å–æ•°æ®
                kline_data = redis_client.get(kline_key)
                
            except Exception as e:
                logger.error(f"âŒ è‡ªåŠ¨æ•°æ®è¡¥å¿å¤±è´¥: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
                raise HTTPException(
                    status_code=404,
                    detail=f"è‚¡ç¥¨ {stock_code} æ— æ³•è·å–å†å²æ•°æ®ã€‚é”™è¯¯ï¼š{str(e)}"
                )
        
        # 4. è§£æå¹¶å¤„ç†æ•°æ®
        trend_data = json.loads(kline_data)
        if isinstance(trend_data, dict):
            kline_json = trend_data.get('data', [])
        elif isinstance(trend_data, list):
            kline_json = trend_data
        else:
            raise HTTPException(status_code=400, detail="æ•°æ®æ ¼å¼é”™è¯¯")
        
        if len(kline_json) < 20:
            raise HTTPException(status_code=400, detail="å†å²æ•°æ®ä¸è¶³")
        
        # 5. è½¬æ¢ä¸ºDataFrameå¹¶æ ‡å‡†åŒ–å­—æ®µ
        df = pd.DataFrame(kline_json)
        
        # å¤„ç†æ—¥æœŸå­—æ®µ
        if 'date' not in df.columns:
            if 'trade_date' in df.columns:
                def convert_tushare_date(date_str):
                    date_str = str(date_str)
                    if len(date_str) == 8 and date_str.isdigit():
                        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
                    return date_str
                df['date'] = pd.to_datetime(df['trade_date'].apply(convert_tushare_date))
            elif 'actual_trade_date' in df.columns:
                df['date'] = pd.to_datetime(df['actual_trade_date'])
            else:
                df['date'] = pd.date_range(start='2024-01-01', periods=len(df), freq='D')
        else:
            df['date'] = pd.to_datetime(df['date'])
        
        # å¤„ç†æˆäº¤é‡å­—æ®µ
        if 'volume' not in df.columns and 'vol' in df.columns:
            df['volume'] = df['vol'].fillna(0) * 100
        elif 'volume' not in df.columns:
            df['volume'] = 1000
        
        df['volume'] = df['volume'].fillna(1000)
        df['volume'] = df['volume'].apply(lambda x: max(x, 1) if x != 0 else 1000)
        
        # éªŒè¯å¿…è¦åˆ—
        required_columns = ['close', 'open', 'high', 'low', 'volume']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise HTTPException(status_code=400, detail=f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
        
        # 6. åº”ç”¨ç­–ç•¥ï¼ˆè€—æ—¶æ“ä½œï¼‰
        logger.info(f"ğŸ”„ è®¡ç®—æŒ‡æ ‡: {stock_code} ({strategy})")
        processed_df, signals = apply_strategy(strategy, df)
        
        # 7. ä¸ºvolume_waveç­–ç•¥æ·»åŠ é¢å¤–æŒ‡æ ‡
        if strategy == 'volume_wave':
            try:
                close_values = processed_df['close'].to_numpy()
                from app.strategies.volume_wave_strategy import VolumeWaveStrategy
                
                if 'ema12' not in processed_df.columns:
                    processed_df['ema12'] = pd.Series(VolumeWaveStrategy.calculate_ema(close_values, 12))
                if 'ema144' not in processed_df.columns:
                    processed_df['ema144'] = pd.Series(VolumeWaveStrategy.calculate_ema(close_values, 144))
                if 'ema169' not in processed_df.columns:
                    processed_df['ema169'] = pd.Series(VolumeWaveStrategy.calculate_ema(close_values, 169))
            except Exception as e:
                logger.warning(f"æ·»åŠ EMAæŒ‡æ ‡å¤±è´¥: {e}")
        
        # 8. æ„å»ºè¿”å›æ•°æ®
        result = {
            "stock": {
                "code": stock_code,
                "name": stock_info.get('name', stock_code)
            },
            "kline_data": _serialize_dataframe(processed_df),
            "signals": [
                {
                    "date": str(sig['date']),
                    "type": sig['type'],
                    "price": float(sig['price']),
                    "reason": sig.get('reason', '')
                }
                for sig in signals
            ],
            "strategy": strategy,
            "cached": False,
            "generated_time": datetime.now().isoformat()
        }
        
        # 9. ç¼“å­˜ç»“æœï¼ˆ1åˆ†é’Ÿï¼‰
        try:
            redis_client.setex(
                cache_key,
                CACHE_TTL_SECONDS,
                json.dumps(result, ensure_ascii=False)
            )
            logger.info(f"ğŸ’¾ ç¼“å­˜æ•°æ®: {stock_code} ({strategy}), TTL={CACHE_TTL_SECONDS}s")
        except Exception as cache_error:
            logger.warning(f"ç¼“å­˜å¤±è´¥ï¼ˆä¸å½±å“è¿”å›ï¼‰: {cache_error}")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–å›¾è¡¨æ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ•°æ®è·å–é”™è¯¯: {str(e)}")


@router.delete(
    "/api/stocks/{stock_code}/chart-data/cache",
    summary="æ¸…é™¤å›¾è¡¨æ•°æ®ç¼“å­˜",
    dependencies=[Depends(verify_token)]
)
async def clear_chart_cache(stock_code: str, strategy: str = Query("volume_wave")):
    """æ¸…é™¤æŒ‡å®šè‚¡ç¥¨çš„å›¾è¡¨æ•°æ®ç¼“å­˜"""
    try:
        redis_client = get_sync_redis_client()
        cache_key = _generate_cache_key(stock_code, strategy)
        deleted = redis_client.delete(cache_key)
        
        return {
            "success": True,
            "deleted": deleted > 0,
            "message": f"å·²æ¸…é™¤ {stock_code} ({strategy}) çš„ç¼“å­˜"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {str(e)}")

