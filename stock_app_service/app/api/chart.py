# -*- coding: utf-8 -*-
"""å›¾è¡¨ç›¸å…³APIè·¯ç”± - Redisç‰ˆæœ¬"""

from fastapi import APIRouter, Depends, HTTPException, Query
from typing import Dict, Any
from datetime import datetime
import json
import pandas as pd
import os
import uuid
from pathlib import Path

from app.core.redis_client import get_redis_client
from app.core.sync_redis_client import get_sync_redis_client  # æ–°å¢ï¼šåŒæ­¥Redis
from app.api.dependencies import verify_token
from app.core.config import CHART_DIR
from app.core.logging import logger
from app.trading.strategies import apply_strategy
from app.trading.renderers import generate_chart_html

router = APIRouter(tags=["è‚¡ç¥¨å›¾è¡¨"])

# ç¡®ä¿å›¾è¡¨ç›®å½•å­˜åœ¨
os.makedirs(CHART_DIR, exist_ok=True)

@router.get("/api/stocks/{stock_code}/chart", summary="ç”Ÿæˆè‚¡ç¥¨Kçº¿å›¾è¡¨ï¼ˆå·²åºŸå¼ƒï¼‰", dependencies=[Depends(verify_token)], deprecated=True)
async def generate_stock_chart(
    stock_code: str,
    strategy: str = Query("volume_wave", description="å›¾è¡¨ç­–ç•¥ç±»å‹: volume_wave(åŠ¨é‡å®ˆæ’) æˆ– volume_wave_enhanced(åŠ¨é‡å®ˆæ’å¢å¼ºç‰ˆ)"),
    theme: str = Query("dark", description="å›¾è¡¨ä¸»é¢˜: light(äº®è‰²) æˆ– dark(æš—è‰²)")
) -> Dict[str, Any]:
    """
    ç”ŸæˆæŒ‡å®šè‚¡ç¥¨çš„Kçº¿å›¾è¡¨ï¼ˆå·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨æ–°æ¥å£ï¼‰
    
    âš ï¸ è­¦å‘Šï¼šæ­¤æ¥å£å·²åºŸå¼ƒï¼Œå°†åœ¨æœªæ¥ç‰ˆæœ¬ç§»é™¤
    
    é—®é¢˜ï¼š
    - æ¯æ¬¡è¯·æ±‚ç”Ÿæˆæ–°HTMLæ–‡ä»¶ï¼Œ1000äººè®¿é—®å¯èƒ½äº§ç”Ÿæ•°ç™¾ä¸‡æ–‡ä»¶
    - ç£ç›˜I/Oç“¶é¢ˆï¼Œæ–‡ä»¶ç³»ç»Ÿæ€§èƒ½ä¸‹é™
    - ä¸é€‚åˆå®æ—¶æ•°æ®åœºæ™¯
    
    æ¨èä½¿ç”¨ï¼š
    - æ•°æ®API: GET /api/stocks/{stock_code}/chart-data
    - é€šç”¨æ¨¡æ¿: /static/chart_template.html?stock={code}&strategy={strategy}
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç 
        strategy: ç­–ç•¥ç±»å‹ï¼Œå¯é€‰ 'volume_wave'(åŠ¨é‡å®ˆæ’) æˆ– 'volume_wave_enhanced'(åŠ¨é‡å®ˆæ’å¢å¼ºç‰ˆ)
        theme: å›¾è¡¨ä¸»é¢˜ï¼Œå¯é€‰ 'light'(äº®è‰²èƒŒæ™¯) æˆ– 'dark'(æš—è‰²èƒŒæ™¯)ï¼Œé»˜è®¤æš—è‰²
        
    Returns:
        å›¾è¡¨URLå’Œå…¶ä»–ä¿¡æ¯
    """
    logger.warning(f"âš ï¸ ä½¿ç”¨å·²åºŸå¼ƒæ¥å£: /api/stocks/{stock_code}/chartï¼Œå»ºè®®è¿ç§»åˆ°æ–°æ¶æ„")
    # æ£€æŸ¥ç­–ç•¥ç±»å‹
    if strategy not in ["volume_wave", "volume_wave_enhanced", "volatility_conservation"]:
        raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„ç­–ç•¥ç±»å‹: {strategy}")
    
    # æ£€æŸ¥ä¸»é¢˜ç±»å‹
    if theme not in ["light", "dark"]:
        theme = "dark"  # é»˜è®¤æš—è‰²ä¸»é¢˜
    
    try:
        # ç›´æ¥ä½¿ç”¨åŒæ­¥Rediså®¢æˆ·ç«¯ï¼Œå®Œå…¨é¿å…äº‹ä»¶å¾ªç¯é—®é¢˜
        logger.info("ä½¿ç”¨åŒæ­¥Rediså®¢æˆ·ç«¯è·å–æ•°æ®ï¼ˆé¿å…äº‹ä»¶å¾ªç¯å†²çªï¼‰")
        redis_client = get_sync_redis_client()
        
        # æ£€æŸ¥è‚¡ç¥¨æ˜¯å¦å­˜åœ¨å¹¶è½¬æ¢ä¸ºts_codeæ ¼å¼
        stock_codes_key = "stocks:codes:all"
        stock_codes_data = redis_client.get(stock_codes_key)
        
        if not stock_codes_data:
            raise HTTPException(status_code=500, detail="è‚¡ç¥¨ä»£ç æ•°æ®ä¸å¯ç”¨")
        
        stock_codes = json.loads(stock_codes_data)
        stock_info = None
        ts_code = None
        
        # æ”¯æŒå¤šç§æ ¼å¼çš„è‚¡ç¥¨ä»£ç æŸ¥æ‰¾
        for stock in stock_codes:
            # æ£€æŸ¥ts_codeæ ¼å¼ (å¦‚: 000001.SZ)
            if stock.get('ts_code') == stock_code:
                stock_info = stock
                ts_code = stock_code
                break
            # æ£€æŸ¥symbolæ ¼å¼ (å¦‚: 000001)
            elif stock.get('symbol') == stock_code:
                stock_info = stock
                ts_code = stock.get('ts_code')
                break
            # æ£€æŸ¥ts_codeå»æ‰åç¼€åæ˜¯å¦åŒ¹é…
            elif stock.get('ts_code', '').split('.')[0] == stock_code:
                stock_info = stock
                ts_code = stock.get('ts_code')
                break
        
        if not stock_info or not ts_code:
            raise HTTPException(status_code=404, detail=f"è‚¡ç¥¨ä»£ç  {stock_code} ä¸å­˜åœ¨")
        
        # è·å–è‚¡ç¥¨å†å²æ•°æ®ï¼ˆåŒæ­¥æ–¹å¼ï¼‰
        kline_key = f"stock_trend:{ts_code}"
        logger.info(f"æ­£åœ¨è·å–è‚¡ç¥¨ {stock_code} (ts_code: {ts_code}) çš„å†å²æ•°æ®ï¼ŒRedisé”®: {kline_key}")
        
        kline_data = redis_client.get(kline_key)
        
        if not kline_data:
            # ğŸ”§ è‡ªåŠ¨æ•°æ®è¡¥å¿æœºåˆ¶ï¼šå¦‚æœRedisä¸­æ²¡æœ‰æ•°æ®ï¼Œç«‹å³ä»Tushareè·å–
            logger.warning(f"Redisä¸­æ²¡æœ‰æ‰¾åˆ°è‚¡ç¥¨ {stock_code} çš„å†å²æ•°æ®ï¼Œé”®: {kline_key}")
            logger.info(f"ğŸš€ å¯åŠ¨è‡ªåŠ¨æ•°æ®è¡¥å¿ï¼šä»Tushareè·å–è‚¡ç¥¨ {ts_code} çš„å†å²æ•°æ®...")
            
            try:
                # ä½¿ç”¨UnifiedDataServiceè·å–æ•°æ®ï¼ˆæ”¯æŒTushareï¼‰
                from app.services.stock.unified_data_service import UnifiedDataService
                import tushare as ts
                
                unified_service = UnifiedDataService()
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºETF
                is_etf = stock_info.get('market', '') == 'ETF' or ts_code.startswith(('51', '15', '16', '56'))
                
                # è·å–180å¤©æ•°æ®ï¼ˆæ»¡è¶³EMA169è®¡ç®—éœ€æ±‚ï¼‰
                logger.info(f"æ­£åœ¨è·å– {ts_code} çš„180å¤©Kçº¿æ•°æ®ï¼ˆ{'ETF' if is_etf else 'è‚¡ç¥¨'}ï¼‰...")
                
                # ä½¿ç”¨åŒæ­¥æ–¹å¼è·å–æ•°æ®
                kline_list = unified_service.fetch_historical_data(
                    ts_code=ts_code,
                    days=180,
                    is_etf=is_etf
                )
                
                if not kline_list or len(kline_list) < 20:
                    logger.error(f"âŒ ä»Tushareè·å–çš„æ•°æ®ä¸è¶³: {len(kline_list) if kline_list else 0} æ¡")
                    raise HTTPException(
                        status_code=404, 
                        detail=f"è‚¡ç¥¨ {stock_code} å†å²æ•°æ®ä¸è¶³ï¼ˆè·å–åˆ°{len(kline_list) if kline_list else 0}æ¡ï¼Œè‡³å°‘éœ€è¦20æ¡ï¼‰"
                    )
                
                # å­˜å‚¨åˆ°Redisï¼ˆæŒ‰ç…§æ ‡å‡†æ ¼å¼ï¼‰
                trend_data_to_store = {
                    'ts_code': ts_code,
                    'data': kline_list,
                    'updated_at': datetime.now().isoformat(),
                    'data_count': len(kline_list),
                    'source': 'tushare_è¡¥å¿'
                }
                
                redis_client.set(kline_key, json.dumps(trend_data_to_store, default=str))
                logger.info(f"âœ… æ•°æ®è¡¥å¿æˆåŠŸ: {ts_code}ï¼Œå·²å­˜å‚¨ {len(kline_list)} æ¡Kçº¿æ•°æ®åˆ°Redis")
                
                # é‡æ–°è¯»å–æ•°æ®
                kline_data = redis_client.get(kline_key)
                
            except ImportError as ie:
                logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {ie}")
                raise HTTPException(
                    status_code=500, 
                    detail=f"æ•°æ®è¡¥å¿å¤±è´¥ï¼šç³»ç»Ÿé…ç½®é”™è¯¯"
                )
            except Exception as e:
                logger.error(f"âŒ è‡ªåŠ¨æ•°æ®è¡¥å¿å¤±è´¥: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
                raise HTTPException(
                    status_code=404, 
                    detail=f"è‚¡ç¥¨ {stock_code} æ— æ³•è·å–å†å²æ•°æ®ã€‚é”™è¯¯ï¼š{str(e)}"
                )
        
        # è§£ææ•°æ®ï¼Œå¤„ç†ä¸åŒçš„å­˜å‚¨æ ¼å¼
        trend_data = json.loads(kline_data)
        
        # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
        if isinstance(trend_data, dict):
            # æ–°æ ¼å¼ï¼š{data: [...], updated_at: ..., source: ...}
            kline_json = trend_data.get('data', [])
        elif isinstance(trend_data, list):
            # æ—§æ ¼å¼ï¼šç›´æ¥æ˜¯Kçº¿æ•°æ®åˆ—è¡¨
            kline_json = trend_data
        else:
            raise HTTPException(status_code=400, detail=f"è‚¡ç¥¨ {stock_code} æ•°æ®æ ¼å¼ä¸æ­£ç¡®")
        
        if not kline_json or len(kline_json) < 20:
            raise HTTPException(status_code=400, detail=f"è‚¡ç¥¨ {stock_code} å†å²æ•°æ®ä¸è¶³")
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(kline_json)
        
        # æ™ºèƒ½å­—æ®µæ˜ å°„ï¼šä¿®å¤æ•°æ®æ ¼å¼æ··ä¹±å¯¼è‡´çš„å›¾è¡¨1æ ¹Kçº¿bug
        logger.info(f"åŸå§‹æ•°æ®åˆ—: {df.columns.tolist()}")
        logger.info(f"æ•°æ®è¡Œæ•°: {len(df)}")
        
        # å…³é”®ä¿®å¤ï¼šç»Ÿä¸€å¤„ç†æ—¥æœŸå­—æ®µ
        if 'date' not in df.columns:
            if 'trade_date' in df.columns:
                # tushareæ ¼å¼ï¼štrade_dateä¸º 20250102
                def convert_tushare_date(date_str):
                    date_str = str(date_str)
                    if len(date_str) == 8 and date_str.isdigit():
                        return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
                    return date_str
                
                df['date'] = pd.to_datetime(df['trade_date'].apply(convert_tushare_date))
                logger.info("ä»trade_dateè½¬æ¢dateå­—æ®µæˆåŠŸ")
            elif 'actual_trade_date' in df.columns:
                # å®é™…äº¤æ˜“æ—¥æœŸ
                df['date'] = pd.to_datetime(df['actual_trade_date'])
                logger.info("ä»actual_trade_dateè½¬æ¢dateå­—æ®µæˆåŠŸ")
            else:
                # æœ€åå…œåº•ï¼šä½¿ç”¨ç´¢å¼•ç”Ÿæˆæ—¥æœŸ
                df['date'] = pd.date_range(start='2024-01-01', periods=len(df), freq='D')
                logger.warning("ä½¿ç”¨é»˜è®¤æ—¥æœŸèŒƒå›´")
        else:
            # å·²ç»æœ‰dateå­—æ®µï¼Œç¡®ä¿æ˜¯æ—¥æœŸæ ¼å¼
            df['date'] = pd.to_datetime(df['date'])
            logger.info("dateå­—æ®µå·²å­˜åœ¨ï¼Œè½¬æ¢ä¸ºdatetimeæ ¼å¼")
        
        # å…³é”®ä¿®å¤ï¼šç»Ÿä¸€å¤„ç†æˆäº¤é‡å­—æ®µï¼Œç¡®ä¿æ‰€æœ‰è¡Œéƒ½æœ‰æœ‰æ•ˆçš„volumeå€¼
        logger.info("å¼€å§‹ä¿®å¤æˆäº¤é‡å­—æ®µ...")
        
        # æ£€æŸ¥volumeå­—æ®µæƒ…å†µ
        has_volume = 'volume' in df.columns
        has_vol = 'vol' in df.columns
        
        logger.info(f"å­—æ®µæƒ…å†µ: has_volume={has_volume}, has_vol={has_vol}")
        
        if has_volume:
            # æ£€æŸ¥volumeå­—æ®µä¸­çš„ç©ºå€¼æƒ…å†µ
            volume_null_count = df['volume'].isnull().sum()
            volume_zero_count = (df['volume'] == 0).sum()
            volume_valid_count = len(df) - volume_null_count - volume_zero_count
            logger.info(f"volumeå­—æ®µåˆ†æ: ç©ºå€¼={volume_null_count}, é›¶å€¼={volume_zero_count}, æœ‰æ•ˆå€¼={volume_valid_count}")
            
            # å¦‚æœvolumeå­—æ®µå¤§éƒ¨åˆ†ä¸ºç©ºæˆ–é›¶ï¼Œå°è¯•ä»volå­—æ®µè¡¥å……
            if volume_null_count + volume_zero_count > len(df) * 0.8 and has_vol:
                logger.info("volumeå­—æ®µå¤§éƒ¨åˆ†æ— æ•ˆï¼Œä»volå­—æ®µè¡¥å……...")
                # ç”¨volå­—æ®µå¡«è¡¥volumeå­—æ®µçš„ç©ºå€¼å’Œé›¶å€¼
                df['volume'] = df.apply(lambda row: 
                    row['vol'] * 100 if (pd.isnull(row['volume']) or row['volume'] == 0) and pd.notnull(row['vol']) and row['vol'] > 0
                    else row['volume'], axis=1)
                
                # å†æ¬¡æ£€æŸ¥ä¿®å¤æ•ˆæœ
                volume_null_count_after = df['volume'].isnull().sum()
                volume_zero_count_after = (df['volume'] == 0).sum()
                volume_valid_count_after = len(df) - volume_null_count_after - volume_zero_count_after
                logger.info(f"ä¿®å¤åvolumeå­—æ®µ: ç©ºå€¼={volume_null_count_after}, é›¶å€¼={volume_zero_count_after}, æœ‰æ•ˆå€¼={volume_valid_count_after}")
        
        if not has_volume and has_vol:
            # å¦‚æœæ²¡æœ‰volumeå­—æ®µä½†æœ‰volå­—æ®µï¼Œç›´æ¥è½¬æ¢
            logger.info("æ²¡æœ‰volumeå­—æ®µï¼Œä»volå­—æ®µåˆ›å»º...")
            df['volume'] = df['vol'].fillna(0) * 100
            logger.info(f"åˆ›å»ºvolumeå­—æ®µæˆåŠŸï¼Œæœ‰æ•ˆå€¼: {(df['volume'] > 0).sum()}")
        elif not has_volume and not has_vol:
            # å¦‚æœä¸¤ä¸ªå­—æ®µéƒ½æ²¡æœ‰ï¼Œåˆ›å»ºé»˜è®¤å€¼
            logger.warning("æ²¡æœ‰ä»»ä½•æˆäº¤é‡å­—æ®µï¼Œåˆ›å»ºé»˜è®¤å€¼")
            df['volume'] = 1000  # ç»™ä¸€ä¸ªéé›¶é»˜è®¤å€¼ï¼Œé¿å…å›¾è¡¨æ˜¾ç¤ºé—®é¢˜
        
        # æœ€ç»ˆç¡®ä¿volumeå­—æ®µæ²¡æœ‰ç©ºå€¼å’Œè´Ÿå€¼
        df['volume'] = df['volume'].fillna(1000)  # ç©ºå€¼å¡«å……ä¸º1000
        df['volume'] = df['volume'].apply(lambda x: max(x, 1) if x != 0 else 1000)  # ç¡®ä¿éƒ½æ˜¯æ­£å€¼
        
        # æ£€æŸ¥æœ€ç»ˆçš„volumeå­—æ®µ
        final_volume_valid = (df['volume'] > 0).sum()
        logger.info(f"æœ€ç»ˆvolumeå­—æ®µæ£€æŸ¥: æ€»è¡Œæ•°={len(df)}, æœ‰æ•ˆå€¼={final_volume_valid}")
        
        if final_volume_valid != len(df):
            logger.error(f"volumeå­—æ®µä»æœ‰é—®é¢˜: {len(df) - final_volume_valid} è¡Œæ— æ•ˆ")
            # å¼ºåˆ¶ä¿®å¤
            df.loc[df['volume'] <= 0, 'volume'] = 1000
            logger.info(f"å¼ºåˆ¶ä¿®å¤volumeå­—æ®µå®Œæˆ")
        
        # å¤„ç†æˆäº¤é¢å­—æ®µ
        if 'amount' in df.columns:
            # tushareæ ¼å¼ï¼šamount (å•ä½ï¼šåƒå…ƒï¼Œéœ€è¦ä¹˜ä»¥1000)
            # å¦‚æœé‡‘é¢å°äº1000000ï¼Œè®¤ä¸ºæ˜¯åƒå…ƒå•ä½ï¼Œéœ€è¦ä¹˜ä»¥1000
            df['amount'] = df['amount'].apply(lambda x: x * 1000 if x > 0 and x < 1000000 else x)
        
        logger.info(f"è½¬æ¢åæ•°æ®åˆ—: {df.columns.tolist()}")
        logger.info(f"æ•°æ®é‡: {len(df)} æ¡")
        
        # éªŒè¯å¿…è¦åˆ—
        required_columns = ['close', 'open', 'high', 'low', 'volume']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise HTTPException(status_code=400, detail=f"æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
        
        # åº”ç”¨ç­–ç•¥
        try:
            processed_df, signals = apply_strategy(strategy, df)
            logger.info(f"ç­–ç•¥åº”ç”¨æˆåŠŸ {stock_code}: ç”Ÿæˆ {len(signals)} ä¸ªä¿¡å·")
        except Exception as e:
            logger.error(f"ç­–ç•¥åº”ç”¨å¤±è´¥ {stock_code}: {str(e)}")
            raise HTTPException(status_code=500, detail=f"ç­–ç•¥åº”ç”¨å¤±è´¥: {str(e)}")
        
        # ä¸ºåŠ¨é‡å®ˆæ’ç­–ç•¥æ·»åŠ é¢å¤–çš„EMAæŒ‡æ ‡ï¼ˆä»…ç”¨äºå›¾è¡¨å±•ç¤ºï¼‰
        if strategy == 'volume_wave':
            try:
                close_values = processed_df['close'].to_numpy()
                
                # è®¡ç®—EMA12ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                if 'ema12' not in processed_df.columns:
                    from app.strategies.volume_wave_strategy import VolumeWaveStrategy
                    processed_df['ema12'] = pd.Series(VolumeWaveStrategy.calculate_ema(close_values, 12))
                
                # è®¡ç®—EMA144ï¼ˆVegaséš§é“ä¸‹è½¨ï¼‰
                if 'ema144' not in processed_df.columns:
                    from app.strategies.volume_wave_strategy import VolumeWaveStrategy
                    processed_df['ema144'] = pd.Series(VolumeWaveStrategy.calculate_ema(close_values, 144))
                
                # è®¡ç®—EMA169ï¼ˆVegaséš§é“ä¸Šè½¨ï¼‰
                if 'ema169' not in processed_df.columns:
                    from app.strategies.volume_wave_strategy import VolumeWaveStrategy
                    processed_df['ema169'] = pd.Series(VolumeWaveStrategy.calculate_ema(close_values, 169))
                
                logger.info(f"å·²ä¸ºå›¾è¡¨æ·»åŠ Vegaséš§é“æŒ‡æ ‡ï¼ˆEMA12, EMA144, EMA169ï¼‰")
            except Exception as ema_error:
                logger.warning(f"æ·»åŠ é¢å¤–EMAæŒ‡æ ‡å¤±è´¥ï¼ˆä¸å½±å“ä¿¡å·è®¡ç®—ï¼‰: {ema_error}")
        
        # å‡†å¤‡å›¾è¡¨æ•°æ®
        stock_data = {
            'stock': {
                'code': stock_code,
                'name': stock_info.get('name', stock_code)
            },
            'data': processed_df,
            'signals': signals,
            'strategy': strategy,
            'theme': theme  # æ·»åŠ ä¸»é¢˜å‚æ•°
        }
        
        # æ¸…ç†æ—§å›¾è¡¨æ–‡ä»¶
        cleanup_old_charts()
        
        # ç”Ÿæˆå›¾è¡¨
        try:
            chart_url = await generate_chart_from_redis_data(stock_data)
            if not chart_url:
                raise HTTPException(status_code=500, detail=f"ç”Ÿæˆè‚¡ç¥¨ {stock_code} çš„å›¾è¡¨å¤±è´¥ï¼šHTMLç”Ÿæˆè¿”å›ç©º")
            logger.info(f"å›¾è¡¨ç”ŸæˆæˆåŠŸ {stock_code}: {chart_url}")
        except Exception as e:
            logger.error(f"å›¾è¡¨ç”Ÿæˆå¤±è´¥ {stock_code}: {str(e)}")
            raise HTTPException(status_code=500, detail=f"å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}")
        
        # åŒæ­¥Redisä½¿ç”¨è¿æ¥æ± ï¼Œä¸éœ€è¦å…³é—­
        
        return {
            "code": stock_code,
            "name": stock_info.get('name', stock_code),
            "chart_url": chart_url,
            "strategy": strategy,
            "signals_count": len(signals),
            "generated_time": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å›¾è¡¨ç”Ÿæˆé”™è¯¯: {str(e)}")

@router.get("/api/chart/{stock_code}", summary="æŸ¥çœ‹è‚¡ç¥¨å›¾è¡¨é¡µé¢")
async def view_stock_chart(
    stock_code: str,
    strategy: str = Query("volume_wave", description="å›¾è¡¨ç­–ç•¥ç±»å‹: volume_wave(åŠ¨é‡å®ˆæ’) æˆ– volume_wave_enhanced(åŠ¨é‡å®ˆæ’å¢å¼ºç‰ˆ)"),
    theme: str = Query("dark", description="å›¾è¡¨ä¸»é¢˜: light(äº®è‰²) æˆ– dark(æš—è‰²)")
):
    """
    æŸ¥çœ‹æŒ‡å®šè‚¡ç¥¨çš„Kçº¿å›¾è¡¨é¡µé¢
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç 
        strategy: ç­–ç•¥ç±»å‹ï¼Œå¯é€‰ 'volume_wave'(åŠ¨é‡å®ˆæ’) æˆ– 'volume_wave_enhanced'(åŠ¨é‡å®ˆæ’å¢å¼ºç‰ˆ)
        theme: å›¾è¡¨ä¸»é¢˜ï¼Œå¯é€‰ 'light'(äº®è‰²èƒŒæ™¯) æˆ– 'dark'(æš—è‰²èƒŒæ™¯)ï¼Œé»˜è®¤æš—è‰²
        
    Returns:
        é‡å®šå‘åˆ°å›¾è¡¨HTMLé¡µé¢
    """
    from fastapi.responses import RedirectResponse
    
    # æ£€æŸ¥ç­–ç•¥ç±»å‹
    if strategy not in ["volume_wave", "volume_wave_enhanced", "volatility_conservation"]:
        raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„ç­–ç•¥ç±»å‹: {strategy}")
    
    # æ£€æŸ¥ä¸»é¢˜ç±»å‹
    if theme not in ["light", "dark"]:
        theme = "dark"  # é»˜è®¤æš—è‰²ä¸»é¢˜
    
    try:
        # ç”Ÿæˆå›¾è¡¨ï¼Œä¼ é€’ä¸»é¢˜å‚æ•°
        chart_result = await generate_stock_chart(stock_code, strategy, theme)
        chart_url = chart_result.get('chart_url')
        
        if not chart_url:
            raise HTTPException(status_code=500, detail=f"ç”Ÿæˆè‚¡ç¥¨ {stock_code} çš„å›¾è¡¨å¤±è´¥")
        
        # é‡å®šå‘åˆ°å›¾è¡¨é¡µé¢
        return RedirectResponse(url=chart_url)
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å›¾è¡¨ç”Ÿæˆé”™è¯¯: {str(e)}")

async def generate_chart_from_redis_data(stock_data: Dict[str, Any]) -> str:
    """
    ä»Redisæ•°æ®ç”Ÿæˆå›¾è¡¨çš„è¾…åŠ©å‡½æ•°
    
    Args:
        stock_data: åŒ…å«è‚¡ç¥¨ä¿¡æ¯ã€æ•°æ®ã€ä¿¡å·å’Œä¸»é¢˜çš„å­—å…¸
        
    Returns:
        å›¾è¡¨URL
    """
    try:
        stock = stock_data['stock']
        strategy = stock_data['strategy']
        theme = stock_data.get('theme', 'dark')  # è·å–ä¸»é¢˜ï¼Œé»˜è®¤æš—è‰²
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        chart_file = f"{stock['code']}_{strategy}_{theme}_{datetime.now().strftime('%Y%m%d%H%M%S')}_{uuid.uuid4().hex[:8]}.html"
        chart_path = os.path.join(CHART_DIR, chart_file)
        
        # ç”ŸæˆHTMLå†…å®¹ï¼Œä¼ é€’ä¸»é¢˜å‚æ•°
        html_content = generate_chart_html(strategy, stock_data, theme=theme)
        
        if not html_content:
            return None
        
        # ç›´æ¥ä½¿ç”¨åŒæ­¥æ–‡ä»¶å†™å…¥ï¼ˆæ–‡ä»¶I/Oå¾ˆå¿«ï¼Œä¸ä¼šé˜»å¡ï¼‰
        # è¿™æ ·å¯ä»¥å®Œå…¨é¿å…äº‹ä»¶å¾ªç¯å†²çªé—®é¢˜
        _write_chart_file(chart_path, html_content)
        
        # è¿”å›å›¾è¡¨URL
        return f"/static/charts/{chart_file}"
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None

def _write_chart_file(file_path: str, content: str):
    """åŒæ­¥å†™å…¥å›¾è¡¨æ–‡ä»¶ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        logger.debug(f"å›¾è¡¨æ–‡ä»¶å†™å…¥æˆåŠŸ: {file_path}")
    except Exception as e:
        logger.error(f"å›¾è¡¨æ–‡ä»¶å†™å…¥å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        raise

def cleanup_old_charts(max_files: int = 100):
    """æ¸…ç†æ—§å›¾è¡¨æ–‡ä»¶ï¼Œä¿ç•™æœ€æ–°çš„Nä¸ª"""
    try:
        files = list(Path(CHART_DIR).glob("*.html"))
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        
        # åˆ é™¤æ—§æ–‡ä»¶
        for file in files[max_files:]:
            os.remove(file)
    except Exception as e:
        print(f"æ¸…ç†æ—§å›¾è¡¨å¤±è´¥: {e}")