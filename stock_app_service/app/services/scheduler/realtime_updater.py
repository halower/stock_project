# -*- coding: utf-8 -*-
"""
å®æ—¶æ•°æ®æ›´æ–°æœåŠ¡
è´Ÿè´£è‚¡ç¥¨å’ŒETFçš„å®æ—¶æ•°æ®è·å–å’Œæ›´æ–°
"""

import asyncio
import threading
import traceback
from datetime import datetime
from typing import Dict, Any, List, Tuple

from app.core.logging import logger
from app.db.session import RedisCache
from app.services.realtime import get_proxy_manager, get_etf_realtime_service_v2

# Redisç¼“å­˜å®¢æˆ·ç«¯
redis_cache = RedisCache()

# Redisé”®åè§„åˆ™
STOCK_KEYS = {
    'stock_codes': 'stocks:codes:all',
    'stock_kline': 'stock_trend:{}',
    'strategy_signals': 'stock:buy_signals',
    'realtime_data': 'stock:realtime',
    'scheduler_log': 'stock:scheduler:log',
    'last_update': 'stock:last_update',
}

ETF_KEYS = {
    'etf_codes': 'etf:codes:all',
    'etf_realtime': 'etf:realtime',
    'etf_kline': 'etf_trend:{}',
    'etf_signals': 'etf:buy_signals',
    'etf_scheduler_log': 'etf:scheduler:log',
    'etf_last_update': 'etf:last_update',
}


def get_stock_realtime_data_sina() -> List[Dict]:
    """
    ä½¿ç”¨æ–°æµªæ¥å£è·å–æ‰€æœ‰è‚¡ç¥¨å®æ—¶æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
    
    Returns:
        è‚¡ç¥¨å®æ—¶æ•°æ®åˆ—è¡¨
    """
    try:
        from app.services.realtime import get_proxy_manager
        from app.services.realtime.stock_realtime_service import StockRealtimeServiceV2
        
        logger.info("ğŸ”„ ä½¿ç”¨æ–°æµªæ¥å£è·å–è‚¡ç¥¨å®æ—¶æ•°æ®...")
        
        # åˆ›å»ºç‹¬ç«‹çš„æ–°æµªæ¥å£æœåŠ¡å®ä¾‹ï¼ˆä¸ä½¿ç”¨å•ä¾‹ï¼‰
        proxy_manager = get_proxy_manager()
        realtime_service = StockRealtimeServiceV2(
            proxy_manager=proxy_manager,
            default_provider='sina',  # æ˜ç¡®æŒ‡å®šä½¿ç”¨æ–°æµªæ¥å£
            auto_switch=False,  # ä¸è‡ªåŠ¨åˆ‡æ¢åˆ°å…¶ä»–æ•°æ®æº
            retry_times=3,
            timeout=15
        )
        result = realtime_service.get_all_stocks_realtime(provider='sina')
        
        if not result.get('success'):
            logger.error(f"æ–°æµªæ¥å£è·å–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return []
        
        realtime_data = result.get('data', [])
        data_source = result.get('source', 'unknown')
        
        logger.info(f"âœ… æ–°æµªæ¥å£æˆåŠŸè·å– {len(realtime_data)} åªè‚¡ç¥¨å®æ—¶æ•°æ®ï¼Œæ•°æ®æº: {data_source}")
        return realtime_data
        
    except Exception as e:
        logger.error(f"âŒ æ–°æµªæ¥å£è·å–è‚¡ç¥¨å®æ—¶æ•°æ®å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return []


def get_stock_realtime_data_akshare() -> List[Dict]:
    """
    ä½¿ç”¨ akshare è·å–æ‰€æœ‰è‚¡ç¥¨å®æ—¶æ•°æ®
    
    Returns:
        è‚¡ç¥¨å®æ—¶æ•°æ®åˆ—è¡¨
    """
    try:
        import akshare as ak
        
        logger.info("ğŸ”„ ä½¿ç”¨ akshare è·å–è‚¡ç¥¨å®æ—¶æ•°æ®...")
        
        # ä½¿ç”¨ akshare çš„å®æ—¶è¡Œæƒ…æ¥å£
        df = ak.stock_zh_a_spot_em()
        
        if df.empty:
            logger.error("akshare è¿”å›çš„æ•°æ®ä¸ºç©º")
            return []
        
        # è½¬æ¢æ•°æ®æ ¼å¼
        realtime_data = []
        for _, row in df.iterrows():
            try:
                code = str(row.get('ä»£ç ', ''))
                if not code:
                    continue
                
                # æ„é€ æ ‡å‡†æ ¼å¼çš„å®æ—¶æ•°æ®
                stock_data = {
                    'code': code,
                    'name': str(row.get('åç§°', '')),
                    'price': float(row.get('æœ€æ–°ä»·', 0)),
                    'change': float(row.get('æ¶¨è·Œé¢', 0)),
                    'change_percent': float(row.get('æ¶¨è·Œå¹…', 0)),
                    'open': float(row.get('ä»Šå¼€', 0)),
                    'high': float(row.get('æœ€é«˜', 0)),
                    'low': float(row.get('æœ€ä½', 0)),
                    'pre_close': float(row.get('æ˜¨æ”¶', 0)),
                    'volume': float(row.get('æˆäº¤é‡', 0)),
                    'amount': float(row.get('æˆäº¤é¢', 0)),
                    'turnover_rate': float(row.get('æ¢æ‰‹ç‡', 0)) if 'æ¢æ‰‹ç‡' in row else 0.0,
                }
                realtime_data.append(stock_data)
            except Exception as e:
                logger.debug(f"è§£æè‚¡ç¥¨ {row.get('ä»£ç ', 'unknown')} æ•°æ®å¤±è´¥: {e}")
                continue
        
        logger.info(f"âœ… akshare æˆåŠŸè·å– {len(realtime_data)} åªè‚¡ç¥¨å®æ—¶æ•°æ®")
        return realtime_data
        
    except Exception as e:
        logger.error(f"âŒ akshare è·å–è‚¡ç¥¨å®æ—¶æ•°æ®å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return []


def get_stock_realtime_data_with_fallback(prefer_source: str = 'sina') -> Tuple[List[Dict], str]:
    """
    è·å–è‚¡ç¥¨å®æ—¶æ•°æ®ï¼ˆå¸¦é™çº§ç­–ç•¥ï¼‰
    
    Args:
        prefer_source: ä¼˜å…ˆæ•°æ®æº ('sina' æˆ– 'akshare')
        
    Returns:
        Tuple[è‚¡ç¥¨å®æ—¶æ•°æ®åˆ—è¡¨, å®é™…ä½¿ç”¨çš„æ•°æ®æº]
    """
    if prefer_source == 'sina':
        # ä¼˜å…ˆä½¿ç”¨æ–°æµªæ¥å£
        logger.info("ğŸ“Š ä¼˜å…ˆä½¿ç”¨æ–°æµªæ¥å£è·å–è‚¡ç¥¨å®æ—¶æ•°æ®")
        realtime_data = get_stock_realtime_data_sina()
        
        if realtime_data and len(realtime_data) > 0:
            return realtime_data, 'sina'
        
        # æ–°æµªå¤±è´¥ï¼Œé™çº§åˆ° akshare
        logger.warning("âš ï¸  æ–°æµªæ¥å£å¤±è´¥ï¼Œé™çº§åˆ° akshare")
        realtime_data = get_stock_realtime_data_akshare()
        
        if realtime_data and len(realtime_data) > 0:
            return realtime_data, 'akshare'
        
        return [], 'none'
    
    else:
        # ä¼˜å…ˆä½¿ç”¨ akshare
        logger.info("ğŸ“Š ä¼˜å…ˆä½¿ç”¨ akshare è·å–è‚¡ç¥¨å®æ—¶æ•°æ®")
        realtime_data = get_stock_realtime_data_akshare()
        
        if realtime_data and len(realtime_data) > 0:
            return realtime_data, 'akshare'
        
        # akshare å¤±è´¥ï¼Œé™çº§åˆ°æ–°æµª
        logger.warning("âš ï¸  akshare å¤±è´¥ï¼Œé™çº§åˆ°æ–°æµªæ¥å£")
        realtime_data = get_stock_realtime_data_sina()
        
        if realtime_data and len(realtime_data) > 0:
            return realtime_data, 'sina'
        
        return [], 'none'


def get_etf_realtime_data(force_update=False) -> Tuple[Dict[str, Dict], str]:
    """
    è·å–ETFå®æ—¶æ•°æ®ï¼ˆä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ¥å£ï¼‰
    
    Args:
        force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
        
    Returns:
        Tuple[å®æ—¶æ•°æ®å­—å…¸, æ•°æ®æº]
        å®æ—¶æ•°æ®å­—å…¸æ ¼å¼: {code: {data}}
    """
    from app.core.etf_config import get_etf_list
    
    try:
        # 1. ä»é…ç½®æ–‡ä»¶è¯»å–ETFåˆ—è¡¨ï¼ˆ121ä¸ªç²¾é€‰ETFï¼‰
        etf_config_list = get_etf_list()
        
        etf_codes_list = []
        for etf in etf_config_list:
            etf_codes_list.append({
                'code': etf['symbol'],
                'name': etf['name'],
                'ts_code': etf['ts_code'],
                'market': etf.get('market', 'ETF')
            })
        
        # å­˜å‚¨ETFä»£ç åˆ—è¡¨åˆ°Redis
        redis_cache.set_cache(ETF_KEYS['etf_codes'], etf_codes_list, ttl=86400)
        
        # 2. è·å–å®æ—¶æ•°æ®ï¼ˆä»…è·å–CSVä¸­çš„ETFï¼‰- ä½¿ç”¨V2æœåŠ¡ï¼ˆæ”¯æŒä»£ç†ï¼‰
        proxy_manager = get_proxy_manager()
        etf_service = get_etf_realtime_service_v2(proxy_manager=proxy_manager)
        result = etf_service.get_all_etfs_realtime()
        
        if not result.get('success'):
            raise Exception(result.get('error', 'è·å–ETFå®æ—¶æ•°æ®å¤±è´¥'))
        
        all_realtime_data = result.get('data', [])
        data_source = result.get('source', 'unknown')
        
        logger.info(f"âœ… æˆåŠŸä» {data_source} è·å– {len(all_realtime_data)} åªETFå®æ—¶æ•°æ®")
        
        # 3. è¿‡æ»¤å‡ºCSVä¸­ç›‘æ§çš„ETFï¼ˆä»¥codeä¸ºkeyï¼‰
        monitored_codes = {etf['code'] for etf in etf_codes_list}
        
        realtime_dict = {}
        for etf in all_realtime_data:
            code = etf.get('code')
            # åªä¿ç•™CSVä¸­ç›‘æ§çš„ETF
            if code and code in monitored_codes:
                realtime_dict[code] = etf
        
        # 4. å­˜å‚¨åˆ°Redisï¼ˆåªå­˜å‚¨ç›‘æ§çš„ETFï¼‰
        redis_cache.set_cache(
            ETF_KEYS['etf_realtime'],
            {
                'data': realtime_dict,
                'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source': data_source,
                'count': len(realtime_dict),
                'monitored_count': len(monitored_codes),
                'total_count': len(all_realtime_data)
            },
            ttl=3600  # 1å°æ—¶è¿‡æœŸ
        )
        
        return realtime_dict, data_source
        
    except Exception as e:
        logger.error(f'è·å–ETFå®æ—¶æ•°æ®å¤±è´¥: {str(e)}')
        logger.error(traceback.format_exc())
        return {}, 'error'


def merge_stock_realtime_to_kline(realtime_data: List[Dict], is_closing_update=False) -> Tuple[int, int]:
    """
    å°†è‚¡ç¥¨å®æ—¶æ•°æ®åˆå¹¶åˆ°Kçº¿æ•°æ®
    
    Args:
        realtime_data: å®æ—¶æ•°æ®åˆ—è¡¨
        is_closing_update: æ˜¯å¦ä¸ºæ”¶ç›˜åæ›´æ–°
        
    Returns:
        Tuple[æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡]
    """
    updated_count = 0
    failed_count = 0
    today_str = datetime.now().strftime('%Y-%m-%d')
    today_trade_date = datetime.now().strftime('%Y%m%d')
    
    try:
        for stock_data in realtime_data:
            try:
                stock_code = stock_data.get('code')
                if not stock_code:
                    continue
                
                # æ„é€ ts_code
                if stock_code.startswith('6'):
                    ts_code = f"{stock_code}.SH"
                elif stock_code.startswith(('43', '83', '87', '88')):
                    ts_code = f"{stock_code}.BJ"
                else:
                    ts_code = f"{stock_code}.SZ"
                
                # è·å–Kçº¿æ•°æ®
                kline_key = STOCK_KEYS['stock_kline'].format(ts_code)
                kline_data = redis_cache.get_cache(kline_key)
                
                if not kline_data:
                    failed_count += 1
                    continue
                
                # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                if isinstance(kline_data, dict):
                    kline_list = kline_data.get('data', [])
                    trend_data = kline_data
                elif isinstance(kline_data, list):
                    kline_list = kline_data
                    trend_data = {
                        'data': kline_list,
                        'updated_at': datetime.now().isoformat(),
                        'data_count': len(kline_list),
                        'source': 'legacy_format'
                    }
                else:
                    continue
                
                if not kline_list:
                    continue
                
                # æ£€æŸ¥æœ€åä¸€æ ¹Kçº¿æ˜¯å¦æ˜¯ä»Šå¤©çš„æ•°æ®
                last_kline = kline_list[-1]
                last_trade_date = str(last_kline.get('trade_date', ''))
                last_date = last_kline.get('actual_trade_date', last_kline.get('date', ''))
                
                # å®æ—¶æ•°æ®ä¸­çš„æˆäº¤é‡æ•°æ®å¤„ç†
                current_volume = stock_data.get('volume', 0)
                if current_volume == 0:
                    current_volume = stock_data.get('vol', 0)
                if current_volume is None or current_volume < 0:
                    current_volume = 0
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦æ–°å¢ä»Šæ—¥Kçº¿
                if last_trade_date != today_trade_date and last_date != today_str:
                    # æ–°å¢ä»Šå¤©çš„Kçº¿
                    new_kline = {
                        'ts_code': ts_code,
                        'trade_date': today_trade_date,
                        'open': stock_data['open'],
                        'high': stock_data['high'],
                        'low': stock_data['low'],
                        'close': stock_data['price'],
                        'pre_close': stock_data['pre_close'],
                        'change': stock_data['change'],
                        'pct_chg': stock_data['change_percent'],
                        'vol': current_volume / 100 if current_volume > 100 else current_volume,
                        'amount': stock_data['amount'] / 1000 if stock_data['amount'] > 1000 else stock_data['amount'],
                        'actual_trade_date': today_str,
                        'is_closing_data': is_closing_update,
                        'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    kline_list.append(new_kline)
                else:
                    # æ›´æ–°æœ€åä¸€æ ¹Kçº¿
                    existing_volume = float(last_kline.get('vol', 0))
                    current_volume_in_hands = current_volume / 100
                    final_volume = max(existing_volume, current_volume_in_hands) if current_volume > 0 else existing_volume
                    
                    last_kline.update({
                        'ts_code': ts_code,
                        'trade_date': today_trade_date,
                        'high': max(float(last_kline.get('high', 0)), stock_data['high']),
                        'low': min(float(last_kline.get('low', float('inf'))), stock_data['low']) if float(last_kline.get('low', float('inf'))) != float('inf') else stock_data['low'],
                        'close': stock_data['price'],
                        'pre_close': stock_data['pre_close'],
                        'change': stock_data['change'],
                        'pct_chg': stock_data['change_percent'],
                        'vol': final_volume,
                        'amount': stock_data['amount'] / 1000,
                        'actual_trade_date': today_str,
                        'is_closing_data': is_closing_update,
                        'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                
                # æ›´æ–°trend_data
                trend_data.update({
                    'data': kline_list,
                    'updated_at': datetime.now().isoformat(),
                    'data_count': len(kline_list),
                    'last_update_type': 'closing_update' if is_closing_update else 'realtime_update'
                })
                
                # æ›´æ–°Redisç¼“å­˜
                redis_cache.set_cache(kline_key, trend_data, ttl=None)
                updated_count += 1
                
            except Exception as e:
                failed_count += 1
                if failed_count <= 5:
                    logger.error(f"å¤„ç†è‚¡ç¥¨ {stock_data.get('code', 'unknown')} å¤±è´¥: {str(e)}")
                continue
        
        if failed_count > 0:
            logger.warning(f"âš ï¸  æœ‰ {failed_count} åªè‚¡ç¥¨æ›´æ–°å¤±è´¥")
            
        return updated_count, failed_count
        
    except Exception as e:
        logger.error(f"åˆå¹¶è‚¡ç¥¨å®æ—¶æ•°æ®åˆ°Kçº¿å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return 0, len(realtime_data) if realtime_data else 0


def merge_etf_realtime_to_kline(realtime_dict: Dict[str, Dict], is_closing_update=False) -> Tuple[int, int]:
    """
    å°†ETFå®æ—¶æ•°æ®åˆå¹¶åˆ°Kçº¿æ•°æ®
    å½“å¤©æ²¡æœ‰Kçº¿åˆ™æ–°å¢ï¼Œæœ‰Kçº¿åˆ™æ›´æ–°
    
    Args:
        realtime_dict: å®æ—¶æ•°æ®å­—å…¸ {code: data}
        is_closing_update: æ˜¯å¦ä¸ºæ”¶ç›˜åæ›´æ–°
        
    Returns:
        Tuple[æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡]
    """
    updated_count = 0
    appended_count = 0
    created_count = 0
    failed_count = 0
    
    try:
        today_str = datetime.now().strftime('%Y-%m-%d')
        today_trade_date = datetime.now().strftime('%Y%m%d')
        
        for code, etf_data in realtime_dict.items():
            try:
                # æ„é€ ts_code
                if code.startswith('5'):
                    ts_code = f"{code}.SH"
                else:
                    ts_code = f"{code}.SZ"
                
                # è·å–Kçº¿æ•°æ®
                kline_key = ETF_KEYS['etf_kline'].format(ts_code)
                kline_data = redis_cache.get_cache(kline_key)
                
                # å¦‚æœæ²¡æœ‰Kçº¿æ•°æ®ï¼Œåˆ›å»ºæ–°çš„Kçº¿æ•°æ®åˆ—è¡¨
                if not kline_data or not isinstance(kline_data, list) or len(kline_data) == 0:
                    new_kline = {
                        'date': today_str,
                        'trade_date': today_trade_date,
                        'open': etf_data.get('open', etf_data.get('price', 0)),
                        'close': etf_data.get('price', 0),
                        'high': etf_data.get('high', etf_data.get('price', 0)),
                        'low': etf_data.get('low', etf_data.get('price', 0)),
                        'volume': etf_data.get('volume', 0),
                        'amount': etf_data.get('amount', 0),
                        'turnover_rate': etf_data.get('turnover_rate', 0),
                        'change': etf_data.get('change', 0),
                        'pct_chg': etf_data.get('change_percent', 0),
                        'is_closing_data': is_closing_update,
                        'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    kline_data = [new_kline]
                    redis_cache.set_cache(kline_key, kline_data, ttl=604800)
                    created_count += 1
                    continue
                
                # è·å–æœ€åä¸€æ¡Kçº¿
                last_kline = kline_data[-1]
                
                # è·å–æœ€åä¸€æ¡Kçº¿çš„æ—¥æœŸ
                last_date = str(last_kline.get('date', ''))
                last_trade_date = str(last_kline.get('trade_date', ''))
                
                # åˆ¤æ–­æ˜¯å¦æ˜¯ä»Šå¤©çš„æ•°æ®
                is_today = (last_date == today_str) or (last_trade_date == today_trade_date)
                
                if is_today:
                    # æ›´æ–°ä»Šå¤©çš„Kçº¿
                    last_kline['close'] = etf_data.get('price', last_kline.get('close', 0))
                    last_kline['high'] = max(
                        last_kline.get('high', 0),
                        etf_data.get('high', 0),
                        etf_data.get('price', 0)
                    )
                    last_kline['low'] = min(
                        last_kline.get('low', 999999),
                        etf_data.get('low', 999999),
                        etf_data.get('price', 999999)
                    ) if last_kline.get('low', 0) > 0 else etf_data.get('low', 0)
                    last_kline['volume'] = etf_data.get('volume', last_kline.get('volume', 0))
                    last_kline['amount'] = etf_data.get('amount', last_kline.get('amount', 0))
                    last_kline['turnover_rate'] = etf_data.get('turnover_rate', last_kline.get('turnover_rate', 0))
                    last_kline['is_closing_data'] = is_closing_update
                    last_kline['update_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    
                    redis_cache.set_cache(kline_key, kline_data, ttl=604800)
                    updated_count += 1
                else:
                    # æ–°å¢ä»Šå¤©çš„Kçº¿
                    new_kline = {
                        'date': today_str,
                        'trade_date': today_trade_date,
                        'open': etf_data.get('open', etf_data.get('price', 0)),
                        'close': etf_data.get('price', 0),
                        'high': etf_data.get('high', etf_data.get('price', 0)),
                        'low': etf_data.get('low', etf_data.get('price', 0)),
                        'volume': etf_data.get('volume', 0),
                        'amount': etf_data.get('amount', 0),
                        'turnover_rate': etf_data.get('turnover_rate', 0),
                        'change': etf_data.get('change', 0),
                        'pct_chg': etf_data.get('change_percent', 0),
                        'is_closing_data': is_closing_update,
                        'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    kline_data.append(new_kline)
                    
                    # ä¿æŒæœ€å¤š1000æ¡Kçº¿
                    if len(kline_data) > 1000:
                        kline_data = kline_data[-1000:]
                    
                    redis_cache.set_cache(kline_key, kline_data, ttl=604800)
                    appended_count += 1
                    
            except Exception as e:
                failed_count += 1
                if failed_count <= 5:
                    logger.warning(f"åˆå¹¶ETF {code} å®æ—¶æ•°æ®å¤±è´¥: {e}")
                continue
        
        total_success = updated_count + appended_count + created_count
        
        if failed_count > 0:
            logger.warning(f"âš ï¸  æœ‰ {failed_count} åªETFæ›´æ–°å¤±è´¥")
        
        return total_success, failed_count
        
    except Exception as e:
        logger.error(f"åˆå¹¶ETFå®æ—¶æ•°æ®åˆ°Kçº¿å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return 0, len(realtime_dict)


def update_realtime_data(force_update=False, is_closing_update=False, auto_calculate_signals=False) -> Dict[str, Any]:
    """
    æ›´æ–°å®æ—¶æ•°æ®ï¼ˆè‚¡ç¥¨+ETFï¼‰
    
    æµç¨‹ï¼š
    1. ä½¿ç”¨æ–°æµªæ¥å£è·å–è‚¡ç¥¨å®æ—¶æ•°æ®ï¼ˆå¤±è´¥æ—¶é™çº§åˆ° akshareï¼‰
    2. ä½¿ç”¨ä¸œæ–¹è´¢å¯Œè·å– ETF å®æ—¶æ•°æ®
    3. åˆå¹¶åˆ°Kçº¿æ•°æ®
    4. è§¦å‘ä¸€æ¬¡ä¿¡å·è®¡ç®—ï¼ˆå¦‚æœé…ç½®å…è®¸ï¼‰
    
    Args:
        force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°ï¼Œå¿½ç•¥äº¤æ˜“æ—¶é—´æ£€æŸ¥
        is_closing_update: æ˜¯å¦ä¸ºæ”¶ç›˜åæ›´æ–°
        auto_calculate_signals: æ˜¯å¦è‡ªåŠ¨è®¡ç®—ä¹°å…¥ä¿¡å·
        
    Returns:
        æ›´æ–°ç»“æœå­—å…¸
    """
    start_time = datetime.now()
    
    try:
        if is_closing_update:
            logger.info("ğŸ“Š å¼€å§‹æ›´æ–°æ”¶ç›˜æ•°æ®ï¼ˆè‚¡ç¥¨+ETFï¼‰...")
        else:
            logger.info("ğŸ“Š å¼€å§‹æ›´æ–°å®æ—¶æ•°æ®ï¼ˆè‚¡ç¥¨+ETFï¼‰...")
        
        # æ­¥éª¤1: ä½¿ç”¨é™çº§ç­–ç•¥è·å–è‚¡ç¥¨å®æ—¶æ•°æ®ï¼ˆä¼˜å…ˆæ–°æµªï¼Œå¤±è´¥æ—¶é™çº§åˆ° akshareï¼‰
        logger.info("ğŸ“Š æ­¥éª¤1/3: è·å–è‚¡ç¥¨å®æ—¶æ•°æ®ï¼ˆæ–°æµªæ¥å£ -> akshare é™çº§ï¼‰...")
        stock_realtime_data, data_source = get_stock_realtime_data_with_fallback(prefer_source='sina')
        
        if not stock_realtime_data:
            raise Exception("è·å–è‚¡ç¥¨å®æ—¶æ•°æ®ä¸ºç©ºï¼ˆæ–°æµªå’Œakshareå‡å¤±è´¥ï¼‰")
        
        # å­˜å‚¨åˆ°Redis
        redis_cache.set_cache(STOCK_KEYS['realtime_data'], {
            'data': stock_realtime_data,
            'count': len(stock_realtime_data),
            'update_time': datetime.now().isoformat(),
            'data_source': data_source,
            'is_closing_data': is_closing_update
        }, ttl=1800)
        
        # åˆå¹¶è‚¡ç¥¨å®æ—¶æ•°æ®åˆ°Kçº¿
        stock_success, stock_failed = merge_stock_realtime_to_kline(stock_realtime_data, is_closing_update)
        logger.info(f"   âœ… è‚¡ç¥¨æ›´æ–°å®Œæˆ: æˆåŠŸ {stock_success} åª, å¤±è´¥ {stock_failed} åª")
        
        # æ­¥éª¤2: è·å–ETFå®æ—¶æ•°æ®
        logger.info("ğŸ“Š æ­¥éª¤2/3: è·å–ETFå®æ—¶æ•°æ®ï¼ˆä¸œæ–¹è´¢å¯Œï¼‰...")
        etf_realtime_dict, etf_source = get_etf_realtime_data(force_update=True)
        
        if not etf_realtime_dict:
            logger.warning("âš ï¸  è·å–ETFå®æ—¶æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ETFæ›´æ–°")
            etf_success, etf_failed = 0, 0
        else:
            # åˆå¹¶ETFå®æ—¶æ•°æ®åˆ°Kçº¿
            etf_success, etf_failed = merge_etf_realtime_to_kline(etf_realtime_dict, is_closing_update)
            logger.info(f"   âœ… ETFæ›´æ–°å®Œæˆ: æˆåŠŸ {etf_success} åª, å¤±è´¥ {etf_failed} åª")
        
        # æ­¥éª¤3: è§¦å‘ä¿¡å·è®¡ç®—ï¼ˆåªè§¦å‘ä¸€æ¬¡ï¼‰
        from app.core.config import REALTIME_AUTO_CALCULATE_SIGNALS
        should_calculate = REALTIME_AUTO_CALCULATE_SIGNALS if not auto_calculate_signals else auto_calculate_signals
        
        if should_calculate:
            logger.info("ğŸ“Š æ­¥éª¤3/3: è§¦å‘ä¹°å…¥ä¿¡å·è®¡ç®—ï¼ˆè‚¡ç¥¨+ETFç»Ÿä¸€è®¡ç®—ï¼‰...")
            # å¯¼å…¥è§¦å‘å‡½æ•°
            from app.services.scheduler.stock_scheduler import _trigger_signal_recalculation_async
            _trigger_signal_recalculation_async()
            signal_status = "âœ… ä¿¡å·è®¡ç®—å·²è§¦å‘"
        else:
            logger.info("ğŸ“Š æ­¥éª¤3/3: è·³è¿‡ä¿¡å·è®¡ç®—ï¼ˆé…ç½®: REALTIME_AUTO_CALCULATE_SIGNALS=falseï¼‰")
            signal_status = "â­ï¸ ä¿¡å·è®¡ç®—å·²è·³è¿‡"
        
        total_success = stock_success + etf_success
        total_failed = stock_failed + etf_failed
        execution_time = (datetime.now() - start_time).total_seconds()
        
        logger.info("=" * 70)
        logger.info("ğŸ‰ å®æ—¶æ•°æ®æ›´æ–°å®Œæˆ")
        logger.info(f"   ğŸ“ˆ è‚¡ç¥¨: æˆåŠŸ {stock_success} åª, å¤±è´¥ {stock_failed} åª")
        logger.info(f"   ğŸ“Š ETF:  æˆåŠŸ {etf_success} åª, å¤±è´¥ {etf_failed} åª")
        logger.info(f"   ğŸ“‹ æ€»è®¡: æˆåŠŸ {total_success} åª, å¤±è´¥ {total_failed} åª")
        logger.info(f"   ğŸ”” ä¿¡å·: {signal_status}")
        logger.info(f"   â±ï¸  è€—æ—¶: {execution_time:.2f}ç§’")
        logger.info(f"   ğŸ“¡ æ•°æ®æº: è‚¡ç¥¨({data_source}) + ETF(ä¸œæ–¹è´¢å¯Œ)")
        logger.info("=" * 70)
        
        return {
            'success': True,
            'stock_success': stock_success,
            'stock_failed': stock_failed,
            'etf_success': etf_success,
            'etf_failed': etf_failed,
            'total_success': total_success,
            'total_failed': total_failed,
            'execution_time': execution_time,
            'signal_status': signal_status
        }
        
    except Exception as e:
        execution_time = (datetime.now() - start_time).total_seconds()
        error_msg = f'å®æ—¶æ•°æ®æ›´æ–°å¤±è´¥: {str(e)}'
        logger.error(f"âŒ {error_msg}")
        logger.error(traceback.format_exc())
        
        return {
            'success': False,
            'error': error_msg,
            'execution_time': execution_time
        }

