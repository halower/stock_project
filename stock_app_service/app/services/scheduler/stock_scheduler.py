# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ•°æ®è°ƒåº¦å™¨ V2 - é‡æ„ç‰ˆ
æŒ‰ç…§DDDåŸåˆ™é‡æ–°ç»„ç»‡ï¼Œåˆ†ç¦»å¯åŠ¨ä»»åŠ¡å’Œè¿è¡Œæ—¶ä»»åŠ¡
"""

import asyncio
import threading
from datetime import datetime, time
from typing import Dict, Any
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger

from app.core.logging import logger
from app.core.config import settings
from app.db.session import RedisCache
from app.services.stock.stock_atomic_service import stock_atomic_service

# Redisç¼“å­˜å®¢æˆ·ç«¯
redis_cache = RedisCache()

# è°ƒåº¦å™¨å®ä¾‹
scheduler = None
job_logs = []  # å­˜å‚¨æœ€è¿‘çš„ä»»åŠ¡æ‰§è¡Œæ—¥å¿—

# ä»»åŠ¡æ‰§è¡Œé”
_task_locks = {
    'realtime_update': threading.Lock(),
    'signal_calculation': threading.Lock(),
    'full_update': threading.Lock(),
}


def add_job_log(job_type: str, status: str, message: str, **kwargs):
    """æ·»åŠ ä»»åŠ¡æ‰§è¡Œæ—¥å¿—"""
    log_entry = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'job_type': job_type,
        'status': status,
        'message': message,
        **kwargs
    }
    
    # å†…å­˜æ—¥å¿—ï¼ˆæœ€è¿‘10æ¡ï¼‰
    global job_logs
    job_logs.insert(0, log_entry)
    job_logs = job_logs[:10]
    
    # Redisæ—¥å¿—ï¼ˆæœ€è¿‘20æ¡ï¼‰
    redis_logs = redis_cache.get_cache('stock:scheduler:log') or []
    redis_logs.insert(0, log_entry)
    redis_logs = redis_logs[:20]
    redis_cache.set_cache('stock:scheduler:log', redis_logs, ttl=86400)
    
    logger.info(f"[{job_type}] {message}")


def is_trading_time() -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´ï¼ˆåŒ…æ‹¬ç›˜åæ—¶é—´ï¼‰
    
    äº¤æ˜“æ—¶é—´: 9:15-12:00, 13:00-15:15
    - 9:15å¼€å§‹ï¼šæå‰15åˆ†é’Ÿå‡†å¤‡
    - 12:00ç»“æŸï¼šä¸Šåˆæ”¶ç›˜å30åˆ†é’Ÿ
    - 15:15ç»“æŸï¼šä¸‹åˆæ”¶ç›˜å15åˆ†é’Ÿ
    """
    now = datetime.now()
    
    # å‘¨æœ«ä¸äº¤æ˜“
    if now.weekday() >= 5:
        return False
    
    current_time = now.time()
    
    # ä¸Šåˆæ—¶æ®µï¼ˆ9:15-12:00ï¼‰
    morning_start = time(9, 15)
    morning_end = time(12, 0)
    
    # ä¸‹åˆæ—¶æ®µï¼ˆ13:00-15:15ï¼‰
    afternoon_start = time(13, 0)
    afternoon_end = time(15, 15)
    
    return (
        (morning_start <= current_time <= morning_end) or
        (afternoon_start <= current_time <= afternoon_end)
    )


# ==================== å¯åŠ¨ä»»åŠ¡ ====================

class StartupTasks:
    """å¯åŠ¨æ—¶æ‰§è¡Œçš„ä»»åŠ¡"""
    
    @staticmethod
    async def execute(init_mode: str = "skip", calculate_signals: bool = False):
        """
        æ‰§è¡Œå¯åŠ¨ä»»åŠ¡
        
        Args:
            init_mode: åˆå§‹åŒ–æ¨¡å¼
                - skip: è·³è¿‡åˆå§‹åŒ–
                - init: å…¨é‡åˆå§‹åŒ–
            calculate_signals: æ˜¯å¦è®¡ç®—ä¿¡å·
        """
        logger.info(f"========== å¼€å§‹æ‰§è¡Œå¯åŠ¨ä»»åŠ¡ ==========")
        logger.info(f"åˆå§‹åŒ–æ¨¡å¼: {init_mode}")
        logger.info(f"æ˜¯å¦è®¡ç®—ä¿¡å·: {calculate_signals}")
        
        start_time = datetime.now()
        
        try:
            # 1. è·å–æœ‰æ•ˆè‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆå¿…é¡»æ‰§è¡Œï¼‰
            await StartupTasks.task_get_valid_stock_codes()
            
            # 2. æ ¹æ®åˆå§‹åŒ–æ¨¡å¼æ‰§è¡Œç›¸åº”æ“ä½œ
            if init_mode == "init":
                await StartupTasks.task_init()
            elif init_mode == "skip":
                logger.info("è·³è¿‡æ•°æ®åˆå§‹åŒ–")
            else:
                logger.warning(f"æœªçŸ¥çš„åˆå§‹åŒ–æ¨¡å¼: {init_mode}ï¼Œè·³è¿‡åˆå§‹åŒ–")
            
            # 3. çˆ¬å–æ–°é—»ï¼ˆå¿…é¡»æ‰§è¡Œï¼‰
            await StartupTasks.task_crawl_news()
            
            # 4. æ ¹æ®é…ç½®å†³å®šæ˜¯å¦è®¡ç®—ä¿¡å·
            if calculate_signals:
                await StartupTasks.task_calculate_signals()
            else:
                logger.info("è·³è¿‡ä¿¡å·è®¡ç®—")
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f"========== å¯åŠ¨ä»»åŠ¡å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’ ==========")
            
            add_job_log(
                'startup',
                'success',
                f'å¯åŠ¨ä»»åŠ¡å®Œæˆï¼Œæ¨¡å¼={init_mode}ï¼Œè®¡ç®—ä¿¡å·={calculate_signals}',
                elapsed_seconds=round(elapsed, 2)
            )
            
        except Exception as e:
            logger.error(f"å¯åŠ¨ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            add_job_log('startup', 'error', f'å¯åŠ¨ä»»åŠ¡å¤±è´¥: {str(e)}')
    
    @staticmethod
    async def task_get_valid_stock_codes():
        """ä»»åŠ¡ï¼šè·å–æœ‰æ•ˆè‚¡ç¥¨ä»£ç åˆ—è¡¨"""
        logger.info(">>> æ‰§è¡Œä»»åŠ¡: è·å–æœ‰æ•ˆè‚¡ç¥¨ä»£ç åˆ—è¡¨")
        start_time = datetime.now()
        
        try:
            stock_list = await stock_atomic_service.get_valid_stock_codes(include_etf=True)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f">>> ä»»åŠ¡å®Œæˆ: è·å–åˆ° {len(stock_list)} åªè‚¡ç¥¨ï¼ˆå«ETFï¼‰ï¼Œè€—æ—¶ {elapsed:.2f}ç§’")
            
            add_job_log(
                'get_stock_codes',
                'success',
                f'è·å–è‚¡ç¥¨ä»£ç æˆåŠŸï¼Œå…± {len(stock_list)} åª',
                count=len(stock_list),
                elapsed_seconds=round(elapsed, 2)
            )
            
        except Exception as e:
            logger.error(f">>> ä»»åŠ¡å¤±è´¥: è·å–è‚¡ç¥¨ä»£ç å¤±è´¥: {e}")
            add_job_log('get_stock_codes', 'error', f'è·å–è‚¡ç¥¨ä»£ç å¤±è´¥: {str(e)}')
            raise
    
    @staticmethod
    async def task_init():
        """ä»»åŠ¡ï¼šå…¨é‡åˆå§‹åŒ–"""
        logger.info(">>> æ‰§è¡Œä»»åŠ¡: å…¨é‡åˆå§‹åŒ–æ‰€æœ‰è‚¡ç¥¨æ•°æ®")
        start_time = datetime.now()
        
        try:
            # ğŸ”§ ä¼˜åŒ–ï¼šé™ä½å¹¶å‘ä»¥é¿å…è§¦å‘APIé™åˆ¶
            # Tushareé™åˆ¶: æ¯åˆ†é’Ÿ500æ¬¡ï¼Œæ¯å¤©20000æ¬¡
            # æ¨èé…ç½®: batch_size=30, max_concurrent=5 â†’ ~300æ¬¡/åˆ†é’Ÿ
            result = await stock_atomic_service.full_update_all_stocks(
                days=180,
                batch_size=30,       # ä»50é™ä½è‡³30ï¼Œå‡å°‘å•æ‰¹æ¬¡å‹åŠ›
                max_concurrent=5     # ä»10é™ä½è‡³5ï¼Œå‡å°‘APIé™æµ
            )
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(
                f">>> ä»»åŠ¡å®Œæˆ: å…¨é‡åˆå§‹åŒ–å®Œæˆï¼Œ"
                f"æˆåŠŸ={result['success_count']}, "
                f"å¤±è´¥={result['failed_count']}, "
                f"è€—æ—¶ {elapsed:.2f}ç§’"
            )
            
            add_job_log(
                'init',
                'success',
                f"å…¨é‡åˆå§‹åŒ–å®Œæˆï¼ŒæˆåŠŸ={result['success_count']}, å¤±è´¥={result['failed_count']}",
                **result
            )
            
        except Exception as e:
            logger.error(f">>> ä»»åŠ¡å¤±è´¥: å…¨é‡åˆå§‹åŒ–å¤±è´¥: {e}")
            add_job_log('init', 'error', f'å…¨é‡åˆå§‹åŒ–å¤±è´¥: {str(e)}')
            raise
    
    @staticmethod
    async def task_crawl_news():
        """ä»»åŠ¡ï¼šçˆ¬å–æ–°é—»"""
        start_time = datetime.now()
        
        try:
            result = await stock_atomic_service.crawl_news(days=1)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            add_job_log(
                'crawl_news',
                'success' if result.get('success') else 'warning',
                f"çˆ¬å–æ–°é—»å®Œæˆï¼Œå…± {result.get('news_count', 0)} æ¡",
                **result
            )
            
        except Exception as e:
            logger.error(f">>> ä»»åŠ¡å¤±è´¥: çˆ¬å–æ–°é—»å¤±è´¥: {e}")
            add_job_log('crawl_news', 'error', f'çˆ¬å–æ–°é—»å¤±è´¥: {str(e)}')
            # æ–°é—»çˆ¬å–å¤±è´¥ä¸å½±å“å¯åŠ¨ï¼Œä¸æŠ›å‡ºå¼‚å¸¸
    
    @staticmethod
    async def task_calculate_signals():
        """ä»»åŠ¡ï¼šè®¡ç®—ç­–ç•¥ä¿¡å·"""
        logger.info(">>> æ‰§è¡Œä»»åŠ¡: è®¡ç®—ç­–ç•¥ä¿¡å·")
        start_time = datetime.now()
        
        try:
            result = await stock_atomic_service.calculate_strategy_signals(
                force_recalculate=True
            )
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f">>> ä»»åŠ¡å®Œæˆ: è®¡ç®—ä¿¡å·å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’")
            
            # ä»resultä¸­æ’é™¤statuså­—æ®µï¼Œé¿å…å‚æ•°å†²çª
            result_data = {k: v for k, v in result.items() if k != 'status'}
            add_job_log(
                'calculate_signals',
                'success' if result.get('success') or result.get('status') == 'success' else 'error',
                f"è®¡ç®—ä¿¡å·å®Œæˆ",
                **result_data
            )
            
        except Exception as e:
            logger.error(f">>> ä»»åŠ¡å¤±è´¥: è®¡ç®—ä¿¡å·å¤±è´¥: {e}")
            add_job_log('calculate_signals', 'error', f'è®¡ç®—ä¿¡å·å¤±è´¥: {str(e)}')
            # ä¿¡å·è®¡ç®—å¤±è´¥ä¸å½±å“å¯åŠ¨ï¼Œä¸æŠ›å‡ºå¼‚å¸¸


# ==================== è¿è¡Œæ—¶ä»»åŠ¡ ====================

class RuntimeTasks:
    """è¿è¡Œæ—¶å®šæ—¶ä»»åŠ¡"""
    
    @staticmethod
    def job_realtime_update():
        """å®šæ—¶ä»»åŠ¡ï¼šå®æ—¶æ›´æ–°æ‰€æœ‰è‚¡ç¥¨æ•°æ®ï¼ˆä»…äº¤æ˜“æ—¶é—´ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´
        if not is_trading_time():
            logger.debug("éäº¤æ˜“æ—¶é—´ï¼Œè·³è¿‡å®æ—¶æ•°æ®æ›´æ–°")
            return
        
        # é˜²æ­¢é‡å¤æ‰§è¡Œ
        if not _task_locks['realtime_update'].acquire(blocking=False):
            logger.warning("å®æ—¶æ•°æ®æ›´æ–°ä»»åŠ¡æ­£åœ¨æ‰§è¡Œä¸­ï¼Œè·³è¿‡æœ¬æ¬¡")
            return
        
        try:
            logger.info("========== å¼€å§‹å®æ—¶æ•°æ®æ›´æ–° ==========")
            start_time = datetime.now()
            
            # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                # 1. æ›´æ–°è‚¡ç¥¨æ•°æ®
                result = loop.run_until_complete(
                    stock_atomic_service.realtime_update_all_stocks()
                )
                
                elapsed = (datetime.now() - start_time).total_seconds()
                logger.info(f"========== å®æ—¶æ•°æ®æ›´æ–°å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’ ==========")
                
                # result ä¸­å·²åŒ…å« message å’Œ elapsed_secondsï¼Œç›´æ¥ä½¿ç”¨
                add_job_log(
                    'realtime_update',
                    'success',
                    result.get('message', 'å®æ—¶æ•°æ®æ›´æ–°å®Œæˆ'),
                    **{k: v for k, v in result.items() if k != 'message'}  # æ’é™¤messageé¿å…é‡å¤
                )
                
                # 2. æ¨é€ä»·æ ¼æ›´æ–°åˆ°WebSocketå®¢æˆ·ç«¯
                try:
                    from app.services.websocket import price_publisher
                    
                    # å¹¿æ’­æ‰€æœ‰æ´»è·ƒç­–ç•¥çš„ä»·æ ¼æ›´æ–°
                    client_count = loop.run_until_complete(
                        price_publisher.broadcast_all_prices()
                    )
                    
                    if client_count > 0:
                        logger.info(f"ä»·æ ¼æ›´æ–°å·²æ¨é€åˆ° {client_count} ä¸ªWebSocketå®¢æˆ·ç«¯")
                    else:
                        logger.debug("æ²¡æœ‰æ´»è·ƒçš„WebSocketå®¢æˆ·ç«¯ï¼Œè·³è¿‡ä»·æ ¼æ¨é€")
                        
                except Exception as e:
                    logger.error(f"WebSocketä»·æ ¼æ¨é€å¤±è´¥: {e}")
                    # ä»·æ ¼æ¨é€å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                
                # æ³¨æ„ï¼šå®æ—¶æ›´æ–°å’Œä¿¡å·è®¡ç®—å·²åˆ†ç¦»ï¼Œä¸å†è‡ªåŠ¨è§¦å‘ä¿¡å·è®¡ç®—
                # ä¿¡å·è®¡ç®—ç”±ç‹¬ç«‹çš„å®šæ—¶ä»»åŠ¡è§¦å‘
                
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"å®æ—¶æ•°æ®æ›´æ–°å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            add_job_log(
                'realtime_update',
                'error',
                f'å®æ—¶æ•°æ®æ›´æ–°å¼‚å¸¸: {str(e)}'  # ä½¿ç”¨ä¸åŒçš„æ¶ˆæ¯ï¼Œé¿å…ä¸resultä¸­çš„messageå†²çª
            )
        finally:
            _task_locks['realtime_update'].release()
    
    @staticmethod
    def job_calculate_signals():
        """
        å®šæ—¶ä»»åŠ¡ï¼šè®¡ç®—ç­–ç•¥ä¿¡å·ï¼ˆç›˜ä¸­ä»…è®¡ç®—è‚¡ç¥¨ä¿¡å·ï¼Œä¸è®¡ç®—ETFï¼‰
        
        æ³¨æ„ï¼šæ­¤ä»»åŠ¡åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œï¼Œä¸ä¼šé˜»å¡APIè¯·æ±‚
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´
        if not is_trading_time():
            logger.debug("éäº¤æ˜“æ—¶é—´ï¼Œè·³è¿‡ä¿¡å·è®¡ç®—")
            return
        
        # é˜²æ­¢é‡å¤æ‰§è¡Œ
        if not _task_locks['signal_calculation'].acquire(blocking=False):
            logger.warning("ä¿¡å·è®¡ç®—ä»»åŠ¡æ­£åœ¨æ‰§è¡Œä¸­ï¼Œè·³è¿‡æœ¬æ¬¡")
            return
        
        try:
            logger.info("========== å¼€å§‹è®¡ç®—ç­–ç•¥ä¿¡å·ï¼ˆä»…è‚¡ç¥¨ï¼‰ ==========")
            start_time = datetime.now()
            
            # ä½¿ç”¨ç‹¬ç«‹çš„äº‹ä»¶å¾ªç¯ï¼Œåœ¨å½“å‰çº¿ç¨‹ä¸­æ‰§è¡Œ
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                # ä¸è®¾ç½®è¶…æ—¶ï¼Œè®©ä¿¡å·è®¡ç®—è‡ªç„¶å®Œæˆ
                result = loop.run_until_complete(
                    stock_atomic_service.calculate_strategy_signals(
                        force_recalculate=True,  # ç›˜ä¸­ä¹Ÿéœ€è¦å¼ºåˆ¶é‡ç®—ï¼Œç¡®ä¿ä¿¡å·æœ€æ–°
                        stock_only=True  # ç›˜ä¸­ä»…è®¡ç®—è‚¡ç¥¨ä¿¡å·
                    )
                )
                
                elapsed = (datetime.now() - start_time).total_seconds()
                logger.info(f"========== ä¿¡å·è®¡ç®—å®Œæˆï¼ˆä»…è‚¡ç¥¨ï¼‰ï¼Œè€—æ—¶ {elapsed:.2f}ç§’ ==========")
                
                # ä»resultä¸­æ’é™¤statuså­—æ®µï¼Œé¿å…å‚æ•°å†²çª
                result_data = {k: v for k, v in result.items() if k != 'status'}
                add_job_log(
                    'signal_calculation',
                    'success' if result.get('success') or result.get('status') == 'success' else 'warning',
                    f'ä¿¡å·è®¡ç®—å®Œæˆ',
                    **result_data
                )
                
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"ä¿¡å·è®¡ç®—å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            add_job_log(
                'signal_calculation',
                'error',
                f'ä¿¡å·è®¡ç®—å¤±è´¥: {str(e)}'
            )
        finally:
            _task_locks['signal_calculation'].release()
    
    @staticmethod
    def job_crawl_news():
        """å®šæ—¶ä»»åŠ¡ï¼šçˆ¬å–æ–°é—»"""
        start_time = datetime.now()
        
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                result = loop.run_until_complete(
                    stock_atomic_service.crawl_news(days=1)
                )
                
                elapsed = (datetime.now() - start_time).total_seconds()
                
                add_job_log(
                    'crawl_news',
                    'success' if result.get('success') else 'warning',
                    f"çˆ¬å–æ–°é—»å®Œæˆï¼Œå…± {result.get('news_count', 0)} æ¡",
                    **result
                )
                
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"æ–°é—»çˆ¬å–å¤±è´¥: {e}")
            add_job_log('crawl_news', 'error', f'æ–°é—»çˆ¬å–å¤±è´¥: {str(e)}')
    
    @staticmethod
    def job_full_update_and_calculate():
        """å®šæ—¶ä»»åŠ¡ï¼šå…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·"""
        # é˜²æ­¢é‡å¤æ‰§è¡Œ
        if not _task_locks['full_update'].acquire(blocking=False):
            logger.warning("å…¨é‡æ›´æ–°ä»»åŠ¡æ­£åœ¨æ‰§è¡Œä¸­ï¼Œè·³è¿‡æœ¬æ¬¡")
            return
        
        try:
            logger.info("========== å¼€å§‹å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å· ==========")
            start_time = datetime.now()
            
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                # 1. å…¨é‡æ›´æ–°ï¼ˆåŒ…å«è‚¡ç¥¨å’ŒETFï¼Œé™ä½å¹¶å‘æ•°ï¼‰
                update_result = loop.run_until_complete(
                    stock_atomic_service.full_update_all_stocks(
                        days=180,
                        batch_size=50,
                        max_concurrent=5  # é™ä½å¹¶å‘æ•°ï¼Œå‡å°‘APIé™æµ
                    )
                )
                
                logger.info(f"å…¨é‡æ›´æ–°å®Œæˆï¼ˆåŒ…å«ETFï¼‰: æˆåŠŸ={update_result['success_count']}, å¤±è´¥={update_result['failed_count']}")
                
                # 2. è®¡ç®—ä¿¡å·ï¼ˆåŒ…å«è‚¡ç¥¨å’ŒETFï¼‰
                signal_result = loop.run_until_complete(
                    stock_atomic_service.calculate_strategy_signals(
                        force_recalculate=True,
                        stock_only=False  # å…¨é‡æ›´æ–°åŒ…å«ETFä¿¡å·
                    )
                )
                
                elapsed = (datetime.now() - start_time).total_seconds()
                logger.info(f"========== å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’ ==========")
                
                add_job_log(
                    'full_update_and_calculate',
                    'success',
                    f"å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·å®Œæˆ",
                    elapsed_seconds=round(elapsed, 2),
                    update_result=update_result,
                    signal_result=signal_result
                )
                
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            add_job_log('full_update_and_calculate', 'error', f'å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·å¤±è´¥: {str(e)}')
        finally:
            _task_locks['full_update'].release()
    
    @staticmethod
    def job_cleanup_charts():
        """å®šæ—¶ä»»åŠ¡ï¼šæ¸…ç†å›¾è¡¨æ–‡ä»¶"""
        logger.info("========== å¼€å§‹æ¸…ç†å›¾è¡¨æ–‡ä»¶ ==========")
        start_time = datetime.now()
        
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                result = loop.run_until_complete(
                    stock_atomic_service.cleanup_chart_files()
                )
                
                elapsed = (datetime.now() - start_time).total_seconds()
                logger.info(f"========== å›¾è¡¨æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’ ==========")
                
                add_job_log(
                    'cleanup_charts',
                    'success',
                    f"æ¸…ç†å›¾è¡¨æ–‡ä»¶å®Œæˆï¼Œåˆ é™¤ {result.get('deleted_count', 0)} ä¸ªæ–‡ä»¶",
                    **result
                )
                
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"æ¸…ç†å›¾è¡¨æ–‡ä»¶å¤±è´¥: {e}")
            add_job_log('cleanup_charts', 'error', f'æ¸…ç†å›¾è¡¨æ–‡ä»¶å¤±è´¥: {str(e)}')
    
    @staticmethod
    def job_websocket_price_push():
        """å®šæ—¶ä»»åŠ¡ï¼šWebSocketä»·æ ¼æ¨é€ï¼ˆä»…åœ¨äº¤æ˜“æ—¶é—´ï¼‰"""
        try:
            from app.services.websocket import price_publisher, connection_manager
            
            # æ£€æŸ¥æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´
            if not is_trading_time():
                logger.debug("WebSocketä»·æ ¼æ¨é€: éäº¤æ˜“æ—¶é—´ï¼Œè·³è¿‡")
                return
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒè¿æ¥
            connection_count = connection_manager.get_connection_count()
            if connection_count == 0:
                logger.debug("WebSocketä»·æ ¼æ¨é€: æ²¡æœ‰æ´»è·ƒè¿æ¥ï¼Œè·³è¿‡")
                return  # æ²¡æœ‰è¿æ¥ï¼Œè·³è¿‡
            
            logger.debug(f"WebSocketä»·æ ¼æ¨é€: æ´»è·ƒè¿æ¥æ•° {connection_count}")
            
            # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                client_count = loop.run_until_complete(
                    price_publisher.broadcast_all_prices()
                )
                
                if client_count > 0:
                    logger.debug(f"ä»·æ ¼æ¨é€å®Œæˆ: {client_count} ä¸ªå®¢æˆ·ç«¯")
                    
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"WebSocketä»·æ ¼æ¨é€å¤±è´¥: {e}")


# ==================== è°ƒåº¦å™¨ç®¡ç† ====================

def start_stock_scheduler(init_mode: str = "skip", calculate_signals: bool = False):
    """
    å¯åŠ¨è‚¡ç¥¨è°ƒåº¦å™¨
    
    Args:
        init_mode: åˆå§‹åŒ–æ¨¡å¼
            - skip: è·³è¿‡åˆå§‹åŒ–
            - init: å…¨é‡åˆå§‹åŒ–
        calculate_signals: æ˜¯å¦åœ¨å¯åŠ¨æ—¶è®¡ç®—ä¿¡å·
    """
    global scheduler
    
    if scheduler is not None and scheduler.running:
        logger.warning("è‚¡ç¥¨è°ƒåº¦å™¨å·²åœ¨è¿è¡Œä¸­")
        return
    
    logger.info("========== å¯åŠ¨è‚¡ç¥¨è°ƒåº¦å™¨ ==========")
    logger.info(f"åˆå§‹åŒ–æ¨¡å¼: {init_mode}")
    logger.info(f"å¯åŠ¨æ—¶è®¡ç®—ä¿¡å·: {calculate_signals}")
    
    # 1. åˆ›å»ºè°ƒåº¦å™¨ï¼ˆå¢åŠ çº¿ç¨‹æ± å¤§å°ï¼Œé¿å…é•¿æ—¶é—´ä»»åŠ¡é˜»å¡å…¶ä»–ä»»åŠ¡ï¼‰
    from apscheduler.executors.pool import ThreadPoolExecutor
    executors = {
        'default': ThreadPoolExecutor(max_workers=10),  # å¢åŠ çº¿ç¨‹æ± å¤§å°
    }
    job_defaults = {
        'coalesce': True,  # åˆå¹¶é”™è¿‡çš„ä»»åŠ¡
        'max_instances': 1,  # æ¯ä¸ªä»»åŠ¡æœ€å¤šåŒæ—¶è¿è¡Œ1ä¸ªå®ä¾‹
        'misfire_grace_time': 60,  # é”™è¿‡æ‰§è¡Œæ—¶é—´å60ç§’å†…ä»å¯æ‰§è¡Œ
    }
    scheduler = BackgroundScheduler(
        timezone='Asia/Shanghai',
        executors=executors,
        job_defaults=job_defaults
    )
    
    # 3. æ·»åŠ è¿è¡Œæ—¶ä»»åŠ¡
    
    # å®æ—¶æ•°æ®æ›´æ–°ï¼šæ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡ENABLE_REALTIME_UPDATEæ§åˆ¶ï¼‰
    if settings.ENABLE_REALTIME_UPDATE:
        realtime_interval_seconds = settings.REALTIME_UPDATE_INTERVAL
        realtime_interval_minutes = realtime_interval_seconds / 60
        scheduler.add_job(
            func=RuntimeTasks.job_realtime_update,
            trigger=IntervalTrigger(seconds=realtime_interval_seconds),
            id='realtime_update',
            name='å®æ—¶æ•°æ®æ›´æ–°',
            replace_existing=True
        )
        logger.info(f"âœ… å®æ—¶æ•°æ®æ›´æ–°ä»»åŠ¡å·²å¯ç”¨ï¼Œé—´éš”: {realtime_interval_seconds}ç§’")
    else:
        logger.info(f"âš ï¸  å®æ—¶æ•°æ®æ›´æ–°ä»»åŠ¡å·²ç¦ç”¨ï¼ˆENABLE_REALTIME_UPDATE=falseï¼‰")
    
    # ä¿¡å·è®¡ç®—ï¼šå›ºå®šæ—¶é—´ç‚¹è§¦å‘ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    # 9:30, 9:50, 10:10, 10:30, 10:50, 11:10, 11:30
    # 13:00, 13:20, 13:40, 14:00, 14:20, 14:40, 15:00, 15:20
    from datetime import datetime
    now = datetime.now()
    
    # å¦‚æœæ˜¯äº¤æ˜“æ—¶é—´ä¸”å¯åŠ¨æ—¶è®¡ç®—ä¿¡å·ï¼Œç«‹å³æ‰§è¡Œä¸€æ¬¡
    if is_trading_time() and calculate_signals:
        logger.info("å¯åŠ¨æ—¶ç«‹å³æ‰§è¡Œä¸€æ¬¡ä¿¡å·è®¡ç®—ï¼Œç¡®ä¿æœ‰æœ€æ–°ä¿¡å·...")
        import threading
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼Œä¸é˜»å¡å¯åŠ¨
        threading.Thread(target=RuntimeTasks.job_calculate_signals, daemon=True).start()
    
    # ä¿¡å·è®¡ç®—ï¼šä»9:30å¼€å§‹ï¼Œæ¯20åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
    # 9:30, 9:50, 10:10, 10:30, 10:50, 11:10, 13:10, 13:30, 13:50, 14:10, 14:30, 14:50, 15:10
    scheduler.add_job(
        func=RuntimeTasks.job_calculate_signals,
        trigger=CronTrigger(
            day_of_week='mon-fri',
            hour='9',
            minute='30,50'
        ),
        id='signal_calculation_morning_1',
        name='ç­–ç•¥ä¿¡å·è®¡ç®—ï¼ˆ9:30-9:50ï¼‰',
        replace_existing=True,
        misfire_grace_time=300
    )
    scheduler.add_job(
        func=RuntimeTasks.job_calculate_signals,
        trigger=CronTrigger(
            day_of_week='mon-fri',
            hour='10-11',
            minute='10,30,50'
        ),
        id='signal_calculation_morning_2',
        name='ç­–ç•¥ä¿¡å·è®¡ç®—ï¼ˆ10:10-11:50ï¼‰',
        replace_existing=True,
        misfire_grace_time=300
    )
    scheduler.add_job(
        func=RuntimeTasks.job_calculate_signals,
        trigger=CronTrigger(
            day_of_week='mon-fri',
            hour='13-14',
            minute='10,30,50'
        ),
        id='signal_calculation_afternoon_1',
        name='ç­–ç•¥ä¿¡å·è®¡ç®—ï¼ˆ13:10-14:50ï¼‰',
        replace_existing=True,
        misfire_grace_time=300
    )
    scheduler.add_job(
        func=RuntimeTasks.job_calculate_signals,
        trigger=CronTrigger(
            day_of_week='mon-fri',
            hour='15',
            minute='10'
        ),
        id='signal_calculation_afternoon_2',
        name='ç­–ç•¥ä¿¡å·è®¡ç®—ï¼ˆ15:10ï¼‰',
        replace_existing=True,
        misfire_grace_time=300
    )
    logger.info("ä¿¡å·è®¡ç®—ä»»åŠ¡å·²æ·»åŠ ï¼Œä»9:30å¼€å§‹ï¼Œæ¯20åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡ï¼ˆå…±13æ¬¡ï¼‰")
    
    # æ–°é—»çˆ¬å–ï¼šæ¯2å°æ—¶æ‰§è¡Œä¸€æ¬¡
    scheduler.add_job(
        func=RuntimeTasks.job_crawl_news,
        trigger=IntervalTrigger(hours=2),
        id='crawl_news',
        name='æ–°é—»çˆ¬å–',
        replace_existing=True
    )
    
    # å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·ï¼šæ¯ä¸ªäº¤æ˜“æ—¥17:35æ‰§è¡Œä¸€æ¬¡
    scheduler.add_job(
        func=RuntimeTasks.job_full_update_and_calculate,
        trigger=CronTrigger(hour=17, minute=35, day_of_week='mon-fri'),
        id='full_update_and_calculate',
        name='å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·',
        replace_existing=True
    )
    
    # å›¾è¡¨æ–‡ä»¶æ¸…ç†ï¼šæ¯å¤©00:00æ‰§è¡Œä¸€æ¬¡
    scheduler.add_job(
        func=RuntimeTasks.job_cleanup_charts,
        trigger=CronTrigger(hour=0, minute=0),
        id='cleanup_charts',
        name='å›¾è¡¨æ–‡ä»¶æ¸…ç†',
        replace_existing=True
    )
    
    # WebSocketä»·æ ¼æ¨é€ï¼šæ¯5ç§’æ‰§è¡Œä¸€æ¬¡ï¼ˆä»…åœ¨äº¤æ˜“æ—¶é—´ï¼‰
    scheduler.add_job(
        func=RuntimeTasks.job_websocket_price_push,
        trigger=IntervalTrigger(seconds=5),
        id='websocket_price_push',
        name='WebSocketä»·æ ¼æ¨é€',
        replace_existing=True
    )
    logger.info("WebSocketä»·æ ¼æ¨é€ä»»åŠ¡å·²æ·»åŠ ï¼Œé—´éš”: 5ç§’ï¼ˆä»…äº¤æ˜“æ—¶é—´ï¼‰")
    
    # 4. å¯åŠ¨è°ƒåº¦å™¨
    scheduler.start()
    logger.info("========== è‚¡ç¥¨è°ƒåº¦å™¨å¯åŠ¨å®Œæˆ ==========")
    logger.info("å®šæ—¶ä»»åŠ¡:")
    logger.info(f"  - å®æ—¶æ•°æ®æ›´æ–°: æ¯{realtime_interval}åˆ†é’Ÿï¼ˆäº¤æ˜“æ—¶é—´ï¼‰")
    logger.info("  - ç­–ç•¥ä¿¡å·è®¡ç®—: å›ºå®šæ—¶é—´ç‚¹ï¼ˆ9:30/9:50/10:10/10:30/10:50/11:10/11:30/13:00/13:20/13:40/14:00/14:20/14:40/15:00/15:20ï¼Œç‹¬ç«‹ä»»åŠ¡ï¼‰")
    logger.info("  - WebSocketä»·æ ¼æ¨é€: æ¯5ç§’ï¼ˆä»…äº¤æ˜“æ—¶é—´ï¼‰")
    logger.info("  - æ–°é—»çˆ¬å–: æ¯2å°æ—¶")
    logger.info("  - å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·: æ¯ä¸ªäº¤æ˜“æ—¥17:35")
    logger.info("  - å›¾è¡¨æ–‡ä»¶æ¸…ç†: æ¯å¤©00:00")
    
    # 5. åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå¯åŠ¨ä»»åŠ¡ï¼ˆä¸é˜»å¡è°ƒåº¦å™¨å’ŒAPIï¼‰
    def run_startup_tasks():
        """åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå¯åŠ¨ä»»åŠ¡"""
        try:
            logger.info("========== å¼€å§‹æ‰§è¡Œå¯åŠ¨ä»»åŠ¡ï¼ˆåå°ï¼‰ ==========")
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(
                    StartupTasks.execute(init_mode=init_mode, calculate_signals=calculate_signals)
                )
                logger.info("========== å¯åŠ¨ä»»åŠ¡æ‰§è¡Œå®Œæˆ ==========")
            finally:
                loop.close()
        except Exception as e:
            logger.error(f"å¯åŠ¨ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    # å¯åŠ¨åå°ä»»åŠ¡
    import threading
    threading.Thread(target=run_startup_tasks, daemon=True, name="StartupTasksThread").start()
    logger.info("å¯åŠ¨ä»»åŠ¡å·²åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼Œä¸é˜»å¡APIæœåŠ¡")


def stop_stock_scheduler():
    """åœæ­¢è‚¡ç¥¨è°ƒåº¦å™¨"""
    global scheduler
    
    if scheduler is not None and scheduler.running:
        scheduler.shutdown()
        scheduler = None
        logger.info("è‚¡ç¥¨è°ƒåº¦å™¨å·²åœæ­¢")
    else:
        logger.warning("è‚¡ç¥¨è°ƒåº¦å™¨æœªè¿è¡Œ")


def get_stock_scheduler_status() -> Dict[str, Any]:
    """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
    global scheduler, job_logs
    
    if scheduler is None:
        return {
            'running': False,
            'message': 'è°ƒåº¦å™¨æœªå¯åŠ¨'
        }
    
    jobs_info = []
    for job in scheduler.get_jobs():
        jobs_info.append({
            'id': job.id,
            'name': job.name,
            'next_run_time': job.next_run_time.isoformat() if job.next_run_time else None
        })
    
    return {
        'running': scheduler.running,
        'jobs': jobs_info,
        'recent_logs': job_logs[:10],
        'is_trading_time': is_trading_time()
    }

