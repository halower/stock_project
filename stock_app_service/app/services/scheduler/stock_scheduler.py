# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ•°æ®è°ƒåº¦å™¨ V2 - é‡æ„ç‰ˆ
æŒ‰ç…§DDDåŸåˆ™é‡æ–°ç»„ç»‡ï¼Œåˆ†ç¦»å¯åŠ¨ä»»åŠ¡å’Œè¿è¡Œæ—¶ä»»åŠ¡
"""

import asyncio
import threading
from datetime import datetime, time
from typing import Dict, Any
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger

from app.core.logging import logger
from app.core.config import settings
from app.db.session import RedisCache
from app.services.stock.stock_atomic_service import stock_atomic_service

# Redisç¼“å­˜å®¢æˆ·ç«¯
redis_cache = RedisCache()

# è°ƒåº¦å™¨å®ä¾‹
scheduler = None
job_logs = []  # å­˜å‚¨æœ€è¿‘çš„ä»»åŠ¡æ‰§è¡Œæ—¥å¿—

# ä»»åŠ¡æ‰§è¡Œé”
_task_locks = {
    'realtime_update': threading.Lock(),
    'signal_calculation': threading.Lock(),
    'full_update': threading.Lock(),
}


def add_job_log(job_type: str, status: str, message: str, **kwargs):
    """æ·»åŠ ä»»åŠ¡æ‰§è¡Œæ—¥å¿—"""
    log_entry = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'job_type': job_type,
        'status': status,
        'message': message,
        **kwargs
    }
    
    # å†…å­˜æ—¥å¿—ï¼ˆæœ€è¿‘10æ¡ï¼‰
    global job_logs
    job_logs.insert(0, log_entry)
    job_logs = job_logs[:10]
    
    # Redisæ—¥å¿—ï¼ˆæœ€è¿‘20æ¡ï¼‰
    redis_logs = redis_cache.get_cache('stock:scheduler:log') or []
    redis_logs.insert(0, log_entry)
    redis_logs = redis_logs[:20]
    redis_cache.set_cache('stock:scheduler:log', redis_logs, ttl=86400)
    
    logger.info(f"[{job_type}] {message}")


def is_trading_time() -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´ï¼ˆåŒ…æ‹¬ç›˜å30åˆ†é’Ÿï¼‰
    
    äº¤æ˜“æ—¶é—´: 9:30-11:30, 13:00-15:00
    ç›˜åæ—¶é—´: 15:00-15:30
    """
    now = datetime.now()
    
    # å‘¨æœ«ä¸äº¤æ˜“
    if now.weekday() >= 5:
        return False
    
    current_time = now.time()
    
    # ä¸Šåˆäº¤æ˜“æ—¶é—´
    morning_start = time(9, 30)
    morning_end = time(11, 30)
    
    # ä¸‹åˆäº¤æ˜“æ—¶é—´
    afternoon_start = time(13, 0)
    afternoon_end = time(15, 0)
    
    # ç›˜åæ—¶é—´ï¼ˆ15:00-15:30ï¼‰
    after_close_end = time(15, 30)
    
    return (
        (morning_start <= current_time <= morning_end) or
        (afternoon_start <= current_time <= after_close_end)
    )


# ==================== å¯åŠ¨ä»»åŠ¡ ====================

class StartupTasks:
    """å¯åŠ¨æ—¶æ‰§è¡Œçš„ä»»åŠ¡"""
    
    @staticmethod
    async def execute(init_mode: str = "skip", calculate_signals: bool = False):
        """
        æ‰§è¡Œå¯åŠ¨ä»»åŠ¡
        
        Args:
            init_mode: åˆå§‹åŒ–æ¨¡å¼
                - skip: è·³è¿‡åˆå§‹åŒ–
                - full_init: å…¨é‡åˆå§‹åŒ–
            calculate_signals: æ˜¯å¦è®¡ç®—ä¿¡å·
        """
        logger.info(f"========== å¼€å§‹æ‰§è¡Œå¯åŠ¨ä»»åŠ¡ ==========")
        logger.info(f"åˆå§‹åŒ–æ¨¡å¼: {init_mode}")
        logger.info(f"æ˜¯å¦è®¡ç®—ä¿¡å·: {calculate_signals}")
        
        start_time = datetime.now()
        
        try:
            # 1. è·å–æœ‰æ•ˆè‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆå¿…é¡»æ‰§è¡Œï¼‰
            await StartupTasks.task_get_valid_stock_codes()
            
            # 2. æ ¹æ®åˆå§‹åŒ–æ¨¡å¼æ‰§è¡Œç›¸åº”æ“ä½œ
            if init_mode == "full_init":
                await StartupTasks.task_full_init()
            elif init_mode == "skip":
                logger.info("è·³è¿‡æ•°æ®åˆå§‹åŒ–")
            else:
                logger.warning(f"æœªçŸ¥çš„åˆå§‹åŒ–æ¨¡å¼: {init_mode}ï¼Œè·³è¿‡åˆå§‹åŒ–")
            
            # 3. çˆ¬å–æ–°é—»ï¼ˆå¿…é¡»æ‰§è¡Œï¼‰
            await StartupTasks.task_crawl_news()
            
            # 4. æ ¹æ®é…ç½®å†³å®šæ˜¯å¦è®¡ç®—ä¿¡å·
            if calculate_signals:
                await StartupTasks.task_calculate_signals()
            else:
                logger.info("è·³è¿‡ä¿¡å·è®¡ç®—")
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f"========== å¯åŠ¨ä»»åŠ¡å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’ ==========")
            
            add_job_log(
                'startup',
                'success',
                f'å¯åŠ¨ä»»åŠ¡å®Œæˆï¼Œæ¨¡å¼={init_mode}ï¼Œè®¡ç®—ä¿¡å·={calculate_signals}',
                elapsed_seconds=round(elapsed, 2)
            )
            
        except Exception as e:
            logger.error(f"å¯åŠ¨ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            add_job_log('startup', 'error', f'å¯åŠ¨ä»»åŠ¡å¤±è´¥: {str(e)}')
    
    @staticmethod
    async def task_get_valid_stock_codes():
        """ä»»åŠ¡ï¼šè·å–æœ‰æ•ˆè‚¡ç¥¨ä»£ç åˆ—è¡¨"""
        logger.info(">>> æ‰§è¡Œä»»åŠ¡: è·å–æœ‰æ•ˆè‚¡ç¥¨ä»£ç åˆ—è¡¨")
        start_time = datetime.now()
        
        try:
            stock_list = await stock_atomic_service.get_valid_stock_codes(include_etf=True)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f">>> ä»»åŠ¡å®Œæˆ: è·å–åˆ° {len(stock_list)} åªè‚¡ç¥¨ï¼ˆå«ETFï¼‰ï¼Œè€—æ—¶ {elapsed:.2f}ç§’")
            
            add_job_log(
                'get_stock_codes',
                'success',
                f'è·å–è‚¡ç¥¨ä»£ç æˆåŠŸï¼Œå…± {len(stock_list)} åª',
                count=len(stock_list),
                elapsed_seconds=round(elapsed, 2)
            )
            
        except Exception as e:
            logger.error(f">>> ä»»åŠ¡å¤±è´¥: è·å–è‚¡ç¥¨ä»£ç å¤±è´¥: {e}")
            add_job_log('get_stock_codes', 'error', f'è·å–è‚¡ç¥¨ä»£ç å¤±è´¥: {str(e)}')
            raise
    
    @staticmethod
    async def task_full_init():
        """ä»»åŠ¡ï¼šå…¨é‡åˆå§‹åŒ–"""
        logger.info(">>> æ‰§è¡Œä»»åŠ¡: å…¨é‡åˆå§‹åŒ–æ‰€æœ‰è‚¡ç¥¨æ•°æ®")
        start_time = datetime.now()
        
        try:
            # ğŸ”§ ä¼˜åŒ–ï¼šé™ä½å¹¶å‘ä»¥é¿å…è§¦å‘APIé™åˆ¶
            # Tushareé™åˆ¶: æ¯åˆ†é’Ÿ500æ¬¡ï¼Œæ¯å¤©20000æ¬¡
            # æ¨èé…ç½®: batch_size=30, max_concurrent=5 â†’ ~300æ¬¡/åˆ†é’Ÿ
            result = await stock_atomic_service.full_update_all_stocks(
                days=180,
                batch_size=30,       # ä»50é™ä½è‡³30ï¼Œå‡å°‘å•æ‰¹æ¬¡å‹åŠ›
                max_concurrent=5     # ä»10é™ä½è‡³5ï¼Œå‡å°‘APIé™æµ
            )
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(
                f">>> ä»»åŠ¡å®Œæˆ: å…¨é‡åˆå§‹åŒ–å®Œæˆï¼Œ"
                f"æˆåŠŸ={result['success_count']}, "
                f"å¤±è´¥={result['failed_count']}, "
                f"è€—æ—¶ {elapsed:.2f}ç§’"
            )
            
            add_job_log(
                'full_init',
                'success',
                f"å…¨é‡åˆå§‹åŒ–å®Œæˆï¼ŒæˆåŠŸ={result['success_count']}, å¤±è´¥={result['failed_count']}",
                **result
            )
            
        except Exception as e:
            logger.error(f">>> ä»»åŠ¡å¤±è´¥: å…¨é‡åˆå§‹åŒ–å¤±è´¥: {e}")
            add_job_log('full_init', 'error', f'å…¨é‡åˆå§‹åŒ–å¤±è´¥: {str(e)}')
            raise
    
    @staticmethod
    async def task_crawl_news():
        """ä»»åŠ¡ï¼šçˆ¬å–æ–°é—»"""
        start_time = datetime.now()
        
        try:
            result = await stock_atomic_service.crawl_news(days=1)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            add_job_log(
                'crawl_news',
                'success' if result.get('success') else 'warning',
                f"çˆ¬å–æ–°é—»å®Œæˆï¼Œå…± {result.get('news_count', 0)} æ¡",
                **result
            )
            
        except Exception as e:
            logger.error(f">>> ä»»åŠ¡å¤±è´¥: çˆ¬å–æ–°é—»å¤±è´¥: {e}")
            add_job_log('crawl_news', 'error', f'çˆ¬å–æ–°é—»å¤±è´¥: {str(e)}')
            # æ–°é—»çˆ¬å–å¤±è´¥ä¸å½±å“å¯åŠ¨ï¼Œä¸æŠ›å‡ºå¼‚å¸¸
    
    @staticmethod
    async def task_calculate_signals():
        """ä»»åŠ¡ï¼šè®¡ç®—ç­–ç•¥ä¿¡å·"""
        logger.info(">>> æ‰§è¡Œä»»åŠ¡: è®¡ç®—ç­–ç•¥ä¿¡å·")
        start_time = datetime.now()
        
        try:
            result = await stock_atomic_service.calculate_strategy_signals(
                force_recalculate=True
            )
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f">>> ä»»åŠ¡å®Œæˆ: è®¡ç®—ä¿¡å·å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’")
            
            # ä»resultä¸­æ’é™¤statuså­—æ®µï¼Œé¿å…å‚æ•°å†²çª
            result_data = {k: v for k, v in result.items() if k != 'status'}
            add_job_log(
                'calculate_signals',
                'success' if result.get('success') or result.get('status') == 'success' else 'error',
                f"è®¡ç®—ä¿¡å·å®Œæˆ",
                **result_data
            )
            
        except Exception as e:
            logger.error(f">>> ä»»åŠ¡å¤±è´¥: è®¡ç®—ä¿¡å·å¤±è´¥: {e}")
            add_job_log('calculate_signals', 'error', f'è®¡ç®—ä¿¡å·å¤±è´¥: {str(e)}')
            # ä¿¡å·è®¡ç®—å¤±è´¥ä¸å½±å“å¯åŠ¨ï¼Œä¸æŠ›å‡ºå¼‚å¸¸


# ==================== è¿è¡Œæ—¶ä»»åŠ¡ ====================

class RuntimeTasks:
    """è¿è¡Œæ—¶å®šæ—¶ä»»åŠ¡"""
    
    @staticmethod
    def job_realtime_update():
        """å®šæ—¶ä»»åŠ¡ï¼šå®æ—¶æ›´æ–°æ‰€æœ‰è‚¡ç¥¨æ•°æ®"""
        # æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´
        if not is_trading_time():
            logger.debug("éäº¤æ˜“æ—¶é—´ï¼Œè·³è¿‡å®æ—¶æ•°æ®æ›´æ–°")
            return
        
        # é˜²æ­¢é‡å¤æ‰§è¡Œ
        if not _task_locks['realtime_update'].acquire(blocking=False):
            logger.warning("å®æ—¶æ•°æ®æ›´æ–°ä»»åŠ¡æ­£åœ¨æ‰§è¡Œä¸­ï¼Œè·³è¿‡æœ¬æ¬¡")
            return
        
        try:
            logger.info("========== å¼€å§‹å®æ—¶æ•°æ®æ›´æ–° ==========")
            start_time = datetime.now()
            
            # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                result = loop.run_until_complete(
                    stock_atomic_service.realtime_update_all_stocks()
                )
                
                elapsed = (datetime.now() - start_time).total_seconds()
                logger.info(f"========== å®æ—¶æ•°æ®æ›´æ–°å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’ ==========")
                
                # result ä¸­å·²åŒ…å« message å’Œ elapsed_secondsï¼Œç›´æ¥ä½¿ç”¨
                add_job_log(
                    'realtime_update',
                    'success',
                    result.get('message', 'å®æ—¶æ•°æ®æ›´æ–°å®Œæˆ'),
                    **{k: v for k, v in result.items() if k != 'message'}  # æ’é™¤messageé¿å…é‡å¤
                )
                
                # æ³¨æ„ï¼šå®æ—¶æ›´æ–°å’Œä¿¡å·è®¡ç®—å·²åˆ†ç¦»ï¼Œä¸å†è‡ªåŠ¨è§¦å‘ä¿¡å·è®¡ç®—
                # ä¿¡å·è®¡ç®—ç”±ç‹¬ç«‹çš„å®šæ—¶ä»»åŠ¡è§¦å‘
                
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"å®æ—¶æ•°æ®æ›´æ–°å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            add_job_log(
                'realtime_update',
                'error',
                f'å®æ—¶æ•°æ®æ›´æ–°å¼‚å¸¸: {str(e)}'  # ä½¿ç”¨ä¸åŒçš„æ¶ˆæ¯ï¼Œé¿å…ä¸resultä¸­çš„messageå†²çª
            )
        finally:
            _task_locks['realtime_update'].release()
    
    @staticmethod
    def job_calculate_signals():
        """å®šæ—¶ä»»åŠ¡ï¼šè®¡ç®—ç­–ç•¥ä¿¡å·ï¼ˆç›˜ä¸­ä»…è®¡ç®—è‚¡ç¥¨ä¿¡å·ï¼Œä¸è®¡ç®—ETFï¼‰"""
        # æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´
        if not is_trading_time():
            logger.debug("éäº¤æ˜“æ—¶é—´ï¼Œè·³è¿‡ä¿¡å·è®¡ç®—")
            return
        
        # é˜²æ­¢é‡å¤æ‰§è¡Œ
        if not _task_locks['signal_calculation'].acquire(blocking=False):
            logger.warning("ä¿¡å·è®¡ç®—ä»»åŠ¡æ­£åœ¨æ‰§è¡Œä¸­ï¼Œè·³è¿‡æœ¬æ¬¡")
            return
        
        try:
            logger.info("========== å¼€å§‹è®¡ç®—ç­–ç•¥ä¿¡å·ï¼ˆä»…è‚¡ç¥¨ï¼‰ ==========")
            start_time = datetime.now()
            
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                result = loop.run_until_complete(
                    stock_atomic_service.calculate_strategy_signals(
                        force_recalculate=False,
                        stock_only=True  # ç›˜ä¸­ä»…è®¡ç®—è‚¡ç¥¨ä¿¡å·
                    )
                )
                
                elapsed = (datetime.now() - start_time).total_seconds()
                logger.info(f"========== ä¿¡å·è®¡ç®—å®Œæˆï¼ˆä»…è‚¡ç¥¨ï¼‰ï¼Œè€—æ—¶ {elapsed:.2f}ç§’ ==========")
                
                # ä»resultä¸­æ’é™¤statuså­—æ®µï¼Œé¿å…å‚æ•°å†²çª
                result_data = {k: v for k, v in result.items() if k != 'status'}
                add_job_log(
                    'signal_calculation',
                    'success' if result.get('success') or result.get('status') == 'success' else 'warning',
                    f'ä¿¡å·è®¡ç®—å®Œæˆ',
                    elapsed_seconds=round(elapsed, 2),
                    **result_data
                )
                
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"ä¿¡å·è®¡ç®—å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            add_job_log(
                'signal_calculation',
                'error',
                f'ä¿¡å·è®¡ç®—å¤±è´¥: {str(e)}'
            )
        finally:
            _task_locks['signal_calculation'].release()
    
    @staticmethod
    def job_crawl_news():
        """å®šæ—¶ä»»åŠ¡ï¼šçˆ¬å–æ–°é—»"""
        start_time = datetime.now()
        
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                result = loop.run_until_complete(
                    stock_atomic_service.crawl_news(days=1)
                )
                
                elapsed = (datetime.now() - start_time).total_seconds()
                
                add_job_log(
                    'crawl_news',
                    'success' if result.get('success') else 'warning',
                    f"çˆ¬å–æ–°é—»å®Œæˆï¼Œå…± {result.get('news_count', 0)} æ¡",
                    **result
                )
                
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"æ–°é—»çˆ¬å–å¤±è´¥: {e}")
            add_job_log('crawl_news', 'error', f'æ–°é—»çˆ¬å–å¤±è´¥: {str(e)}')
    
    @staticmethod
    def job_full_update_and_calculate():
        """å®šæ—¶ä»»åŠ¡ï¼šå…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·"""
        # é˜²æ­¢é‡å¤æ‰§è¡Œ
        if not _task_locks['full_update'].acquire(blocking=False):
            logger.warning("å…¨é‡æ›´æ–°ä»»åŠ¡æ­£åœ¨æ‰§è¡Œä¸­ï¼Œè·³è¿‡æœ¬æ¬¡")
            return
        
        try:
            logger.info("========== å¼€å§‹å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å· ==========")
            start_time = datetime.now()
            
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                # 1. å…¨é‡æ›´æ–°ï¼ˆåŒ…å«è‚¡ç¥¨å’ŒETFï¼Œé™ä½å¹¶å‘æ•°ï¼‰
                update_result = loop.run_until_complete(
                    stock_atomic_service.full_update_all_stocks(
                        days=180,
                        batch_size=50,
                        max_concurrent=5  # é™ä½å¹¶å‘æ•°ï¼Œå‡å°‘APIé™æµ
                    )
                )
                
                logger.info(f"å…¨é‡æ›´æ–°å®Œæˆï¼ˆåŒ…å«ETFï¼‰: æˆåŠŸ={update_result['success_count']}, å¤±è´¥={update_result['failed_count']}")
                
                # 2. è®¡ç®—ä¿¡å·ï¼ˆåŒ…å«è‚¡ç¥¨å’ŒETFï¼‰
                signal_result = loop.run_until_complete(
                    stock_atomic_service.calculate_strategy_signals(
                        force_recalculate=True,
                        stock_only=False  # å…¨é‡æ›´æ–°åŒ…å«ETFä¿¡å·
                    )
                )
                
                elapsed = (datetime.now() - start_time).total_seconds()
                logger.info(f"========== å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’ ==========")
                
                add_job_log(
                    'full_update_and_calculate',
                    'success',
                    f"å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·å®Œæˆ",
                    elapsed_seconds=round(elapsed, 2),
                    update_result=update_result,
                    signal_result=signal_result
                )
                
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            add_job_log('full_update_and_calculate', 'error', f'å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·å¤±è´¥: {str(e)}')
        finally:
            _task_locks['full_update'].release()
    
    @staticmethod
    def job_cleanup_charts():
        """å®šæ—¶ä»»åŠ¡ï¼šæ¸…ç†å›¾è¡¨æ–‡ä»¶"""
        logger.info("========== å¼€å§‹æ¸…ç†å›¾è¡¨æ–‡ä»¶ ==========")
        start_time = datetime.now()
        
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                result = loop.run_until_complete(
                    stock_atomic_service.cleanup_chart_files()
                )
                
                elapsed = (datetime.now() - start_time).total_seconds()
                logger.info(f"========== å›¾è¡¨æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’ ==========")
                
                add_job_log(
                    'cleanup_charts',
                    'success',
                    f"æ¸…ç†å›¾è¡¨æ–‡ä»¶å®Œæˆï¼Œåˆ é™¤ {result.get('deleted_count', 0)} ä¸ªæ–‡ä»¶",
                    **result
                )
                
            finally:
                loop.close()
                
        except Exception as e:
            logger.error(f"æ¸…ç†å›¾è¡¨æ–‡ä»¶å¤±è´¥: {e}")
            add_job_log('cleanup_charts', 'error', f'æ¸…ç†å›¾è¡¨æ–‡ä»¶å¤±è´¥: {str(e)}')


# ==================== è°ƒåº¦å™¨ç®¡ç† ====================

def start_stock_scheduler(init_mode: str = "skip", calculate_signals: bool = False):
    """
    å¯åŠ¨è‚¡ç¥¨è°ƒåº¦å™¨
    
    Args:
        init_mode: åˆå§‹åŒ–æ¨¡å¼
            - skip: è·³è¿‡åˆå§‹åŒ–
            - full_init: å…¨é‡åˆå§‹åŒ–
        calculate_signals: æ˜¯å¦åœ¨å¯åŠ¨æ—¶è®¡ç®—ä¿¡å·
    """
    global scheduler
    
    if scheduler is not None and scheduler.running:
        logger.warning("è‚¡ç¥¨è°ƒåº¦å™¨å·²åœ¨è¿è¡Œä¸­")
        return
    
    logger.info("========== å¯åŠ¨è‚¡ç¥¨è°ƒåº¦å™¨ ==========")
    logger.info(f"åˆå§‹åŒ–æ¨¡å¼: {init_mode}")
    logger.info(f"å¯åŠ¨æ—¶è®¡ç®—ä¿¡å·: {calculate_signals}")
    
    # 1. æ‰§è¡Œå¯åŠ¨ä»»åŠ¡
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(
            StartupTasks.execute(init_mode=init_mode, calculate_signals=calculate_signals)
        )
    finally:
        loop.close()
    
    # 2. åˆ›å»ºè°ƒåº¦å™¨
    scheduler = BackgroundScheduler(timezone='Asia/Shanghai')
    
    # 3. æ·»åŠ è¿è¡Œæ—¶ä»»åŠ¡
    
    # å®æ—¶æ•°æ®æ›´æ–°ï¼šæ¯åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡REALTIME_UPDATE_INTERVALé…ç½®ï¼‰
    realtime_interval = settings.REALTIME_UPDATE_INTERVAL
    scheduler.add_job(
        func=RuntimeTasks.job_realtime_update,
        trigger=IntervalTrigger(minutes=realtime_interval),
        id='realtime_update',
        name='å®æ—¶æ•°æ®æ›´æ–°',
        replace_existing=True
    )
    logger.info(f"å®æ—¶æ•°æ®æ›´æ–°ä»»åŠ¡å·²æ·»åŠ ï¼Œé—´éš”: {realtime_interval}åˆ†é’Ÿ")
    
    # ä¿¡å·è®¡ç®—ï¼šå›ºå®šæ—¶é—´ç‚¹è§¦å‘ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    # 9:30, 9:50, 10:10, 10:30, 10:50, 11:10, 11:30
    # 13:00, 13:20, 13:40, 14:00, 14:20, 14:40, 15:00, 15:20
    from datetime import datetime
    now = datetime.now()
    
    # å¦‚æœæ˜¯äº¤æ˜“æ—¶é—´ä¸”å¯åŠ¨æ—¶è®¡ç®—ä¿¡å·ï¼Œç«‹å³æ‰§è¡Œä¸€æ¬¡
    if is_trading_time() and calculate_signals:
        logger.info("å¯åŠ¨æ—¶ç«‹å³æ‰§è¡Œä¸€æ¬¡ä¿¡å·è®¡ç®—ï¼Œç¡®ä¿æœ‰æœ€æ–°ä¿¡å·...")
        import threading
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼Œä¸é˜»å¡å¯åŠ¨
        threading.Thread(target=RuntimeTasks.job_calculate_signals, daemon=True).start()
    
    scheduler.add_job(
        func=RuntimeTasks.job_calculate_signals,
        trigger=CronTrigger(
            day_of_week='mon-fri',
            hour='9-11,13-15',
            minute='0,10,20,30,40,50'
        ),
        id='signal_calculation',
        name='ç­–ç•¥ä¿¡å·è®¡ç®—',
        replace_existing=True
    )
    logger.info("ä¿¡å·è®¡ç®—ä»»åŠ¡å·²æ·»åŠ ï¼Œå›ºå®šæ—¶é—´ç‚¹è§¦å‘ï¼ˆçº¦æ¯20åˆ†é’Ÿï¼‰")
    
    # æ–°é—»çˆ¬å–ï¼šæ¯2å°æ—¶æ‰§è¡Œä¸€æ¬¡
    scheduler.add_job(
        func=RuntimeTasks.job_crawl_news,
        trigger=IntervalTrigger(hours=2),
        id='crawl_news',
        name='æ–°é—»çˆ¬å–',
        replace_existing=True
    )
    
    # å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·ï¼šæ¯ä¸ªäº¤æ˜“æ—¥17:35æ‰§è¡Œä¸€æ¬¡
    scheduler.add_job(
        func=RuntimeTasks.job_full_update_and_calculate,
        trigger=CronTrigger(hour=17, minute=35, day_of_week='mon-fri'),
        id='full_update_and_calculate',
        name='å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·',
        replace_existing=True
    )
    
    # å›¾è¡¨æ–‡ä»¶æ¸…ç†ï¼šæ¯å¤©00:00æ‰§è¡Œä¸€æ¬¡
    scheduler.add_job(
        func=RuntimeTasks.job_cleanup_charts,
        trigger=CronTrigger(hour=0, minute=0),
        id='cleanup_charts',
        name='å›¾è¡¨æ–‡ä»¶æ¸…ç†',
        replace_existing=True
    )
    
    # 4. å¯åŠ¨è°ƒåº¦å™¨
    scheduler.start()
    logger.info("========== è‚¡ç¥¨è°ƒåº¦å™¨å¯åŠ¨å®Œæˆ ==========")
    logger.info("å®šæ—¶ä»»åŠ¡:")
    logger.info(f"  - å®æ—¶æ•°æ®æ›´æ–°: æ¯{realtime_interval}åˆ†é’Ÿï¼ˆäº¤æ˜“æ—¶é—´ï¼‰")
    logger.info("  - ç­–ç•¥ä¿¡å·è®¡ç®—: å›ºå®šæ—¶é—´ç‚¹ï¼ˆ9:30/9:50/10:10/10:30/10:50/11:10/11:30/13:00/13:20/13:40/14:00/14:20/14:40/15:00/15:20ï¼Œç‹¬ç«‹ä»»åŠ¡ï¼‰")
    logger.info("  - æ–°é—»çˆ¬å–: æ¯2å°æ—¶")
    logger.info("  - å…¨é‡æ›´æ–°å¹¶è®¡ç®—ä¿¡å·: æ¯ä¸ªäº¤æ˜“æ—¥17:35")
    logger.info("  - å›¾è¡¨æ–‡ä»¶æ¸…ç†: æ¯å¤©00:00")


def stop_stock_scheduler():
    """åœæ­¢è‚¡ç¥¨è°ƒåº¦å™¨"""
    global scheduler
    
    if scheduler is not None and scheduler.running:
        scheduler.shutdown()
        scheduler = None
        logger.info("è‚¡ç¥¨è°ƒåº¦å™¨å·²åœæ­¢")
    else:
        logger.warning("è‚¡ç¥¨è°ƒåº¦å™¨æœªè¿è¡Œ")


def get_stock_scheduler_status() -> Dict[str, Any]:
    """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
    global scheduler, job_logs
    
    if scheduler is None:
        return {
            'running': False,
            'message': 'è°ƒåº¦å™¨æœªå¯åŠ¨'
        }
    
    jobs_info = []
    for job in scheduler.get_jobs():
        jobs_info.append({
            'id': job.id,
            'name': job.name,
            'next_run_time': job.next_run_time.isoformat() if job.next_run_time else None
        })
    
    return {
        'running': scheduler.running,
        'jobs': jobs_info,
        'recent_logs': job_logs[:10],
        'is_trading_time': is_trading_time()
    }

