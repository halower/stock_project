# -*- coding: utf-8 -*-
"""æ–°é—»æ¶ˆæ¯é¢åˆ†ææœåŠ¡æ¨¡å—"""

import os
import json
import requests
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
import pandas as pd
import time
import random
import re
from bs4 import BeautifulSoup

from app.core.logging import logger
from app.core.config import (
    AI_NEWS_ANALYSIS_MAX_TOKENS, AI_TEMPERATURE
)
from app.db.session import RedisCache

# Redisç¼“å­˜å®¢æˆ·ç«¯
redis_cache = RedisCache()

# æ–°é—»æ•°æ®ç¼“å­˜é”®å®šä¹‰
NEWS_KEYS = {
    'news_latest': 'news:latest',
    'news_last_update': 'news:last_update',
    'news_scheduler_log': 'news:scheduler:log',
}

# æ–°é—»æ•°æ®ç¼“å­˜ï¼ˆå†…å­˜ç¼“å­˜ä½œä¸ºå¤‡ç”¨ï¼‰
news_cache = {}  # {key: {"data": [...], "timestamp": datetimeå¯¹è±¡}}
news_analysis_cache = {}  # {key: {"analysis": "...", "timestamp": datetimeå¯¹è±¡}}

def get_phoenix_finance_news(days: int = 1, skip_content: bool = False, force_crawl: bool = False) -> List[Dict[str, Any]]:
    """ä»å‡¤å‡°è´¢ç»çˆ¬å–æ–°é—»æ•°æ®
    
    Args:
        days: è·å–æœ€è¿‘å‡ å¤©çš„æ•°æ®
        skip_content: æ˜¯å¦è·³è¿‡è·å–æ–°é—»è¯¦æƒ…å†…å®¹ï¼Œä»…è·å–æ ‡é¢˜å’Œé“¾æ¥
        force_crawl: æ˜¯å¦å¼ºåˆ¶çˆ¬å–ï¼Œå¿½ç•¥ç¼“å­˜
        
    Returns:
        æ–°é—»æ•°æ®åˆ—è¡¨
    """
    # é¦–å…ˆæ£€æŸ¥Redisç¼“å­˜ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    if not force_crawl and days <= 2:  # å¯¹äº1-2å¤©çš„æ–°é—»ï¼Œä¼˜å…ˆä½¿ç”¨Redisç¼“å­˜
        redis_news = redis_cache.get_cache(NEWS_KEYS['news_latest'])
        if redis_news and redis_news.get('news'):
            logger.info(f"ä½¿ç”¨Redisç¼“å­˜çš„å‡¤å‡°è´¢ç»æ–°é—»æ•°æ®ï¼Œæ–°é—»æ•°é‡: {len(redis_news['news'])}")
            return redis_news['news']
    
    # æ£€æŸ¥å†…å­˜ç¼“å­˜
    cache_key = f"phoenix_finance_{days}"
    if not force_crawl and cache_key in news_cache:
        cache_data = news_cache[cache_key]
        # ç¼“å­˜æœ‰æ•ˆæœŸ2å°æ—¶
        if (datetime.now() - cache_data["timestamp"]).total_seconds() < 7200:
            logger.info(f"ä½¿ç”¨å†…å­˜ç¼“å­˜çš„å‡¤å‡°è´¢ç»æ–°é—»æ•°æ®: {cache_key}, æ–°é—»æ•°é‡: {len(cache_data['data'])}")
            return cache_data["data"]
    
    news_list = []
    
    try:
        # è¯·æ±‚å¤´
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
        }
        
        # å‡¤å‡°è´¢ç»æ–°é—»åˆ—è¡¨é¡µ
        phoenix_urls = [
            "https://finance.ifeng.com/",  # å‡¤å‡°è´¢ç»é¦–é¡µ
            "https://finance.ifeng.com/stock/",  # è‚¡ç¥¨
            "https://finance.ifeng.com/ipo/",  # IPO
            "https://finance.ifeng.com/financial/",  # é‡‘è
        ]
        
        # è®¾ç½®è¯·æ±‚å‚æ•°ï¼Œç¦ç”¨ä»£ç†
        request_kwargs = {
            "headers": headers, 
            "timeout": 10,
            "verify": False,  # ç¦ç”¨SSLéªŒè¯
            "proxies": {"http": None, "https": None}  # ç¦ç”¨ä»£ç†
        }
        
        # ç¦ç”¨SSLè­¦å‘Š
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        processed_titles = set()  # ç”¨äºå»é‡
        
        for url in phoenix_urls:
            if len(news_list) >= 50:  # æ§åˆ¶æ–°é—»æ€»æ•°
                break
                
            try:
                # é˜²æ­¢è¯·æ±‚è¿‡å¿«
                time.sleep(random.uniform(0.3, 0.5))
                
                # å°è¯•ä½¿ç”¨ç¦ç”¨ä»£ç†çš„æ–¹å¼è¯·æ±‚
                try:
                    response = requests.get(url, **request_kwargs)
                except Exception as e:
                    logger.warning(f"ä½¿ç”¨æ— ä»£ç†æ¨¡å¼è¯·æ±‚å¤±è´¥: {url}, {str(e)}")
                    # å¦‚æœç¦ç”¨ä»£ç†å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤è®¾ç½®
                    response = requests.get(url, headers=headers, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # è·å–æ–°é—»åˆ—è¡¨
                    news_items = []
                    
                    # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–æ–°é—»åˆ—è¡¨
                    selectors = [
                        '.news-stream-newsStream-news-item h2 a',  # ä¸»æµæ–°é—»é¡¹
                        '.content-list h3 a',  # å†…å®¹åˆ—è¡¨
                        '.newsList li h3 a',  # æ–°é—»åˆ—è¡¨
                        '.news-list li a',  # æ ‡å‡†æ–°é—»åˆ—è¡¨
                        '.box-list li a',  # ç›’å­åˆ—è¡¨
                        'a.tit',  # æ ‡é¢˜é“¾æ¥
                        '.headline-news h2 a',  # å¤´æ¡æ–°é—»
                    ]
                    
                    for selector in selectors:
                        items = soup.select(selector)
                        if items:
                            news_items.extend(items)
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–°é—»ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
                    if not news_items:
                        items = soup.select('a[href*="finance.ifeng.com"]')
                        news_items.extend(items)
                    
                    # å¤„ç†æ‰¾åˆ°çš„æ–°é—»
                    for item in news_items:
                        if len(news_list) >= 50:  # æ§åˆ¶æ–°é—»æ€»æ•°
                            break
                            
                        title = item.get_text().strip()
                        link = item.get('href')
                        
                        # è¿‡æ»¤æ— æ•ˆæ ‡é¢˜å’Œé“¾æ¥
                        if not title or not link or len(title) < 5 or title in processed_titles:
                            continue
                        
                        # è¿‡æ»¤æ‰"ä¸­å›½æ·±åº¦è´¢ç»"å’Œ"ä¸Šå¸‚å…¬å¸ç ”ç©¶é™¢"è¿™ä¸¤ä¸ªéæ–°é—»æ ‡é¢˜
                        if title == "ä¸­å›½æ·±åº¦è´¢ç»" or title == "ä¸Šå¸‚å…¬å¸ç ”ç©¶é™¢":
                            continue
                            
                        # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                        if not link.startswith('http'):
                            if link.startswith('/'):
                                link = 'https://finance.ifeng.com' + link
                            else:
                                continue
                        
                        # åªå¤„ç†å‡¤å‡°è´¢ç»çš„é“¾æ¥
                        if not ('ifeng.com' in link and ('finance' in link or 'stock' in link)):
                            continue
                            
                        processed_titles.add(title)
                        
                        # æ·»åŠ åˆ°æ–°é—»åˆ—è¡¨
                        news_list.append({
                            'title': title,
                            'url': link,
                            'source': 'å‡¤å‡°è´¢ç»',
                            'datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'content': ''
                        })
            except Exception as e:
                logger.warning(f"è·å–å‡¤å‡°è´¢ç»æ–°é—»åˆ—è¡¨å¤±è´¥: {url}, {str(e)}")
        
        logger.info(f"ä»å‡¤å‡°è´¢ç»è·å–åˆ° {len(news_list)} æ¡æ–°é—»")
        
        # å¦‚æœè·å–åˆ°çš„æ–°é—»æ•°é‡å¤ªå°‘ï¼Œä½†ç¼“å­˜ä¸­æœ‰æ•°æ®ï¼Œåˆ™ä½¿ç”¨ç¼“å­˜
        if len(news_list) < 5 and cache_key in news_cache:
            logger.warning(f"è·å–åˆ°çš„æ–°é—»æ•°é‡å¤ªå°‘({len(news_list)}æ¡)ï¼Œä½¿ç”¨ç¼“å­˜ä¸­çš„æ–°é—»æ•°æ®")
            return news_cache[cache_key]["data"]
        
        # è·å–æ¯ç¯‡æ–°é—»çš„è¯¦æƒ…å†…å®¹ï¼Œä»…å½“skip_contentä¸ºFalseæ—¶è¿›è¡Œ
        if not skip_content:
            # åªåœ¨ DEBUG æ¨¡å¼ä¸‹è¾“å‡ºè¯¦ç»†æ—¥å¿—
            import logging
            is_debug = logger.level <= logging.DEBUG
            if is_debug:
                logger.debug(f"å¼€å§‹è·å– {len(news_list)} æ¡æ–°é—»çš„è¯¦æƒ…å†…å®¹")
            content_count = 0
            
            for i, news in enumerate(news_list):
                try:
                    # ä¼˜åŒ–ï¼šå‡å°‘å»¶è¿Ÿæ—¶é—´ï¼Œæå‡é€Ÿåº¦ï¼ˆ0.05-0.1ç§’è¶³å¤Ÿé¿å…è¢«å°ï¼‰
                    time.sleep(random.uniform(0.05, 0.1))
                    
                    # å°è¯•ä½¿ç”¨ç¦ç”¨ä»£ç†çš„æ–¹å¼è¯·æ±‚
                    try:
                        detail_response = requests.get(news['url'], **request_kwargs)
                    except Exception as e:
                        logger.warning(f"ä½¿ç”¨æ— ä»£ç†æ¨¡å¼è¯·æ±‚æ–°é—»è¯¦æƒ…å¤±è´¥: {news['url']}, {str(e)}")
                        # å¦‚æœç¦ç”¨ä»£ç†å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤è®¾ç½®
                        detail_response = requests.get(news['url'], headers=headers, timeout=10)
                    
                    if detail_response.status_code == 200:
                        detail_soup = BeautifulSoup(detail_response.text, 'html.parser')
                        
                        # æå–å‘å¸ƒæ—¶é—´
                        time_str = None
                        time_selectors = [
                            '.time', '.date', '.ss_time', '.p_time', '.artTime', '.article-time',
                            '.article-meta .time', '.detail-title-date', '.date-source'
                        ]
                        for selector in time_selectors:
                            time_element = detail_soup.select_one(selector)
                            if time_element:
                                time_str = time_element.get_text().strip()
                                break
                        
                        # è§£ææ—¶é—´
                        if time_str:
                            # å°è¯•å¤šç§æ ¼å¼è§£ææ—¶é—´
                            time_patterns = [
                                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥\s*(\d{1,2}):(\d{1,2})',  # 2024å¹´5æœˆ27æ—¥ 19:52
                                r'(\d{4})-(\d{1,2})-(\d{1,2})\s*(\d{1,2}):(\d{1,2})',      # 2024-5-27 19:52
                                r'(\d{4})\/(\d{1,2})\/(\d{1,2})\s*(\d{1,2}):(\d{1,2})',    # 2024/5/27 19:52
                            ]
                            
                            for pattern in time_patterns:
                                match = re.search(pattern, time_str)
                                if match:
                                    if len(match.groups()) == 5:  # å¹´æœˆæ—¥æ—¶åˆ†
                                        year, month, day, hour, minute = match.groups()
                                        time_str = f"{year}-{month.zfill(2)}-{day.zfill(2)} {hour}:{minute}"
                                        break
                        
                            try:
                                pub_time = datetime.strptime(time_str, '%Y-%m-%d %H:%M')
                                news['datetime'] = pub_time.strftime('%Y-%m-%d %H:%M:%S')
                            except Exception as e:
                                # å¦‚æœè§£æå¤±è´¥ï¼Œä¿ç•™å½“å‰æ—¶é—´
                                logger.warning(f"è§£ææ—¶é—´å¤±è´¥: {time_str}, {str(e)}")
                        
                        # æå–å†…å®¹æ‘˜è¦
                        summary = ""
                        
                        # å…ˆå°è¯•è·å–æ‘˜è¦
                        summary_selectors = [
                            '.main-content', '.article-content', '.article_content', '.content',
                            '#main_content', '.all-content', '.article'
                        ]
                        
                        for selector in summary_selectors:
                            content_element = detail_soup.select_one(selector)
                            if content_element:
                                # è·å–æ‰€æœ‰æ®µè½
                                paragraphs = content_element.select('p')
                                if paragraphs:
                                    # æå–å‰ä¸¤æ®µæ–‡å­—ä½œä¸ºæ‘˜è¦
                                    text_paras = [p.get_text().strip() for p in paragraphs if p.get_text().strip()]
                                    if text_paras:
                                        summary = ' '.join(text_paras[:2])
                                        if len(summary) > 300:
                                            summary = summary[:297] + '...'
                                        break
                        
                        news['content'] = summary
                        content_count += 1
                        
                        # ä¼˜åŒ–ï¼šåªåœ¨ DEBUG æ¨¡å¼ä¸‹è¾“å‡ºè¿›åº¦æ—¥å¿—ï¼Œé¿å…æ—¥å¿—åˆ·å±
                        if is_debug and content_count % 10 == 0:
                            logger.debug(f"å·²è·å– {content_count}/{len(news_list)} æ¡æ–°é—»çš„è¯¦æƒ…å†…å®¹")
                        
                except Exception as e:
                    if is_debug:
                        logger.debug(f"è·å–æ–°é—»è¯¦æƒ…å¤±è´¥: {news['url']}, {str(e)}")
            
            # å®Œæˆåè¾“å‡ºä¸€æ¬¡æ€»ç»“æ—¥å¿—
            if content_count > 0:
                logger.info(f"âœ“ æˆåŠŸè·å– {content_count}/{len(news_list)} æ¡æ–°é—»è¯¦æƒ…")
        else:
            if is_debug:
                logger.debug("è·³è¿‡è·å–æ–°é—»è¯¦æƒ…å†…å®¹")
            # å¯¹äºè·³è¿‡å†…å®¹çš„æƒ…å†µï¼Œè®¾ç½®ç©ºå†…å®¹
            for news in news_list:
                news['content'] = ''
        
        # æ ¹æ®å‘å¸ƒæ—¶é—´è¿‡æ»¤æ–°é—»
        filtered_news = []
        for news in news_list:
            try:
                news_time = datetime.strptime(news['datetime'], '%Y-%m-%d %H:%M:%S')
                if (datetime.now() - news_time).days <= days:
                    filtered_news.append(news)
            except Exception:
                # å¦‚æœæ—¥æœŸè§£æå¤±è´¥ï¼Œé»˜è®¤ä¿ç•™
                filtered_news.append(news)
        
        news_list = filtered_news
        
        # åªæœ‰å½“è·å–åˆ°è¶³å¤Ÿçš„æ–°é—»æ—¶æ‰æ›´æ–°ç¼“å­˜
        if len(news_list) >= 5:
            news_cache[cache_key] = {
                "data": news_list,
                "timestamp": datetime.now()
            }
            logger.info(f"æ›´æ–°å‡¤å‡°è´¢ç»æ–°é—»ç¼“å­˜: {cache_key}, æ–°é—»æ•°é‡: {len(news_list)}")
        else:
            logger.warning(f"è·å–åˆ°çš„æ–°é—»æ•°é‡å¤ªå°‘({len(news_list)}æ¡)ï¼Œä¸æ›´æ–°ç¼“å­˜")
        
        return news_list
    except Exception as e:
        logger.error(f"è·å–å‡¤å‡°è´¢ç»æ–°é—»å¤±è´¥: {str(e)}")
        # å¦‚æœå‡ºé”™ä¸”ç¼“å­˜ä¸­æœ‰æ•°æ®ï¼Œåˆ™ä½¿ç”¨ç¼“å­˜
        if cache_key in news_cache:
            logger.info(f"çˆ¬è™«å¤±è´¥ï¼Œä½¿ç”¨ç¼“å­˜ä¸­çš„æ–°é—»æ•°æ®: {cache_key}")
            return news_cache[cache_key]["data"]
        # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œè¿”å›ç©ºåˆ—è¡¨
        return []

def get_today_news() -> List[Dict[str, Any]]:
    """è·å–å½“å¤©çš„å‡¤å‡°è´¢ç»æ–°é—»ï¼Œç”¨äºå‰ç«¯å±•ç¤º
    
    Returns:
        å½“å¤©çš„æ–°é—»åˆ—è¡¨
    """
    # è·å–ä»Šå¤©çš„æ–°é—»ï¼Œéœ€è¦åŒ…å«è¯¦æƒ…å†…å®¹
    news_list = get_phoenix_finance_news(days=1, skip_content=False)
    
    # ç»Ÿè®¡è·å–åˆ°çš„æ–°é—»æ•°é‡
    logger.info(f"è·å–åˆ° {len(news_list)} æ¡ä»Šæ—¥æ–°é—»")
    
    # æ ¼å¼åŒ–ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
    formatted_news = []
    for news in news_list:
        formatted_news.append({
            'title': news['title'],
            'url': news['url'],
            'datetime': news['datetime'],
            'source': news['source'],
            'summary': news.get('content', '')[:150] + '...' if news.get('content') and len(news.get('content')) > 150 else news.get('content', '')
        })
    
    return formatted_news

def analyze_news_by_titles(news_list: List[Dict[str, Any]], use_cache: bool = True, ai_model_name: str = None, ai_endpoint: str = None, ai_api_key: str = None) -> str:
    """
    åˆ†ææ–°é—»æ ‡é¢˜ï¼Œç”Ÿæˆæ¶ˆæ¯é¢åˆ†ææŠ¥å‘Š
    
    Args:
        news_list: æ–°é—»åˆ—è¡¨
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        ai_model_name: AIæ¨¡å‹åç§°
        ai_endpoint: AIæ¥å£åœ°å€
        ai_api_key: AIæ¥å£å¯†é’¥
        
    Returns:
        åˆ†ææ–‡æœ¬
    """
    from app.services.analysis.llm_service import get_completion_with_custom_params
    from app.core.config import AI_NEWS_ANALYSIS_MAX_TOKENS, AI_TEMPERATURE
    
    # å‡†å¤‡æ–°é—»æ ‡é¢˜æ•°æ®
    news_titles = [news["title"] for news in news_list[:50]]  
    titles_text = "\n".join([f"{i+1}. {title}" for i, title in enumerate(news_titles)])
    
    # æ„é€ æç¤ºè¯ - ä¼˜åŒ–ä¸ºä¸“ä¸šäº¤æ˜“æŒ‡å¯¼æŠ¥å‘Š
    prompt = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„Aè‚¡äº¤æ˜“åˆ†æå¸ˆï¼Œæ‹¥æœ‰15å¹´å¸‚åœºç»éªŒã€‚è¯·åŸºäºä»¥ä¸‹{len(news_titles)}æ¡æœ€æ–°è´¢ç»æ–°é—»ï¼Œæ’°å†™ä¸€ä»½ä¸“ä¸šçš„äº¤æ˜“æŒ‡å¯¼åˆ†ææŠ¥å‘Šã€‚
    
ã€ä»Šæ—¥è´¢ç»æ–°é—»ã€‘
    {titles_text}
    
è¯·æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºç®€æ´ã€ä¸“ä¸šã€å®ç”¨çš„åˆ†ææŠ¥å‘Šï¼š
    
    ## 1. å¸‚åœºæ•´ä½“æƒ…ç»ªåˆ†æ
**å¸‚åœºæƒ…ç»ªï¼š** [ç§¯æ/ä¸­æ€§ååˆ†åŒ–/è°¨æ…è§‚æœ›/ä¹è§‚å‘ä¸Š]
â€¢ **ç§‘æŠ€è¡Œä¸šï¼š** [å…·ä½“åˆ†ææ”¿ç­–å½±å“ã€é¾™å¤´è‚¡è¡¨ç°]
â€¢ **é‡‘èè¡Œä¸šï¼š** [å…·ä½“åˆ†æé“¶è¡Œã€åˆ¸å•†ã€ä¿é™©è¡¨ç°]
â€¢ **æ¶ˆè´¹æ¿å—ï¼š** [å…·ä½“åˆ†ææ¶ˆè´¹æ•°æ®ã€è¡Œä¸šè¶‹åŠ¿]
â€¢ **å‘¨æœŸæ¿å—ï¼š** [å…·ä½“åˆ†æåŸææ–™ã€èƒ½æºã€åŸºå»º]
**æƒ…ç»ªæŒ‡æ•°ï¼š** [0-100åˆ†] | å¤šå¤´åŠ›é‡ï¼š[å¼º/ä¸­/å¼±] | ç©ºå¤´å‹åŠ›ï¼š[å¼º/ä¸­/å¼±]

## 2. çƒ­ç‚¹è¡Œä¸šæ·±åº¦è§£è¯»
### ğŸ”¥ [è¡Œä¸šåç§°1]
- **çƒ­åº¦è¯„çº§ï¼š** â­â­â­â­â­
- **é©±åŠ¨å› ç´ ï¼š** [å…·ä½“æ”¿ç­–ã€äº‹ä»¶ã€æ•°æ®]
- **é¾™å¤´æ ‡çš„ï¼š** [å…·ä½“æ¿å—æˆ–æ¦‚å¿µ]
- **æŒç»­æ€§ï¼š** [1-3å¤©/1-2å‘¨/ä¸­é•¿æœŸ]
- **æŠ•èµ„å»ºè®®ï¼š** [ä¹°å…¥/å…³æ³¨/è§‚æœ›/å›é¿]
- **é£é™©æç¤ºï¼š** [å…·ä½“é£é™©ç‚¹]

### ğŸ”¥ [è¡Œä¸šåç§°2ã€3]ï¼ˆåŒä¸Šç»“æ„ï¼Œåˆ—å‡º2-4ä¸ªçƒ­ç‚¹è¡Œä¸šï¼‰

## 3. å®è§‚ç»æµä¸æ”¿ç­–åˆ†æ
### æ”¿ç­–é¢
â€¢ **è´§å¸æ”¿ç­–ï¼š** [å¤®è¡Œæ€åº¦ã€æµåŠ¨æ€§ã€åˆ©ç‡èµ°åŠ¿]
â€¢ **è´¢æ”¿æ”¿ç­–ï¼š** [æ”¿åºœæ”¯å‡ºã€å‡ç¨é™è´¹ã€åŸºå»º]
â€¢ **ç›‘ç®¡æ”¿ç­–ï¼š** [æœ€æ–°ç›‘ç®¡ã€å¯¹å¸‚åœºå½±å“]
### ç»æµæ•°æ®
â€¢ **ç»æµå¢é€Ÿï¼š** [GDPã€PMIã€è¿›å‡ºå£æ•°æ®]
â€¢ **é€šèƒ€æ°´å¹³ï¼š** [CPIã€PPIèµ°åŠ¿]
### å›½é™…ç¯å¢ƒ
â€¢ **ç¾è”å‚¨æ”¿ç­–ï¼š** [åŠ æ¯é¢„æœŸã€å¯¹Aè‚¡å½±å“]
â€¢ **åœ°ç¼˜æ”¿æ²»ï¼š** [å›½é™…å…³ç³»ã€è´¸æ˜“æ‘©æ“¦]
    
    ## 4. é£é™©å› ç´ è¯†åˆ«
### ğŸš¨ é«˜é£é™©ï¼ˆéœ€é«˜åº¦è­¦æƒ•ï¼‰
1. **[é£é™©åç§°]ï¼š** [å…·ä½“å†…å®¹] | å½±å“ï¼š[å…¨å¸‚åœº/ç‰¹å®šæ¿å—] | é£é™©ç­‰çº§ï¼šâš ï¸âš ï¸âš ï¸ | åº”å¯¹ï¼š[æ“ä½œå»ºè®®]
2. [å…¶ä»–é«˜é£é™©å› ç´ ]
### âš ï¸ ä¸­ç­‰é£é™©ï¼ˆéœ€å…³æ³¨ï¼‰
- [åˆ—å‡º2-3ä¸ªä¸­ç­‰é£é™©]
### ğŸ’¡ æ½œåœ¨é»‘å¤©é¹…
- [ä½æ¦‚ç‡é«˜å½±å“äº‹ä»¶]

## 5. å®æˆ˜æŠ•èµ„ç­–ç•¥ï¼ˆæ ¸å¿ƒï¼‰
### çŸ­æœŸç­–ç•¥ï¼ˆ1-5äº¤æ˜“æ—¥ï¼‰
**æ“ä½œæ–¹å‘ï¼š**
- åšå¤šï¼š[å…·ä½“æ¿å—/ä¸»é¢˜]
- å›é¿ï¼š[éœ€è§„é¿çš„æ¿å—]
**ä»“ä½ç®¡ç†ï¼š**
- å»ºè®®æ€»ä»“ä½ï¼š[50-70%ç­‰å…·ä½“åŒºé—´]
- å•ç¥¨ä¸Šé™ï¼š[5-10%]
- é˜²å®ˆä»“ä½ï¼š[0-20%ç°é‡‘/å€ºåŸº]
**æ‹©æ—¶å»ºè®®ï¼š**
- è¿›åœºæ—¶æœºï¼š[å¼€ç›˜/å›è°ƒ/å°¾ç›˜]
- æ­¢ç›ˆï¼š[å…·ä½“ç™¾åˆ†æ¯”]
- æ­¢æŸï¼š[3-5%]
**å…·ä½“æ“ä½œï¼š**
1. [æ¿å—1]ï¼š[ä¹°å…¥/æŒæœ‰/å‡ä»“]ï¼Œç†ç”±ï¼š[...]
2. [æ¿å—2]ï¼š[...]
3. [æ¿å—3]ï¼š[...]

### ä¸­æœŸç­–ç•¥ï¼ˆ1-4å‘¨ï¼‰
**è¶‹åŠ¿ï¼š** [ä¸Šæ¶¨/éœ‡è¡/ä¸‹è·Œ] | **é‡ç‚¹å…³æ³¨ï¼š** [æˆé•¿/ä»·å€¼/ä¸»é¢˜] | **é…ç½®ï¼š** è‚¡ç¥¨[60-80%] å€ºåˆ¸[20-40%]

### é•¿æœŸç­–ç•¥ï¼ˆ1-6æœˆï¼‰
**æŠ•èµ„ä¸»çº¿ï¼š** [ç§‘æŠ€è‡ªä¸»/æ¶ˆè´¹å‡çº§/æ–°èƒ½æºç­‰] | **æ ¸å¿ƒæŒä»“ï¼š** 1.[è¡Œä¸š1 30-40%] 2.[è¡Œä¸š2 20-30%] 3.[é˜²å®ˆå“ç§ 10-20%]

## 6. å…³é”®ä¿¡æ¯ä¸æ“ä½œè¦ç‚¹
### ğŸ¯ ä»Šæ—¥5ä¸ªæœ€é‡è¦å¸‚åœºä¿¡å·
1. **[ä¿¡å·1]ï¼š** [å†…å®¹] â†’ å½±å“ï¼š[...]
2-5. [å…¶ä»–ä¿¡å·]
### ğŸ’¡ æ ¸å¿ƒæŠ•èµ„é€»è¾‘
**å¤šå¤´é€»è¾‘ï¼š** [æ”¯æŒä¸Šæ¶¨ç†ç”±] | **ç©ºå¤´é€»è¾‘ï¼š** [å¯èƒ½ä¸‹è·Œç†ç”±] | **ä¸­æ€§å› ç´ ï¼š** [ä¸æ˜ç¡®å› ç´ ]
### âœ… ä»Šæ—¥æ“ä½œå»ºè®®ï¼ˆä¸€å¥è¯ï¼‰
[ä»Šå¤©åº”è¯¥æ€ä¹ˆåšçš„ç®€æ´æ€»ç»“ï¼Œå¦‚"å»ºè®®å…³æ³¨ç§‘æŠ€è‚¡å›è°ƒæœºä¼šï¼Œæ§åˆ¶ä»“ä½60%ï¼Œè§„é¿é«˜ä½å‘¨æœŸè‚¡"]

## 7. é£é™©æç¤º
âš ï¸ æœ¬åˆ†æåŸºäºå…¬å¼€ä¿¡æ¯ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚å¸‚åœºæœ‰é£é™©ï¼ŒæŠ•èµ„éœ€è°¨æ…ã€‚

---
**è¦æ±‚ï¼š** 1.å†…å®¹ä¸“ä¸šå®¢è§‚å®ç”¨ 2.æ•°æ®æœ‰ç†æœ‰æ® 3.æ“ä½œå»ºè®®å…·ä½“æ˜ç¡® 4.é£é™©æç¤ºå…¨é¢ 5.è¯­è¨€ç®€æ´ä¸“ä¸š 6.ç¯‡å¹…1500-2500å­—
    """
    
    # è°ƒç”¨AIè¿›è¡Œåˆ†æ
    analysis_result = get_completion_with_custom_params(
        prompt=prompt,
        model=ai_model_name,
        endpoint=ai_endpoint,
        api_key=ai_api_key,
        max_tokens=AI_NEWS_ANALYSIS_MAX_TOKENS,  # ä½¿ç”¨æ–°é—»åˆ†æä¸“ç”¨çš„max_tokens
        temperature=AI_TEMPERATURE
    )
    
    return analysis_result

def get_news_sentiment_analysis(force_refresh: bool = False, ai_model_name: str = None, ai_endpoint: str = None, ai_api_key: str = None) -> Dict[str, Any]:
    """è·å–æ–°é—»æ¶ˆæ¯é¢åˆ†æ
    
    Args:
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°åˆ†æç»“æœ
        ai_model_name: AIæ¨¡å‹åç§°
        ai_endpoint: AIæ¥å£åœ°å€
        ai_api_key: AIæ¥å£å¯†é’¥
        
    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    logger.info(f"å¼€å§‹è·å–æ–°é—»æ¶ˆæ¯é¢åˆ†æ, force_refresh: {force_refresh}, model: {ai_model_name}")
    
    # æ£€æŸ¥ç¼“å­˜
    cache_key = "news:analysis_result"
    if not force_refresh:
        # æ£€æŸ¥Redisç¼“å­˜
        cached_result = redis_cache.get_cache(cache_key)
        if cached_result:
            logger.info("ä½¿ç”¨Redisç¼“å­˜çš„æ¶ˆæ¯é¢åˆ†æç»“æœ")
            return cached_result
        
        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if "news_sentiment" in news_analysis_cache:
            cache_data = news_analysis_cache["news_sentiment"]
            # ç¼“å­˜æœ‰æ•ˆæœŸ6å°æ—¶
            if (datetime.now() - cache_data["timestamp"]).total_seconds() < 21600:
                logger.info("ä½¿ç”¨å†…å­˜ç¼“å­˜çš„æ¶ˆæ¯é¢åˆ†æç»“æœ")
                return cache_data["data"]
    
    # è·å–æœ€æ–°æ–°é—»æ•°æ®
    news_data = redis_cache.get_cache(NEWS_KEYS['news_latest'])
    if not news_data or not news_data.get('news'):
        return {"error": "æ²¡æœ‰æ‰¾åˆ°æ–°é—»æ•°æ®ï¼Œè¯·å…ˆè·å–æ–°é—»"}
    
    news_list = news_data.get('news', [])
    logger.info(f"ä½¿ç”¨Redisç¼“å­˜çš„æ–°é—»æ•°æ®è¿›è¡Œåˆ†æï¼Œæ–°é—»æ•°é‡: {len(news_list)}")
    
    # åˆ†ææ–°é—»æ ‡é¢˜
    try:
        start_time = time.time()
        
        # å‡†å¤‡åˆ†ææ•°æ®
        logger.info(f"å‡†å¤‡åˆ†æ {len(news_list)} æ¡æ–°é—»æ ‡é¢˜")
        
        # è°ƒç”¨AIæ¨¡å‹è¿›è¡Œåˆ†æ
        logger.info(f"è°ƒç”¨AIæ¨¡å‹: {ai_model_name}")
        analysis_text = analyze_news_by_titles(news_list, use_cache=not force_refresh, ai_model_name=ai_model_name, ai_endpoint=ai_endpoint, ai_api_key=ai_api_key)
        
        # å‡†å¤‡è¿”å›æ•°æ®
        result = {
            "analysis": analysis_text,
            "generated_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "model": ai_model_name,
            "news_count": len(news_list)
        }
        
        # æ›´æ–°ç¼“å­˜
        redis_cache.set_cache(cache_key, result, ttl=21600)  # 6å°æ—¶
        news_analysis_cache["news_sentiment"] = {
            "data": result,
            "timestamp": datetime.now()
        }
        
        end_time = time.time()
        logger.info(f"AIåˆ†æå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
        logger.info("æ›´æ–°å…±äº«æ¶ˆæ¯é¢åˆ†æç¼“å­˜åˆ°Rediså’Œå†…å­˜ï¼Œæœ‰æ•ˆæœŸ6å°æ—¶")
        
        return result
        
    except Exception as e:
        logger.error(f"åˆ†ææ–°é—»å¤±è´¥: {str(e)}")
        return {"error": f"åˆ†ææ–°é—»å¤±è´¥: {str(e)}"} 