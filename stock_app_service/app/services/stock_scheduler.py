# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ•°æ®è°ƒåº¦å™¨
ç®€åŒ–é€»è¾‘ï¼Œå¢åŠ ç¨³å®šæ€§
- åˆ é™¤æ—¶æ•ˆæ€§æ£€æŸ¥
- å‘¨1-5æ¯å¤©17:30å…¨é‡æ¸…ç©ºé‡æ–°è·å–å†å²æ•°æ®
- 15:35æ”¶ç›˜ååˆæ­¥ä¿¡å·è®¡ç®—ï¼Œ17:35æœ€ç»ˆä¿¡å·è®¡ç®—
- å®æ—¶æ•°æ®æ›´æ–°æ—¶è‡ªåŠ¨åˆå¹¶åˆ°Kçº¿æ•°æ®
"""

import asyncio
import threading
import traceback
from datetime import datetime, time, timedelta
from typing import Dict, Any, List
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger

from app.core.logging import logger
from app.core.config import settings
from app.db.session import RedisCache
from app.services.stock_data_manager import StockDataManager
# ç§»é™¤å…¨å±€çº¿ç¨‹æ± å¯¼å…¥ï¼Œschedulerä¸åº”è¯¥å½±å“APIæœåŠ¡
# from app.core.thread_pool import global_thread_pool
from app.services.signal_manager import signal_manager
import akshare as ak
import pandas as pd
import json

# Redisç¼“å­˜å®¢æˆ·ç«¯
redis_cache = RedisCache()

# è°ƒåº¦å™¨å®ä¾‹
scheduler = None
job_logs = []  # å­˜å‚¨æœ€è¿‘çš„ä»»åŠ¡æ‰§è¡Œæ—¥å¿—

# ä¿¡å·è®¡ç®—é”ï¼Œé˜²æ­¢é‡å¤æ‰§è¡Œ
_signal_calculation_lock = threading.Lock()
_signal_calculation_running = False

# Redisé”®åè§„åˆ™
STOCK_KEYS = {
    'stock_codes': 'stocks:codes:all',               # è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆä¿®æ­£ï¼šåº”ä¸ºstocks:codes:allï¼‰
    'stock_kline': 'stock_trend:{}',                 # Kçº¿æ•°æ®æ ¼å¼ï¼Œéœ€è¦ç”¨ts_codeå¡«å……
    'strategy_signals': 'stock:buy_signals',         # ç­–ç•¥ä¿¡å·
    'realtime_data': 'stock:realtime',               # å®æ—¶æ•°æ®
    'scheduler_log': 'stock:scheduler:log',          # è°ƒåº¦å™¨æ—¥å¿—
    'last_update': 'stock:last_update',              # æœ€åæ›´æ–°æ—¶é—´
}

def add_stock_job_log(job_type: str, status: str, message: str, count: int = 0, execution_time: float = 0.0):
    """æ·»åŠ è‚¡ç¥¨ä»»åŠ¡æ‰§è¡Œæ—¥å¿—"""
    log_entry = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'job_type': job_type,
        'status': status,
        'message': message,
        'count': count,
        'execution_time': round(execution_time, 2)
    }
    
    # å†…å­˜æ—¥å¿—ï¼ˆæœ€è¿‘10æ¡ï¼‰
    global job_logs
    job_logs.insert(0, log_entry)
    job_logs = job_logs[:10]
    
    # Redisæ—¥å¿—ï¼ˆæœ€è¿‘20æ¡ï¼‰
    redis_logs = redis_cache.get_cache(STOCK_KEYS['scheduler_log']) or []
    redis_logs.insert(0, log_entry)
    redis_logs = redis_logs[:20]
    redis_cache.set_cache(STOCK_KEYS['scheduler_log'], redis_logs, ttl=86400)
    
    logger.info(f"[{job_type}] {message}")

def is_trading_time() -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´"""
    now = datetime.now()
    
    # å‘¨æœ«ä¸äº¤æ˜“
    if now.weekday() >= 5:  # 5=å‘¨å…­, 6=å‘¨æ—¥
        return False
    
    current_time = now.time()
    
    # äº¤æ˜“æ—¶é—´: 9:30-11:30, 13:00-15:00
    morning_start = time(9, 30)
    morning_end = time(11, 30)
    afternoon_start = time(13, 0)
    afternoon_end = time(15, 0)
    
    return ((morning_start <= current_time <= morning_end) or 
            (afternoon_start <= current_time <= afternoon_end))

def is_trading_day() -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¥ï¼ˆå‘¨ä¸€åˆ°å‘¨äº”ï¼‰"""
    return datetime.now().weekday() < 5

# ===================== ä»»åŠ¡å‡½æ•° =====================

def _init_etf_only():
    """ä»…åˆå§‹åŒ– ETF æ•°æ®ï¼ˆåŒ…æ‹¬æ¸…å•å’ŒKçº¿æ•°æ®ï¼‰"""
    try:
        logger.info("========== å¼€å§‹ ETF ä¸“é¡¹åˆå§‹åŒ– ==========")
        
        def run_etf_init():
            """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œ ETF åˆå§‹åŒ–"""
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                async def init_etf():
                    # åˆå§‹åŒ– StockDataManager
                    from app.services.stock_data_manager import StockDataManager
                    sdm = StockDataManager()
                    await sdm.initialize()
                    
                    # 1. åˆå§‹åŒ– ETF æ¸…å•
                    logger.info("æ­¥éª¤ 1: åˆå§‹åŒ– ETF æ¸…å•...")
                    etf_success = await sdm.initialize_etf_list()
                    if not etf_success:
                        logger.error("ETF æ¸…å•åˆå§‹åŒ–å¤±è´¥")
                        return False
                    
                    # 2. è·å– ETF åˆ—è¡¨ï¼ˆä» CSVï¼Œå·²è¿‡æ»¤ LOFï¼‰
                    from app.services.etf_manager import etf_manager
                    etf_list = etf_manager.get_etf_list(enrich=False, use_csv=True)
                    
                    if not etf_list:
                        logger.error("æ— æ³•è·å– ETF åˆ—è¡¨")
                        return False
                    
                    logger.info(f"æ­¥éª¤ 2: è·å– {len(etf_list)} ä¸ª ETF çš„Kçº¿æ•°æ®...")
                    
                    # 3. è·å– ETF Kçº¿æ•°æ®
                    success_count = 0
                    failed_count = 0
                    
                    for i, etf in enumerate(etf_list, 1):
                        ts_code = etf['ts_code']
                        try:
                            # è·å–180å¤©Kçº¿æ•°æ®
                            success = await sdm.update_stock_trend_data(ts_code, days=180)
                            if success:
                                success_count += 1
                                logger.info(f"[{i}/{len(etf_list)}] âœ… {ts_code} {etf['name']}")
                            else:
                                failed_count += 1
                                logger.warning(f"[{i}/{len(etf_list)}] âŒ {ts_code} {etf['name']} - è·å–å¤±è´¥")
                        except Exception as e:
                            failed_count += 1
                            logger.error(f"[{i}/{len(etf_list)}] âŒ {ts_code} {etf['name']} - é”™è¯¯: {e}")
                    
                    logger.info(f"âœ… ETF Kçº¿æ•°æ®è·å–å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
                    
                    # 4. å…ˆè®¡ç®—è‚¡ç¥¨ä¿¡å·ï¼ˆä¼˜å…ˆï¼Œæ¸…ç©ºæ—§ä¿¡å·ï¼‰
                    logger.info("æ­¥éª¤ 3: è®¡ç®—è‚¡ç¥¨ä¹°å…¥ä¿¡å·ï¼ˆä¼˜å…ˆï¼Œæ¸…ç©ºæ—§ä¿¡å·ï¼‰...")
                    await _calculate_signals_async(stock_only=True, clear_existing=True)
                    
                    # 5. å†è®¡ç®— ETF ä¿¡å·ï¼ˆè¿½åŠ ï¼Œä¸æ¸…ç©ºï¼‰
                    logger.info("æ­¥éª¤ 4: è®¡ç®— ETF ä¹°å…¥ä¿¡å·ï¼ˆè¿½åŠ åˆ°è‚¡ç¥¨ä¿¡å·åï¼‰...")
                    await _calculate_signals_async(etf_only=True, clear_existing=False)
                    
                    logger.info("========== ETF ä¸“é¡¹åˆå§‹åŒ–å®Œæˆ ==========")
                    await sdm.close()
                    return True
                
                loop.run_until_complete(init_etf())
            except Exception as e:
                logger.error(f"ETF åˆå§‹åŒ–å¤±è´¥: {e}")
                import traceback
                logger.error(traceback.format_exc())
            finally:
                loop.close()
        
        # åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œ
        init_thread = threading.Thread(target=run_etf_init, daemon=True)
        init_thread.start()
        logger.info("ETF åˆå§‹åŒ–ä»»åŠ¡å·²åœ¨åå°å¯åŠ¨")
        
    except Exception as e:
        logger.error(f"å¯åŠ¨ ETF åˆå§‹åŒ–å¤±è´¥: {e}")

def init_stock_system(mode: str = "tasks_only"):
    """åˆå§‹åŒ–è‚¡ç¥¨ç³»ç»Ÿæ•°æ®
    
    Args:
        mode: åˆå§‹åŒ–æ¨¡å¼
            - "skip": è·³è¿‡åˆå§‹åŒ–ï¼Œå¯åŠ¨æ—¶ä»€ä¹ˆéƒ½ä¸æ‰§è¡Œï¼Œç­‰å¾…æ‰‹åŠ¨è§¦å‘
            - "tasks_only": ä»…æ‰§è¡Œä»»åŠ¡ï¼Œä¸è·å–å†å²Kçº¿æ•°æ®ï¼Œåªæ‰§è¡Œä¿¡å·è®¡ç®—ã€æ–°é—»è·å–ç­‰ä»»åŠ¡
            - "full_init": å®Œæ•´åˆå§‹åŒ–ï¼Œæ¸…ç©ºæ‰€æœ‰æ•°æ®ï¼ˆè‚¡ç¥¨+ETFï¼‰é‡æ–°è·å–
            - "etf_only": ä»…åˆå§‹åŒ–ETFï¼Œåªè·å–å’Œæ›´æ–°ETFæ•°æ®
            
        æ³¨æ„ï¼šä¸ºäº†å‘åå…¼å®¹ï¼Œä»ç„¶æ”¯æŒæ—§æ¨¡å¼åç§°ï¼ˆnone, only_tasks, clear_allï¼‰
    """
    start_time = datetime.now()
    
    # å‘åå…¼å®¹ï¼šæ˜ å°„æ—§æ¨¡å¼åç§°
    mode_mapping = {
        "none": "skip",
        "only_tasks": "tasks_only",
        "clear_all": "full_init"
    }
    if mode in mode_mapping:
        old_mode = mode
        mode = mode_mapping[mode]
        logger.info(f"æ£€æµ‹åˆ°æ—§æ¨¡å¼åç§° '{old_mode}'ï¼Œè‡ªåŠ¨æ˜ å°„ä¸ºæ–°æ¨¡å¼ '{mode}'")
    
    try:
        if mode == "skip":
            logger.info("ç”¨æˆ·é€‰æ‹©ã€skipã€‘æ¨¡å¼ - å¯åŠ¨æ—¶ä»€ä¹ˆéƒ½ä¸æ‰§è¡Œ")
            execution_time = (datetime.now() - start_time).total_seconds()
            add_stock_job_log('init_system', 'success', 'skipæ¨¡å¼: ä¸æ‰§è¡Œä»»ä½•åˆå§‹åŒ–', 0, execution_time)
            return
        
        # ç‰¹æ®Šæ¨¡å¼ï¼šä»…åˆå§‹åŒ– ETF
        if mode == "etf_only":
            logger.info("ç”¨æˆ·é€‰æ‹©ã€etf_onlyã€‘æ¨¡å¼ - ä»…åˆå§‹åŒ– ETF æ•°æ®")
            _init_etf_only()
            execution_time = (datetime.now() - start_time).total_seconds()
            add_stock_job_log('init_system', 'success', 'etf_onlyæ¨¡å¼: ä»…åˆå§‹åŒ–ETF', 0, execution_time)
            return
        
        # tasks_only å’Œ full_init æ¨¡å¼éƒ½éœ€è¦ç»§ç»­æ‰§è¡Œï¼Œåªæ˜¯åœ¨Kçº¿æ•°æ®å¤„ç†ä¸Šæœ‰åŒºåˆ«
        if mode == "tasks_only":
            logger.info("ç”¨æˆ·é€‰æ‹©ã€tasks_onlyã€‘æ¨¡å¼ - è·³è¿‡Kçº¿æ•°æ®è·å–ï¼Œå…¶ä»–è®¡åˆ’ä»»åŠ¡æ­£å¸¸æ‰§è¡Œ")
        
        # å…¶ä»–æ¨¡å¼éœ€è¦è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼Œç½‘ç»œè¯·æ±‚ä½œä¸ºå¤‡é€‰ï¼‰
        logger.info("ğŸ“¥ æ­£åœ¨è·å–è‚¡ç¥¨åˆ—è¡¨...")
        
        # é¦–å…ˆå°è¯•ä»ç¼“å­˜è·å–
        stock_codes = redis_cache.get_cache(STOCK_KEYS['stock_codes'])
        
        if not stock_codes or len(stock_codes) < 100:
            logger.warning(" ç¼“å­˜ä¸­æ— è‚¡ç¥¨æ•°æ®æˆ–æ•°æ®ä¸å®Œæ•´ï¼Œå°è¯•ä»ç½‘ç»œåˆ·æ–°...")
            refresh_result = refresh_stock_list()
            if refresh_result.get('success'):
                stock_codes = redis_cache.get_cache(STOCK_KEYS['stock_codes'])
                logger.info(f" æˆåŠŸä»ç½‘ç»œåˆ·æ–°è‚¡ç¥¨åˆ—è¡¨: {len(stock_codes)}åª")
            else:
                logger.error(" è‡ªåŠ¨åˆ·æ–°è‚¡ç¥¨åˆ—è¡¨å¤±è´¥ï¼Œæ— æ³•ç»§ç»­åˆå§‹åŒ–")
                error_msg = refresh_result.get('error', 'æœªçŸ¥é”™è¯¯')
                add_stock_job_log('init_system', 'failed', f'åˆ·æ–°è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {error_msg}')
                return
        else:
            logger.info(f" ä½¿ç”¨ç¼“å­˜ä¸­çš„è‚¡ç¥¨åˆ—è¡¨: {len(stock_codes)}åª")

        if not stock_codes:
            logger.error(" æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œåˆå§‹åŒ–ä¸­æ–­")
            add_stock_job_log('init_system', 'failed', 'æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨')
            return
        
        # Kçº¿æ•°æ®å¤„ç†ï¼štasks_only è·³è¿‡ï¼Œfull_init æ‰§è¡Œ
        if mode == "full_init":
            logger.info("ç”¨æˆ·é€‰æ‹©ã€full_initã€‘æ¨¡å¼ - æ¸…ç©ºæ‰€æœ‰æ•°æ®ï¼ˆè‚¡ç¥¨+ETFï¼‰é‡æ–°è·å–")
            
            # æ¸…ç©ºæ‰€æœ‰å†å²æ•°æ®
            logger.info("æ­£åœ¨æ¸…ç©ºæ‰€æœ‰è‚¡ç¥¨å†å²æ•°æ®...")
            cleared_count = 0
            for stock in stock_codes:
                ts_code = stock.get('ts_code')
                if ts_code:
                    # ä½¿ç”¨ä¸¤ç§é”®æ ¼å¼ç¡®ä¿å®Œå…¨æ¸…ç©º
                    key1 = STOCK_KEYS['stock_kline'].format(ts_code)  # æ—§æ ¼å¼
                    key2 = f"stock_trend:{ts_code}"  # æ–°æ ¼å¼
                    
                    # åˆ é™¤ä¸¤ç§å¯èƒ½çš„é”®
                    if redis_cache.redis_client.delete(key1):
                        cleared_count += 1
                    if redis_cache.redis_client.delete(key2):
                        logger.debug(f"é¢å¤–æ¸…ç©ºæ–°æ ¼å¼é”®: {key2}")
                    
            logger.info(f"å·²æ¸…ç©º {cleared_count} åªè‚¡ç¥¨çš„Kçº¿æ•°æ®")
            
            # æ¸…ç©ºä¿¡å·æ•°æ®
            redis_cache.redis_client.delete(STOCK_KEYS['strategy_signals'])
            logger.info("å·²æ¸…ç©ºç­–ç•¥ä¿¡å·æ•°æ®")
        
        # ç»Ÿä¸€çš„åå°ä»»åŠ¡å¤„ç†å‡½æ•°
        def run_background_tasks():
            """è¿è¡Œåå°ä»»åŠ¡ï¼šKçº¿æ•°æ®è·å–ï¼ˆå¯é€‰ï¼‰+ ä¿¡å·è®¡ç®—ç­‰å…¶ä»–ä»»åŠ¡"""
            try:
                # åˆ›å»ºç‹¬ç«‹çš„äº‹ä»¶å¾ªç¯ç”¨äºåå°ä»»åŠ¡
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                async def execute_tasks():
                    try:
                        # æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦è·å–Kçº¿æ•°æ®
                        if mode == "full_init":
                            logger.info("å¼€å§‹é‡æ–°è·å–æ‰€æœ‰è‚¡ç¥¨å’Œ ETF å†å²æ•°æ®...")
                            await _fetch_all_kline_data(stock_codes)
                            logger.info("Kçº¿æ•°æ®è·å–å®Œæˆ")
                        elif mode == "tasks_only":
                            logger.info("tasks_onlyæ¨¡å¼ï¼šè·³è¿‡Kçº¿æ•°æ®è·å–")
                        
                        # æ‰€æœ‰æ¨¡å¼éƒ½æ‰§è¡Œä¿¡å·è®¡ç®—ç­‰å…¶ä»–ä»»åŠ¡
                        logger.info("å¼€å§‹è®¡ç®—ä¹°å…¥ä¿¡å·ï¼ˆè‚¡ç¥¨+ETFï¼‰...")
                        await _calculate_signals_async()
                        logger.info("ä¹°å…¥ä¿¡å·è®¡ç®—å®Œæˆ")
                        
                    except Exception as e:
                        logger.error(f"åå°ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                
                loop.run_until_complete(execute_tasks())
            except Exception as e:
                logger.error(f"åå°çº¿ç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            finally:
                try:
                    loop.close()
                except Exception:
                    pass
        
        # å¯åŠ¨åå°ä»»åŠ¡çº¿ç¨‹
        task_thread = threading.Thread(target=run_background_tasks, daemon=True)
        task_thread.start()
        
        execution_time = (datetime.now() - start_time).total_seconds()
        if mode == "full_init":
            logger.info(f"full_initæ¨¡å¼å¯åŠ¨å®Œæˆï¼ŒKçº¿æ•°æ®è·å–å’Œä¿¡å·è®¡ç®—æ­£åœ¨åå°æ‰§è¡Œï¼Œè€—æ—¶ {execution_time:.2f}ç§’")
            add_stock_job_log('init_system', 'success', f'full_initæ¨¡å¼å¯åŠ¨: {len(stock_codes)}åªè‚¡ç¥¨+ETF', len(stock_codes), execution_time)
        elif mode == "tasks_only":
            logger.info(f"tasks_onlyæ¨¡å¼å¯åŠ¨å®Œæˆï¼Œä¿¡å·è®¡ç®—ç­‰ä»»åŠ¡æ­£åœ¨åå°æ‰§è¡Œï¼Œè€—æ—¶ {execution_time:.2f}ç§’")
            add_stock_job_log('init_system', 'success', f'tasks_onlyæ¨¡å¼å¯åŠ¨: {len(stock_codes)}åªè‚¡ç¥¨+ETF', len(stock_codes), execution_time)
            
        
    except Exception as e:
        execution_time = (datetime.now() - start_time).total_seconds()
        error_msg = f'è‚¡ç¥¨ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}'
        logger.error(f" {error_msg}")
        add_stock_job_log('init_system', 'failed', error_msg, 0, execution_time)

def clear_and_refetch_kline_data():
    """æ¸…ç©ºå¹¶é‡æ–°è·å–æ‰€æœ‰è‚¡ç¥¨Kçº¿æ•°æ®ï¼ˆæ¯å¤©17:30æ‰§è¡Œï¼‰"""
    current_time = datetime.now()
    logger.info(f"========== 17:30å®šæ—¶ä»»åŠ¡è§¦å‘ ==========")
    logger.info(f"å½“å‰æ—¶é—´: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"æ˜ŸæœŸ: {current_time.strftime('%A')}")
    
    if not is_trading_day():
        logger.info("âš ï¸ éäº¤æ˜“æ—¥ï¼Œè·³è¿‡Kçº¿æ•°æ®æ›´æ–°")
        add_stock_job_log('clear_refetch', 'skipped', 'éäº¤æ˜“æ—¥è·³è¿‡', 0, 0)
        return
    
    start_time = datetime.now()
    
    try:
        logger.info("âœ… äº¤æ˜“æ—¥ç¡®è®¤ï¼Œå¼€å§‹æ‰§è¡ŒKçº¿æ•°æ®å…¨é‡æ›´æ–°ä»»åŠ¡...")
        logger.info("æ­¥éª¤ 1/4: è·å–è‚¡ç¥¨åˆ—è¡¨")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_codes = redis_cache.get_cache(STOCK_KEYS['stock_codes'])
        if not stock_codes:
            logger.error("âŒ è‚¡ç¥¨ä»£ç åˆ—è¡¨ä¸ºç©ºï¼Œè¯·å…ˆæ‰§è¡Œè‚¡ç¥¨ä»£ç åˆå§‹åŒ–")
            raise Exception("è‚¡ç¥¨ä»£ç åˆ—è¡¨ä¸ºç©ºï¼Œè¯·å…ˆæ‰§è¡Œè‚¡ç¥¨ä»£ç åˆå§‹åŒ–")
        
        logger.info(f"âœ… è·å–åˆ° {len(stock_codes)} åªè‚¡ç¥¨")
        
        # æ¸…ç©ºæ‰€æœ‰Kçº¿æ•°æ® - ä½¿ç”¨æ›´å®‰å…¨çš„æ¸…ç©ºæ–¹å¼
        logger.info("æ­¥éª¤ 2/4: æ¸…ç©ºæ‰€æœ‰Kçº¿æ•°æ®ï¼ˆåŒ…æ‹¬æ–°æ—§æ ¼å¼ï¼‰")
        cleared_count = 0
        old_format_cleared = 0
        new_format_cleared = 0
        
        for stock in stock_codes:
            ts_code = stock.get('ts_code')
            if ts_code:
                # ä½¿ç”¨ä¸¤ç§é”®æ ¼å¼ç¡®ä¿å®Œå…¨æ¸…ç©º
                key1 = STOCK_KEYS['stock_kline'].format(ts_code)  # æ—§æ ¼å¼
                key2 = f"stock_trend:{ts_code}"  # æ–°æ ¼å¼
                
                # åˆ é™¤æ—§æ ¼å¼é”®
                if redis_cache.redis_client.delete(key1):
                    old_format_cleared += 1
                    cleared_count += 1
                    
                # åˆ é™¤æ–°æ ¼å¼é”®
                if redis_cache.redis_client.delete(key2):
                    new_format_cleared += 1
                    
        logger.info(f"âœ… å·²æ¸…ç©ºKçº¿æ•°æ®:")
        logger.info(f"   - æ—§æ ¼å¼: {old_format_cleared} åª")
        logger.info(f"   - æ–°æ ¼å¼: {new_format_cleared} åª")
        logger.info(f"   - æ€»è®¡: {cleared_count} åª")
        
        # æ¸…ç©ºä¿¡å·æ•°æ®ï¼ˆé‡è¦ï¼šé¿å…åŸºäºæ—§æ•°æ®çš„ä¿¡å·æ®‹ç•™ï¼‰
        logger.info("æ­¥éª¤ 3/4: æ¸…ç©ºç­–ç•¥ä¿¡å·æ•°æ®")
        redis_cache.redis_client.delete(STOCK_KEYS['strategy_signals'])
        logger.info("âœ… å·²æ¸…ç©ºç­–ç•¥ä¿¡å·æ•°æ®")
        
        # é‡æ–°è·å–Kçº¿æ•°æ®
        logger.info("æ­¥éª¤ 4/4: é‡æ–°è·å–æ‰€æœ‰è‚¡ç¥¨Kçº¿æ•°æ®")
        logger.info(f"   é¢„è®¡éœ€è¦æ—¶é—´: {len(stock_codes) * 0.5 / 60:.1f} åˆ†é’Ÿ")
        
        def run_async_fetch():
            """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥è·å–"""
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                logger.info("ğŸ”„ å¼‚æ­¥æ•°æ®è·å–ä»»åŠ¡å¯åŠ¨...")
                loop.run_until_complete(_fetch_all_kline_data(stock_codes))
                logger.info("âœ… å¼‚æ­¥æ•°æ®è·å–ä»»åŠ¡å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ å¼‚æ­¥æ•°æ®è·å–ä»»åŠ¡å¤±è´¥: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
            finally:
                loop.close()
        
        # åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
        fetch_thread = threading.Thread(target=run_async_fetch, daemon=True)
        fetch_thread.start()
        logger.info("â³ ç­‰å¾…æ•°æ®è·å–å®Œæˆï¼ˆæœ€é•¿1å°æ—¶ï¼‰...")
        fetch_thread.join(timeout=3600)  # æœ€å¤šç­‰å¾…1å°æ—¶
        
        if fetch_thread.is_alive():
            logger.warning("âš ï¸ æ•°æ®è·å–ä»»åŠ¡è¶…æ—¶ï¼ˆ1å°æ—¶ï¼‰ï¼Œä½†ä»»åŠ¡ä»åœ¨åå°ç»§ç»­æ‰§è¡Œ")
        
        execution_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"âœ… Kçº¿æ•°æ®å…¨é‡æ›´æ–°å®Œæˆï¼Œè€—æ—¶ {execution_time:.2f}ç§’ ({execution_time/60:.1f}åˆ†é’Ÿ)")
        
        add_stock_job_log('clear_refetch', 'success', f'Kçº¿æ•°æ®å…¨é‡æ›´æ–°å®Œæˆ: {len(stock_codes)}åª', len(stock_codes), execution_time)
        
        # Kçº¿å…¨é‡æ›´æ–°å®Œæˆåï¼Œè‡ªåŠ¨è§¦å‘ä¹°å…¥ä¿¡å·è®¡ç®—
        logger.info("ğŸ”„ Kçº¿æ•°æ®å…¨é‡æ›´æ–°å®Œæˆï¼Œè‡ªåŠ¨è§¦å‘ä¹°å…¥ä¿¡å·é‡æ–°è®¡ç®—...")
        _trigger_signal_recalculation_async()
        logger.info("========== 17:30å®šæ—¶ä»»åŠ¡å®Œæˆ ==========")
        
    except Exception as e:
        execution_time = (datetime.now() - start_time).total_seconds()
        error_msg = f'Kçº¿æ•°æ®å…¨é‡æ›´æ–°å¤±è´¥: {str(e)}'
        logger.error(f"âŒ {error_msg}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
        add_stock_job_log('clear_refetch', 'failed', error_msg, 0, execution_time)
        logger.info("========== 17:30å®šæ—¶ä»»åŠ¡å¤±è´¥ ==========")

async def _fetch_all_kline_data(stock_codes: List[Dict]):
    """å¼‚æ­¥è·å–æ‰€æœ‰è‚¡ç¥¨Kçº¿æ•°æ®"""
    # åˆ›å»ºè‚¡ç¥¨æ•°æ®ç®¡ç†å™¨ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å¤šçº¿ç¨‹è®¾ç½®
    stock_data_manager = StockDataManager()
    await stock_data_manager.initialize()
    
    try:
        success_count = 0
        failed_count = 0
        batch_size = 10  # å‡å°æ‰¹å¤„ç†å¤§å°ï¼Œé¿å…é˜»å¡APIæœåŠ¡
        
        total_batches = (len(stock_codes) + batch_size - 1) // batch_size
        
        for i in range(0, len(stock_codes), batch_size):
            batch = stock_codes[i:i + batch_size]
            current_batch = i // batch_size + 1
            
            logger.info(f" å¤„ç†ç¬¬ {current_batch}/{total_batches} æ‰¹è‚¡ç¥¨ ({len(batch)} åª)")
            
            # ä¸²è¡Œå¤„ç†ä»¥é¿å…å ç”¨è¿‡å¤šèµ„æºå½±å“APIæœåŠ¡
            for j, stock in enumerate(batch):
                try:
                    thread_id = f"batch_{current_batch}_stock_{j}" if stock_data_manager.use_multithreading else None
                    result = await _fetch_single_stock_data(stock_data_manager, stock, thread_id)
                    
                    if isinstance(result, Exception):
                        failed_count += 1
                    elif result:
                        success_count += 1
                    else:
                        failed_count += 1
                        
                    # æ ¹æ®åå°ä»»åŠ¡ä¼˜å…ˆçº§è®¾ç½®ä¼‘æ¯æ—¶é—´
                    if settings.BACKGROUND_TASK_PRIORITY == "low":
                        await asyncio.sleep(0.5)  # ä½ä¼˜å…ˆçº§ï¼šæ›´å¤šä¼‘æ¯æ—¶é—´
                    elif settings.BACKGROUND_TASK_PRIORITY == "normal":
                        await asyncio.sleep(0.2)  # æ­£å¸¸ä¼˜å…ˆçº§
                    else:  # high priority
                        await asyncio.sleep(0.1)  # é«˜ä¼˜å…ˆçº§ï¼šæœ€å°‘ä¼‘æ¯æ—¶é—´
                        
                except Exception as e:
                    logger.error(f"å¤„ç†è‚¡ç¥¨å¼‚å¸¸: {e}")
                    failed_count += 1
            
            logger.info(f" ç¬¬ {current_batch} æ‰¹å®Œæˆ | æ€»è®¡æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯ï¼Œé¿å…é¢‘ç‡é™åˆ¶ï¼ŒåŒæ—¶é‡Šæ”¾èµ„æºç»™APIæœåŠ¡
            await asyncio.sleep(2)
        
        logger.info(f" Kçº¿æ•°æ®è·å–å®Œæˆ: æˆåŠŸ {success_count} åª, å¤±è´¥ {failed_count} åª")
        
    finally:
        await stock_data_manager.close()

async def _fetch_single_stock_data(manager: StockDataManager, stock: Dict, thread_id: str = None) -> bool:
    """è·å–å•åªè‚¡ç¥¨æ•°æ®"""
    try:
        ts_code = stock.get('ts_code')
        if not ts_code:
            logger.warning(f" è‚¡ç¥¨æ•°æ®ç¼ºå°‘ts_code: {stock}")
            return False
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨çº¿ç¨‹æ§åˆ¶
        if manager.use_multithreading and thread_id:
            logger.debug(f"[{thread_id}] å¼€å§‹è·å–è‚¡ç¥¨æ•°æ®")
            success = await manager.update_stock_trend_data(ts_code, days=180)
        else:
            # ç›´æ¥è·å–æ•°æ®
            success = await manager.update_stock_trend_data(ts_code, days=180)
        
        if success:
            logger.debug(f" {ts_code} æ•°æ®è·å–æˆåŠŸ")
            return True
        else:
            logger.warning(f" {ts_code} æ•°æ®è·å–å¤±è´¥ï¼šæ— æ³•è·å–å†å²æ•°æ®")
            return False
        
    except Exception as e:
        # æ”¹ä¸ºwarningçº§åˆ«ï¼Œè¿™æ ·èƒ½çœ‹åˆ°é”™è¯¯ä¿¡æ¯
        logger.warning(f" è·å– {stock.get('ts_code', 'unknown')} æ•°æ®å¤±è´¥: {e}")
        return False

def calculate_strategy_signals():
    """è®¡ç®—ç­–ç•¥ä¹°å…¥ä¿¡å·ï¼ˆäº¤æ˜“æ—¶é—´å†…æ¯30åˆ†é’Ÿæ‰§è¡Œï¼Œ15:00åé¢å¤–æ‰§è¡Œï¼‰"""
    if not is_trading_day():
        logger.info("éäº¤æ˜“æ—¥ï¼Œè·³è¿‡ç­–ç•¥ä¿¡å·è®¡ç®—")
        return

    start_time = datetime.now()
    
    try:
        logger.info(" å¼€å§‹è®¡ç®—ç­–ç•¥ä¹°å…¥ä¿¡å·...")
        
        def run_async_calc():
            """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥è®¡ç®—"""
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(_calculate_signals_async())
            finally:
                loop.close()
        
        # åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡ï¼Œä¸ç­‰å¾…å®Œæˆ
        calc_thread = threading.Thread(target=run_async_calc, daemon=True)
        calc_thread.start()
        # ä¸ç­‰å¾…çº¿ç¨‹å®Œæˆï¼Œé¿å…é˜»å¡ä¸»è¿›ç¨‹
        
        execution_time = (datetime.now() - start_time).total_seconds()
        logger.info(f" ç­–ç•¥ä¿¡å·è®¡ç®—å®Œæˆï¼Œè€—æ—¶ {execution_time:.2f}ç§’")
        
        add_stock_job_log('calc_signals', 'success', 'ç­–ç•¥ä¿¡å·è®¡ç®—å®Œæˆ', 0, execution_time)
        
    except Exception as e:
        execution_time = (datetime.now() - start_time).total_seconds()
        error_msg = f'ç­–ç•¥ä¿¡å·è®¡ç®—å¤±è´¥: {str(e)}'
        logger.error(f" {error_msg}")
        add_stock_job_log('calc_signals', 'failed', error_msg, 0, execution_time)

async def _calculate_signals_async(etf_only: bool = False, stock_only: bool = False, clear_existing: bool = True):
    """
    å¼‚æ­¥è®¡ç®—ä¿¡å·
    
    Args:
        etf_only: æ˜¯å¦ä»…è®¡ç®— ETF ä¿¡å·ï¼ˆTrue=ä»…ETF, False=å…¨éƒ¨æˆ–ä»…è‚¡ç¥¨ï¼‰
        stock_only: æ˜¯å¦ä»…è®¡ç®—è‚¡ç¥¨ä¿¡å·ï¼ˆTrue=ä»…è‚¡ç¥¨, False=å…¨éƒ¨æˆ–ä»…ETFï¼‰
        clear_existing: æ˜¯å¦æ¸…ç©ºç°æœ‰ä¿¡å·ï¼ˆé»˜è®¤Trueï¼Œè¿½åŠ æ¨¡å¼è®¾ä¸ºFalseï¼‰
    """
    from app.services.signal_manager import SignalManager
    
    local_signal_manager = None
    try:
        local_signal_manager = SignalManager()
        await local_signal_manager.initialize()
        result = await local_signal_manager.calculate_buy_signals(
            force_recalculate=True,
            etf_only=etf_only,
            stock_only=stock_only,
            clear_existing=clear_existing
        )
        
        if result.get('status') == 'success':
            total_signals = result.get('total_signals', 0)
            if etf_only:
                signal_type = "ETF"
            elif stock_only:
                signal_type = "è‚¡ç¥¨"
            else:
                signal_type = "è‚¡ç¥¨+ETF"
            mode = "è¿½åŠ " if not clear_existing else "é‡æ–°è®¡ç®—"
            logger.info(f"âœ… {signal_type}ä¹°å…¥ä¿¡å·{mode}å®Œæˆ: ç”Ÿæˆ {total_signals} ä¸ªä¿¡å·")
        else:
            logger.warning(f"âŒ ä¹°å…¥ä¿¡å·è®¡ç®—å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            
    except Exception as e:
        logger.error(f" ä¹°å…¥ä¿¡å·è®¡ç®—å¼‚å¸¸: {e}")
    finally:
        if local_signal_manager:
            try:
                await local_signal_manager.close()
            except Exception as e:
                logger.error(f"SignalManagerå…³é—­å¤±è´¥: {e}")

# å·²åˆ é™¤calculate_final_strategy_signalså‡½æ•°ï¼Œå› ä¸ºå®æ—¶æ›´æ–°å·²å»¶é•¿åˆ°15:20
# åœ¨Kçº¿å…¨é‡æ›´æ–°å’Œå®æ—¶æ›´æ–°æ—¶ä¼šè‡ªåŠ¨è§¦å‘ä¹°å…¥ä¿¡å·è®¡ç®—

def update_realtime_stock_data(force_update=False, is_closing_update=False, auto_calculate_signals=False):
    """æ›´æ–°å®æ—¶è‚¡ç¥¨æ•°æ®ï¼ˆäº¤æ˜“æ—¶é—´å†…æ¯20åˆ†é’Ÿæ‰§è¡Œï¼‰
    
    Args:
        force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°ï¼Œå¿½ç•¥äº¤æ˜“æ—¶é—´æ£€æŸ¥
        is_closing_update: æ˜¯å¦ä¸ºæ”¶ç›˜åæ›´æ–°ï¼Œä½¿ç”¨ä¸åŒçš„æ•°æ®æº
        auto_calculate_signals: æ˜¯å¦è‡ªåŠ¨è®¡ç®—ä¹°å…¥ä¿¡å·
    """
    if not force_update and not is_trading_time():
        logger.info("éäº¤æ˜“æ—¶é—´ï¼Œè·³è¿‡å®æ—¶æ•°æ®æ›´æ–°")
        return

    start_time = datetime.now()
    
    try:
        if is_closing_update:
            logger.info(" å¼€å§‹æ›´æ–°æ”¶ç›˜è‚¡ç¥¨æ•°æ®ï¼ˆä½¿ç”¨æ”¶ç›˜ä»·ï¼‰...")
        else:
            logger.info(" å¼€å§‹æ›´æ–°å®æ—¶è‚¡ç¥¨æ•°æ®...")
        
        # è·å–Aè‚¡å®æ—¶è¡Œæƒ…æ•°æ®
        df = ak.stock_zh_a_spot_em()
        
        if df.empty:
            raise Exception("è·å–å®æ—¶æ•°æ®å¤±è´¥")
        
        # è½¬æ¢æ•°æ®æ ¼å¼
        realtime_data = []
        for _, row in df.iterrows():
            stock_data = {
                'code': row['ä»£ç '],
                'name': row['åç§°'],
                'price': float(row['æœ€æ–°ä»·']) if pd.notna(row['æœ€æ–°ä»·']) else 0,
                'change': float(row['æ¶¨è·Œé¢']) if pd.notna(row['æ¶¨è·Œé¢']) else 0,
                'change_percent': float(row['æ¶¨è·Œå¹…']) if pd.notna(row['æ¶¨è·Œå¹…']) else 0,
                'volume': float(row['æˆäº¤é‡']) if pd.notna(row['æˆäº¤é‡']) else 0,
                'amount': float(row['æˆäº¤é¢']) if pd.notna(row['æˆäº¤é¢']) else 0,
                'high': float(row['æœ€é«˜']) if pd.notna(row['æœ€é«˜']) else 0,
                'low': float(row['æœ€ä½']) if pd.notna(row['æœ€ä½']) else 0,
                'open': float(row['ä»Šå¼€']) if pd.notna(row['ä»Šå¼€']) else 0,
                'pre_close': float(row['æ˜¨æ”¶']) if pd.notna(row['æ˜¨æ”¶']) else 0,
                'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            realtime_data.append(stock_data)
        
        # å­˜å‚¨åˆ°Redis
        redis_cache.set_cache(STOCK_KEYS['realtime_data'], {
            'data': realtime_data,
            'count': len(realtime_data),
            'update_time': datetime.now().isoformat(),
            'data_source': 'akshare',
            'is_closing_data': is_closing_update
        }, ttl=1800)  # 30åˆ†é’Ÿè¿‡æœŸ
        
        # æ–°å¢ï¼šå°†å®æ—¶æ•°æ®åˆå¹¶åˆ°Kçº¿æ•°æ®çš„æœ€åä¸€æ ¹Kçº¿
        logger.info("å¼€å§‹å°†å®æ—¶æ•°æ®åˆå¹¶åˆ°Kçº¿æ•°æ®...")
        updated_kline_count = _merge_realtime_to_kline_data(realtime_data, is_closing_update=is_closing_update)
        logger.info(f" å·²æ›´æ–° {updated_kline_count} åªè‚¡ç¥¨çš„Kçº¿æ•°æ®")
        
        execution_time = (datetime.now() - start_time).total_seconds()
        logger.info(f" å®æ—¶æ•°æ®æ›´æ–°å®Œæˆ: {len(realtime_data)}åªè‚¡ç¥¨ï¼ŒKçº¿æ›´æ–°: {updated_kline_count}åªï¼Œè€—æ—¶ {execution_time:.2f}ç§’")
        
        add_stock_job_log('update_realtime', 'success', f'å®æ—¶æ•°æ®æ›´æ–°å®Œæˆ: {len(realtime_data)}åªï¼ŒKçº¿æ›´æ–°: {updated_kline_count}åª', len(realtime_data), execution_time)
        
        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦è§¦å‘ä¿¡å·é‡æ–°è®¡ç®—
        if auto_calculate_signals:
            logger.info("å®æ—¶æ•°æ®æ›´æ–°å®Œæˆï¼Œè‡ªåŠ¨è§¦å‘ä¹°å…¥ä¿¡å·é‡æ–°è®¡ç®—...")
            _trigger_signal_recalculation_async()
        else:
            logger.info("å®æ—¶æ•°æ®æ›´æ–°å®Œæˆï¼Œè·³è¿‡ä¿¡å·è®¡ç®—ï¼ˆæœªå¯ç”¨auto_calculate_signalsï¼‰")
        
    except Exception as e:
        execution_time = (datetime.now() - start_time).total_seconds()
        error_msg = f'å®æ—¶æ•°æ®æ›´æ–°å¤±è´¥: {str(e)}'
        logger.error(f" {error_msg}")
        add_stock_job_log('update_realtime', 'failed', error_msg, 0, execution_time)

def _merge_realtime_to_kline_data(realtime_data: List[Dict], is_closing_update=False) -> int:
    """å°†å®æ—¶æ•°æ®åˆå¹¶åˆ°Kçº¿æ•°æ®çš„æœ€åä¸€æ ¹Kçº¿
    
    ä¿®å¤BUG: ç¡®ä¿å­—æ®µæ ¼å¼ç»Ÿä¸€ï¼Œé¿å…å†å²æ•°æ®å’Œå®æ—¶æ•°æ®å­—æ®µå†²çª
    
    Args:
        realtime_data: å®æ—¶æ•°æ®åˆ—è¡¨
        is_closing_update: æ˜¯å¦ä¸ºæ”¶ç›˜åæ›´æ–°ï¼Œæ”¶ç›˜åæ›´æ–°ä¼šå¼ºåˆ¶æ›´æ–°ä»·æ ¼
    
    Returns:
        æ›´æ–°çš„è‚¡ç¥¨æ•°é‡
    """
    updated_count = 0
    today_str = datetime.now().strftime('%Y-%m-%d')
    today_trade_date = datetime.now().strftime('%Y%m%d')
    
    try:
        for stock_data in realtime_data:
            try:
                stock_code = stock_data['code']
                
                # æ„é€ ts_code
                if stock_code.startswith('6'):
                    ts_code = f"{stock_code}.SH"
                elif stock_code.startswith(('43', '83', '87', '88')):
                    ts_code = f"{stock_code}.BJ"
                else:
                    ts_code = f"{stock_code}.SZ"
                
                # è·å–Kçº¿æ•°æ®
                kline_key = STOCK_KEYS['stock_kline'].format(ts_code)
                kline_data = redis_cache.get_cache(kline_key)
                
                if not kline_data:
                    continue
                
                # è§£æKçº¿æ•°æ®ï¼Œå¤„ç†ä¸åŒçš„å­˜å‚¨æ ¼å¼
                if isinstance(kline_data, dict):
                    trend_data = kline_data
                elif isinstance(kline_data, str):
                    trend_data = json.loads(kline_data)
                else:
                    trend_data = kline_data
                
                # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                if isinstance(trend_data, dict):
                    # æ–°æ ¼å¼ï¼š{data: [...], updated_at: ..., source: ...}
                    kline_list = trend_data.get('data', [])
                elif isinstance(trend_data, list):
                    # æ—§æ ¼å¼ï¼šç›´æ¥æ˜¯Kçº¿æ•°æ®åˆ—è¡¨
                    kline_list = trend_data
                    # ä¸ºäº†åç»­æ›´æ–°æ“ä½œï¼Œéœ€è¦åŒ…è£…æˆå­—å…¸æ ¼å¼
                    trend_data = {
                        'data': kline_list,
                        'updated_at': datetime.now().isoformat(),
                        'data_count': len(kline_list),
                        'source': 'legacy_format'
                    }
                else:
                    continue
                
                if not kline_list:
                    continue
                
                # å…³é”®ä¿®å¤ï¼šç»Ÿä¸€å­—æ®µæ ¼å¼ï¼Œé¿å…å­—æ®µå†²çª
                logger.debug(f"å¼€å§‹å¤„ç† {ts_code} çš„å­—æ®µæ ¼å¼ç»Ÿä¸€...")
                
                # ç»Ÿä¸€å†å²æ•°æ®çš„å­—æ®µæ ¼å¼
                for i, kline in enumerate(kline_list):
                    # ç¡®ä¿æ‰€æœ‰å†å²æ•°æ®éƒ½æœ‰ç»Ÿä¸€çš„å­—æ®µæ ¼å¼ï¼ˆtushareæ ¼å¼ï¼‰
                    if 'ts_code' not in kline:
                        kline['ts_code'] = ts_code
                    
                    # ç¡®ä¿æœ‰trade_dateå­—æ®µ
                    if 'trade_date' not in kline and 'date' in kline:
                        # å¦‚æœåªæœ‰dateå­—æ®µï¼Œè½¬æ¢ä¸ºtrade_date
                        date_val = kline['date']
                        if isinstance(date_val, str) and len(date_val) == 10:  # YYYY-MM-DDæ ¼å¼
                            kline['trade_date'] = date_val.replace('-', '')
                        else:
                            kline['trade_date'] = str(date_val).replace('-', '')
                    
                    # ç¡®ä¿æœ‰actual_trade_dateå­—æ®µ
                    if 'actual_trade_date' not in kline:
                        trade_date = kline.get('trade_date', '')
                        if len(str(trade_date)) == 8:
                            kline['actual_trade_date'] = f"{trade_date[:4]}-{trade_date[4:6]}-{trade_date[6:8]}"
                        else:
                            kline['actual_trade_date'] = today_str
                    
                    # å…³é”®ä¿®å¤ï¼šç§»é™¤å®æ—¶æ›´æ–°å­—æ®µï¼Œä¿æŒå†å²æ•°æ®æ ¼å¼çº¯å‡€
                    # ç§»é™¤å®æ—¶æ›´æ–°ç›¸å…³çš„é¢å¤–å­—æ®µï¼Œä¿æŒtushareæ ¼å¼çš„çº¯å‡€æ€§
                    historical_fields = ['ts_code', 'trade_date', 'open', 'high', 'low', 'close', 
                                       'pre_close', 'change', 'pct_chg', 'vol', 'amount', 'actual_trade_date']
                    
                    # å¦‚æœæ˜¯å‰120æ¡å†å²æ•°æ®ï¼Œç¡®ä¿åªä¿ç•™å†å²å­—æ®µ
                    if i < len(kline_list) - 1:  # éæœ€åä¸€æ¡æ•°æ®
                        # ä¿ç•™å½“å‰æ‰€æœ‰å­—æ®µä½†ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
                        for field in historical_fields:
                            if field not in kline:
                                if field == 'vol' and 'volume' in kline:
                                    kline['vol'] = kline['volume']
                                elif field in ['change', 'pre_close', 'pct_chg'] and field not in kline:
                                    kline[field] = 0.0  # é»˜è®¤å€¼
                        
                        # ç§»é™¤å¯èƒ½å¹²æ‰°çš„å­—æ®µï¼ˆåªåœ¨å†å²æ•°æ®ä¸­ç§»é™¤ï¼‰
                        fields_to_remove = ['date', 'volume', 'turnover_rate', 'is_realtime_updated', 
                                          'update_time', 'realtime_source', 'realtime_volume_source']
                        for field in fields_to_remove:
                            if field in kline:
                                del kline[field]
                
                # æ£€æŸ¥æœ€åä¸€æ ¹Kçº¿æ˜¯å¦æ˜¯ä»Šå¤©çš„æ•°æ®
                last_kline = kline_list[-1]
                last_trade_date = str(last_kline.get('trade_date', ''))
                last_date = last_kline.get('actual_trade_date', last_kline.get('date', ''))
                
                # å®æ—¶æ•°æ®ä¸­çš„æˆäº¤é‡æ•°æ®å¤„ç†
                current_volume = stock_data.get('volume', 0)
                if current_volume == 0:
                    # å¦‚æœå®æ—¶æˆäº¤é‡ä¸º0ï¼Œå°è¯•ä»å…¶ä»–å­—æ®µè·å–
                    current_volume = stock_data.get('vol', 0)
                
                # è°ƒè¯•æ—¥å¿—ï¼šè®°å½•æˆäº¤é‡æ•°æ®
                if current_volume > 0:
                    logger.debug(f"{ts_code} å®æ—¶æˆäº¤é‡: {current_volume} (åŸå§‹å€¼), è½¬æ¢å: {current_volume / 100} æ‰‹")
                
                # ç¡®ä¿æˆäº¤é‡æ•°æ®æœ‰æ•ˆ
                if current_volume is None or current_volume < 0:
                    current_volume = 0
                
                # å…³é”®ä¿®å¤ï¼šä»Šæ—¥æ•°æ®å¤„ç†ç­–ç•¥
                if last_trade_date != today_trade_date and last_date != today_str:
                    # å¦‚æœæœ€åä¸€æ ¹Kçº¿ä¸æ˜¯ä»Šå¤©çš„ï¼Œè¿½åŠ ä»Šå¤©çš„æ–°Kçº¿
                    # ä½¿ç”¨ç»Ÿä¸€çš„tushareæ ¼å¼
                    new_kline = {
                        'ts_code': ts_code,
                        'trade_date': today_trade_date,
                        'open': stock_data['open'],
                        'high': stock_data['high'],
                        'low': stock_data['low'],
                        'close': stock_data['price'],  # å½“å‰ä»·æ ¼ä½œä¸ºæ”¶ç›˜ä»·
                        'pre_close': stock_data['pre_close'],
                        'change': stock_data['change'],
                        'pct_chg': stock_data['change_percent'],
                        'vol': current_volume / 100 if current_volume > 100 else current_volume,  # ç»Ÿä¸€ä¸ºæ‰‹å•ä½
                        'amount': stock_data['amount'] / 1000 if stock_data['amount'] > 1000 else stock_data['amount'],  # ç»Ÿä¸€ä¸ºåƒå…ƒå•ä½
                        'actual_trade_date': today_str,
                        'is_closing_data': is_closing_update,  # æ ‡è®°æ˜¯å¦ä¸ºæ”¶ç›˜æ•°æ®
                        'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    kline_list.append(new_kline)
                    logger.debug(f"ä¸º {ts_code} è¿½åŠ ä»Šæ—¥Kçº¿: {today_str}, å†å²æ•°æ®: {len(kline_list)-1}æ¡, ä½¿ç”¨tushareæ ¼å¼")
                else:
                    # å…³é”®ä¿®å¤ï¼šæ›´æ–°æœ€åä¸€æ ¹Kçº¿ï¼Œä½†ä¿æŒtushareæ ¼å¼
                    # ä¿ç•™åŸæœ‰çš„æˆäº¤é‡ï¼Œå¦‚æœå®æ—¶æˆäº¤é‡æ›´å¤§åˆ™æ›´æ–°
                    existing_volume = float(last_kline.get('vol', 0))
                    
                    # æˆäº¤é‡é‡‡ç”¨ç´¯ç§¯ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨æ›´å¤§çš„å€¼ï¼Œä¿è¯æ•°æ®çš„å‡†ç¡®æ€§
                    current_volume_in_hands = current_volume / 100  # è½¬æ¢ä¸ºæ‰‹æ•°
                    
                    # å¦‚æœå®æ—¶æˆäº¤é‡å¤§äº0ï¼Œä½¿ç”¨å®æ—¶æ•°æ®ï¼›å¦åˆ™ä¿ç•™å†å²æ•°æ®
                    if current_volume > 0:
                        final_volume = max(existing_volume, current_volume_in_hands)
                    else:
                        final_volume = existing_volume  # ä¿æŒåŸæœ‰æˆäº¤é‡
                    
                    # è®°å½•æˆäº¤é‡æ›´æ–°æƒ…å†µ
                    if current_volume > 0 and final_volume != existing_volume:
                        logger.debug(f"{ts_code} æˆäº¤é‡æ›´æ–°: {existing_volume} -> {final_volume} æ‰‹")
                    
                    # å¦‚æœæ˜¯æ”¶ç›˜åæ›´æ–°ï¼Œå¼ºåˆ¶æ›´æ–°ä»·æ ¼å’Œå…¶ä»–æ•°æ®
                    if is_closing_update:
                        logger.debug(f" æ”¶ç›˜åæ›´æ–° {ts_code} çš„ä»·æ ¼æ•°æ®: {stock_data['price']}")
                        # æ›´æ–°æœ€åä¸€æ ¹Kçº¿ï¼Œä½†ä¸¥æ ¼ä¿æŒtushareå­—æ®µæ ¼å¼
                        last_kline.update({
                            'ts_code': ts_code,  # ç¡®ä¿æœ‰ts_code
                            'trade_date': today_trade_date,  # ç¡®ä¿trade_dateæ ¼å¼æ­£ç¡®
                            'high': max(float(last_kline.get('high', 0)), stock_data['high']),
                            'low': min(float(last_kline.get('low', float('inf'))), stock_data['low']) if float(last_kline.get('low', float('inf'))) != float('inf') else stock_data['low'],
                            'close': stock_data['price'],  # å½“å‰ä»·æ ¼ä½œä¸ºæ”¶ç›˜ä»·
                            'pre_close': stock_data['pre_close'],
                            'change': stock_data['change'],
                            'pct_chg': stock_data['change_percent'],
                            'vol': final_volume,  # ä½¿ç”¨ç»Ÿä¸€çš„æ‰‹å•ä½
                            'amount': stock_data['amount'] / 1000,  # ä½¿ç”¨ç»Ÿä¸€çš„åƒå…ƒå•ä½
                            'actual_trade_date': today_str,
                            'is_closing_data': True,  # æ ‡è®°ä¸ºæ”¶ç›˜æ•°æ®
                            'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        })
                    else:
                        # äº¤æ˜“æ—¶é—´å†…çš„æ›´æ–°ï¼Œåªæœ‰å½“å‰ä»·æ ¼é«˜äºæœ€é«˜ä»·æˆ–ä½äºæœ€ä½ä»·æ—¶æ‰æ›´æ–°
                        current_high = max(float(last_kline.get('high', 0)), stock_data['high'])
                        current_low = min(float(last_kline.get('low', float('inf'))), stock_data['low']) if float(last_kline.get('low', float('inf'))) != float('inf') else stock_data['low']
                        
                        # æ›´æ–°æœ€åä¸€æ ¹Kçº¿ï¼Œä½†ä¸¥æ ¼ä¿æŒtushareå­—æ®µæ ¼å¼
                        last_kline.update({
                            'ts_code': ts_code,  # ç¡®ä¿æœ‰ts_code
                            'trade_date': today_trade_date,  # ç¡®ä¿trade_dateæ ¼å¼æ­£ç¡®
                            'high': current_high,
                            'low': current_low,
                            'close': stock_data['price'],  # å½“å‰ä»·æ ¼ä½œä¸ºæ”¶ç›˜ä»·
                            'pre_close': stock_data['pre_close'],
                            'change': stock_data['change'],
                            'pct_chg': stock_data['change_percent'],
                            'vol': final_volume,  # ä½¿ç”¨ç»Ÿä¸€çš„æ‰‹å•ä½
                            'amount': stock_data['amount'] / 1000,  # ä½¿ç”¨ç»Ÿä¸€çš„åƒå…ƒå•ä½
                            'actual_trade_date': today_str,
                            'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        })
                    
                    # å…³é”®ä¿®å¤ï¼šç§»é™¤å®æ—¶æ›´æ–°å­—æ®µï¼Œä¿æŒæ ¼å¼ç»Ÿä¸€
                    # ç§»é™¤æ‰€æœ‰å®æ—¶æ›´æ–°ç›¸å…³å­—æ®µï¼Œç¡®ä¿æ ¼å¼ç»Ÿä¸€
                    realtime_fields_to_remove = ['date', 'volume', 'turnover_rate', 'is_realtime_updated', 
                                                'realtime_source', 'realtime_volume_source']
                    for field in realtime_fields_to_remove:
                        if field in last_kline:
                            del last_kline[field]
                    
                    if is_closing_update:
                        logger.debug(f"æ”¶ç›˜åæ›´æ–° {ts_code} æœ€åä¸€æ ¹Kçº¿: æ”¶ç›˜ä»· {stock_data['price']}, æˆäº¤é‡: {final_volume}æ‰‹")
                    else:
                        logger.debug(f"æ›´æ–° {ts_code} æœ€åä¸€æ ¹Kçº¿: æ”¶ç›˜ä»· {stock_data['price']}, æˆäº¤é‡: {final_volume}æ‰‹, ä¿æŒtushareæ ¼å¼")
                
                # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æœ‰ç»Ÿä¸€çš„å­—æ®µæ ¼å¼
                logger.debug(f"{ts_code} å­—æ®µæ ¼å¼éªŒè¯...")
                for i, kline in enumerate(kline_list):
                    # ç¡®ä¿æ¯æ¡è®°å½•éƒ½æœ‰å¿…è¦çš„tushareå­—æ®µ
                    required_fields = ['ts_code', 'trade_date', 'open', 'high', 'low', 'close', 
                                     'pre_close', 'change', 'pct_chg', 'vol', 'amount', 'actual_trade_date']
                    
                    for field in required_fields:
                        if field not in kline:
                            # å¡«å……é»˜è®¤å€¼
                            if field == 'ts_code':
                                kline[field] = ts_code
                            elif field in ['change', 'pre_close', 'pct_chg']:
                                kline[field] = 0.0
                            elif field in ['vol', 'amount']:
                                kline[field] = 0.0
                            elif field == 'actual_trade_date':
                                trade_date = kline.get('trade_date', today_trade_date)
                                if len(str(trade_date)) == 8:
                                    kline[field] = f"{trade_date[:4]}-{trade_date[4:6]}-{trade_date[6:8]}"
                                else:
                                    kline[field] = today_str
                
                # æ›´æ–°trend_dataçš„å…ƒæ•°æ®
                trend_data.update({
                    'data': kline_list,
                    'updated_at': datetime.now().isoformat(),
                    'data_count': len(kline_list),
                    'last_update_type': 'closing_update' if is_closing_update else 'realtime_update'
                })
                
                # æ›´æ–°Redisç¼“å­˜
                redis_cache.set_cache(kline_key, trend_data, ttl=None)  # æ°¸ä¹…å­˜å‚¨
                
                # åŒæ—¶æ›´æ–°å®æ—¶ä»·æ ¼ç¼“å­˜ï¼ˆç”¨äºä¿¡å·è®¡ç®—ï¼‰
                realtime_price_key = f"stocks:realtime:{ts_code}"
                realtime_price_data = {
                    'price': stock_data['price'],
                    'change': stock_data['change'],
                    'pct_chg': stock_data['change_percent'],
                    'volume': stock_data['volume'],
                    'amount': stock_data['amount'],
                    'high': stock_data['high'],
                    'low': stock_data['low'],
                    'open': stock_data['open'],
                    'pre_close': stock_data['pre_close'],
                    'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'is_closing_data': is_closing_update
                }
                redis_cache.set_cache(realtime_price_key, json.dumps(realtime_price_data), ttl=3600)  # 1å°æ—¶è¿‡æœŸ
                
                updated_count += 1
                
            except Exception as e:
                logger.error(f"å¤„ç†è‚¡ç¥¨ {stock_data.get('code', 'unknown')} çš„å®æ—¶æ•°æ®å¤±è´¥: {str(e)}")
                continue
                
        return updated_count
        
    except Exception as e:
        logger.error(f"åˆå¹¶å®æ—¶æ•°æ®åˆ°Kçº¿æ•°æ®å¤±è´¥: {str(e)}")
        return 0

def _trigger_signal_recalculation_async():
    """å¼‚æ­¥è§¦å‘ä¹°å…¥ä¿¡å·é‡æ–°è®¡ç®—ï¼ˆéé˜»å¡ï¼Œé˜²é‡å¤æ‰§è¡Œï¼‰"""
    global _signal_calculation_running
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¿¡å·è®¡ç®—ä»»åŠ¡åœ¨è¿è¡Œ
    with _signal_calculation_lock:
        if _signal_calculation_running:
            logger.info(" ä¹°å…¥ä¿¡å·è®¡ç®—ä»»åŠ¡å·²åœ¨è¿è¡Œä¸­ï¼Œè·³è¿‡æœ¬æ¬¡è§¦å‘")
            return
        _signal_calculation_running = True
    
    try:
        import concurrent.futures
        
        def _run_signal_calculation():
            """åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œä¿¡å·è®¡ç®—"""
            global _signal_calculation_running
            try:
                async def _calculate():
                    # åœ¨æ–°äº‹ä»¶å¾ªç¯ä¸­åˆ›å»ºæ–°çš„signal_managerå®ä¾‹ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
                    from app.services.signal_manager import SignalManager
                    
                    local_signal_manager = None
                    try:
                        local_signal_manager = SignalManager()
                        await local_signal_manager.initialize()
                        logger.info("å¼€å§‹é‡æ–°è®¡ç®—ä¹°å…¥ä¿¡å·...")
                        
                        result = await local_signal_manager.calculate_buy_signals(force_recalculate=True)
                        
                        if result.get('status') == 'success':
                            total_signals = result.get('total_signals', 0)
                            elapsed = result.get('elapsed_seconds', 0)
                            logger.info(f" ä¹°å…¥ä¿¡å·é‡æ–°è®¡ç®—å®Œæˆ: ç”Ÿæˆ {total_signals} ä¸ªä¿¡å·ï¼Œè€—æ—¶ {elapsed:.1f}ç§’")
                        else:
                            logger.warning(f" ä¹°å…¥ä¿¡å·é‡æ–°è®¡ç®—å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                            
                    except Exception as e:
                        logger.error(f"è®¡ç®—ä¹°å…¥ä¿¡å·å¤±è´¥: {e}")
                        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                    finally:
                        if local_signal_manager:
                            try:
                                await local_signal_manager.close()
                            except Exception as e:
                                logger.error(f"SignalManagerå…³é—­å¤±è´¥: {e}")
                
                # åœ¨æ–°çº¿ç¨‹ä¸­åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    loop.run_until_complete(_calculate())
                finally:
                    loop.close()
                
            except Exception as e:
                logger.error(f" ä¿¡å·è®¡ç®—çº¿ç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            finally:
                # é‡ç½®è¿è¡Œæ ‡å¿—
                with _signal_calculation_lock:
                    _signal_calculation_running = False
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œï¼Œé¿å…é˜»å¡ä¸»æµç¨‹
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            executor.submit(_run_signal_calculation)
            
    except Exception as e:
        logger.error(f" è§¦å‘å¼‚æ­¥ä¿¡å·è®¡ç®—å¤±è´¥: {e}")
        # é‡ç½®è¿è¡Œæ ‡å¿—
        with _signal_calculation_lock:
            _signal_calculation_running = False

# ===================== è°ƒåº¦å™¨ç®¡ç† =====================

def start_stock_scheduler():
    """å¯åŠ¨è‚¡ç¥¨è°ƒåº¦å™¨"""
    global scheduler
    
    if scheduler and scheduler.running:
        logger.warning(" è‚¡ç¥¨è°ƒåº¦å™¨å·²ç»åœ¨è¿è¡Œä¸­")
        return
    
    try:
        # åˆ›å»ºåå°è°ƒåº¦å™¨
        scheduler = BackgroundScheduler(
            timezone='Asia/Shanghai',
            job_defaults={
                'coalesce': True,  # åˆå¹¶æœªæ‰§è¡Œçš„ä»»åŠ¡
                'max_instances': 1,  # åŒæ—¶åªè¿è¡Œä¸€ä¸ªå®ä¾‹
                'misfire_grace_time': 300  # 5åˆ†é’Ÿçš„å®¹é”™æ—¶é—´
            }
        )
        
        # 1. ç³»ç»Ÿå¯åŠ¨å®Œæˆï¼Œç­‰å¾…ç”¨æˆ·é€‰æ‹©åˆå§‹åŒ–æ–¹å¼
        logger.info(" è‚¡ç¥¨è°ƒåº¦å™¨å¯åŠ¨å®Œæˆï¼Œç­‰å¾…ç”¨æˆ·é€‰æ‹©åˆå§‹åŒ–æ–¹å¼...")
        logger.info("è¯·ä½¿ç”¨APIæ‰‹åŠ¨è§¦å‘åˆå§‹åŒ–:")
        logger.info("   â€¢ æ¸…ç©ºé‡æ–°åˆå§‹åŒ–: POST /api/stocks/scheduler/init?clear_data=true")
        logger.info("   â€¢ è·³è¿‡æ¸…ç©ºæ£€æŸ¥ç°æœ‰æ•°æ®: POST /api/stocks/scheduler/init?clear_data=false")
        
        # 2. Kçº¿æ•°æ®å…¨é‡æ›´æ–°ä»»åŠ¡ - æ¯ä¸ªäº¤æ˜“æ—¥17:30æ‰§è¡Œï¼ˆæ”¶ç›˜åè·å–å®Œæ•´æ•°æ®ï¼‰
        # ä½¿ç”¨éé˜»å¡çš„åå°çº¿ç¨‹æ‰§è¡Œï¼Œé¿å…é˜»å¡APIæœåŠ¡
        def non_blocking_kline_refresh():
            """éé˜»å¡çš„Kçº¿æ•°æ®åˆ·æ–°"""
            threading.Thread(target=clear_and_refetch_kline_data, daemon=True).start()
            
        scheduler.add_job(
            func=non_blocking_kline_refresh,
            trigger=CronTrigger(hour=17, minute=30, second=0, day_of_week='mon-fri'),
            id='daily_kline_refresh',
            name='æ¯æ—¥Kçº¿æ•°æ®å…¨é‡åˆ·æ–°ï¼ˆéé˜»å¡ï¼‰',
            replace_existing=True
        )
        
        # 3. å®æ—¶æ•°æ®æ›´æ–°ä»»åŠ¡ - äº¤æ˜“æ—¶é—´å†…æ¯20åˆ†é’Ÿæ‰§è¡Œï¼ˆåŒ…å«æ”¶ç›˜å20åˆ†é’Ÿï¼‰
        # ä½¿ç”¨éé˜»å¡çš„åå°çº¿ç¨‹æ‰§è¡Œï¼Œé¿å…é˜»å¡APIæœåŠ¡
        def non_blocking_realtime_update():
            """éé˜»å¡çš„å®æ—¶æ•°æ®æ›´æ–°"""
            def run_update():
                update_realtime_stock_data(auto_calculate_signals=True)
            threading.Thread(target=run_update, daemon=True).start()
            
        scheduler.add_job(
            func=non_blocking_realtime_update,
            trigger=CronTrigger(minute='0,20,40', second=0, hour='9-11,13-15', day_of_week='mon-fri'),
            id='realtime_data_update',
            name='å®æ—¶æ•°æ®æ›´æ–°+ä¿¡å·è®¡ç®—ï¼ˆéé˜»å¡ï¼‰',
            replace_existing=True
        )
        
        # å·²åˆ é™¤15:05æ”¶ç›˜æ•°æ®æ›´æ–°ä»»åŠ¡ï¼Œå› ä¸ºå®æ—¶æ›´æ–°å·²å»¶é•¿åˆ°15:20ï¼Œè¦†ç›–äº†æ”¶ç›˜æ—¶é—´
        
        # åˆ é™¤åŸæœ‰çš„17:35æœ€ç»ˆä¿¡å·è®¡ç®—ä»»åŠ¡ï¼Œå› ä¸ºå®æ—¶æ›´æ–°å·²å»¶é•¿åˆ°15:20
        # åœ¨Kçº¿å…¨é‡æ›´æ–°åä¼šè‡ªåŠ¨è§¦å‘ä¿¡å·è®¡ç®—
        
        # å¯åŠ¨è°ƒåº¦å™¨
        scheduler.start()
        
        logger.info("=" * 70)
        logger.info(" è‚¡ç¥¨è°ƒåº¦å™¨å¯åŠ¨æˆåŠŸ")
        logger.info("=" * 70)
        logger.info("å®šæ—¶ä»»åŠ¡é…ç½®:")
        logger.info("  â€¢ Kçº¿æ•°æ®åˆ·æ–°: æ¯ä¸ªäº¤æ˜“æ—¥17:30 (è‡ªåŠ¨è§¦å‘ä¿¡å·è®¡ç®—)")
        logger.info("  â€¢ å®æ—¶æ•°æ®æ›´æ–°: äº¤æ˜“æ—¶é—´å†…æ¯20åˆ†é’Ÿ (9:00-11:30, 13:00-15:20)")
        logger.info("  â€¢ å·²åˆ é™¤: 15:05æ”¶ç›˜æ•°æ®æ›´æ–°ä»»åŠ¡ï¼ˆå®æ—¶æ›´æ–°å·²è¦†ç›–ï¼‰")
        logger.info("  â€¢ é‡è¦æ”¹è¿›: å®æ—¶æ›´æ–°å»¶é•¿åˆ°15:20ï¼Œç¡®ä¿æ”¶ç›˜ä»·æ ¼è¢«æ•è·")
        logger.info("  â€¢ å·²åˆ é™¤: 17:35æœ€ç»ˆä¿¡å·è®¡ç®—ä»»åŠ¡")
        logger.info("")
        logger.info("å·²æ³¨å†Œçš„å®šæ—¶ä»»åŠ¡:")
        jobs = scheduler.get_jobs()
        for job in jobs:
            next_run = job.next_run_time.strftime('%Y-%m-%d %H:%M:%S') if job.next_run_time else "æœªå®‰æ’"
            logger.info(f"  â€¢ {job.name} (ID: {job.id})")
            logger.info(f"    - ä¸‹æ¬¡æ‰§è¡Œ: {next_run}")
            logger.info(f"    - è§¦å‘å™¨: {job.trigger}")
        logger.info("")
        logger.info("å¯åŠ¨å®Œæˆ: ç­‰å¾…ç”¨æˆ·é€‰æ‹©åˆå§‹åŒ–æ–¹å¼")
        logger.info("=" * 70)
        
    except Exception as e:
        logger.error(f" å¯åŠ¨è‚¡ç¥¨è°ƒåº¦å™¨å¤±è´¥: {str(e)}")

def stop_stock_scheduler():
    """åœæ­¢è‚¡ç¥¨è°ƒåº¦å™¨"""
    global scheduler
    
    if scheduler and scheduler.running:
        scheduler.shutdown(wait=False)
        logger.info("è‚¡ç¥¨è°ƒåº¦å™¨å·²åœæ­¢")
    else:
        logger.info("è‚¡ç¥¨è°ƒåº¦å™¨æœªè¿è¡Œ")

def get_stock_scheduler_status() -> Dict[str, Any]:
    """è·å–è‚¡ç¥¨è°ƒåº¦å™¨çŠ¶æ€"""
    global scheduler, job_logs
    
    try:
        if not scheduler:
            return {
                'running': False,
                'error': 'è°ƒåº¦å™¨æœªåˆå§‹åŒ–'
            }
        
        # è·å–ä»»åŠ¡ä¿¡æ¯
        jobs_info = []
        if scheduler.running:
            for job in scheduler.get_jobs():
                next_run = job.next_run_time
                jobs_info.append({
                    'id': job.id,
                    'name': job.name,
                    'next_run': next_run.strftime('%Y-%m-%d %H:%M:%S') if next_run else None,
                    'trigger': str(job.trigger)
                })
        
        # è·å–æ•°æ®çŠ¶æ€
        stock_codes = redis_cache.get_cache(STOCK_KEYS['stock_codes'])
        realtime_data = redis_cache.get_cache(STOCK_KEYS['realtime_data'])
        signals_data = redis_cache.get_cache(STOCK_KEYS['strategy_signals'])
        
        data_status = {
            'stock_codes': {
                'exists': stock_codes is not None,
                'count': len(stock_codes) if stock_codes else 0
            },
            'realtime_data': {
                'exists': realtime_data is not None,
                'count': realtime_data.get('count', 0) if realtime_data else 0,
                'last_update': realtime_data.get('update_time') if realtime_data else None
            },
            'strategy_signals': {
                'exists': signals_data is not None,
                'count': len(signals_data) if isinstance(signals_data, dict) else 0
            }
        }
        
        return {
            'running': scheduler.running if scheduler else False,
            'jobs': jobs_info,
            'recent_logs': job_logs[:5],  # æœ€è¿‘5æ¬¡æ—¥å¿—
            'data_status': data_status,
            'trading_status': {
                'is_trading_day': is_trading_day(),
                'is_trading_time': is_trading_time(),
                'current_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            },
            'scheduler_type': 'APScheduler V3 ä¼˜åŒ–ç‰ˆ',
            'description': 'ä¼˜åŒ–è°ƒåº¦å™¨ï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œ20åˆ†é’Ÿæ›´æ–°å‘¨æœŸï¼Œæ¯æ—¥17:30å…¨é‡åˆ·æ–°'
        }
        
    except Exception as e:
        return {
            'running': False,
            'error': str(e)
        }

def trigger_stock_task(task_type: str, mode: str = "only_tasks", is_closing_update: bool = False) -> Dict[str, Any]:
    """æ‰‹åŠ¨è§¦å‘è‚¡ç¥¨ä»»åŠ¡"""
    try:
        if task_type == 'init_system':
            threading.Thread(target=init_stock_system, args=(mode,), daemon=True).start()
            action_desc = {
                "clear_all": "æ¸…ç©ºæ‰€æœ‰æ•°æ®é‡æ–°è·å– - åˆ é™¤æ‰€æœ‰å†å²æ•°æ®ï¼Œé‡æ–°è·å–",
                "only_tasks": "åªæ‰§è¡Œè®¡åˆ’ä»»åŠ¡ - ä¸è·å–Kçº¿æ•°æ®ï¼Œåªæ‰§è¡Œä¿¡å·è®¡ç®—ã€æ–°é—»è·å–ç­‰ä»»åŠ¡",
                "none": "ä¸æ‰§è¡Œä»»ä½•åˆå§‹åŒ– - å¯åŠ¨æ—¶ä»€ä¹ˆéƒ½ä¸æ‰§è¡Œ"
            }.get(mode, f"æœªçŸ¥æ¨¡å¼: {mode}")
            
            return {
                'success': True,
                'message': f'è‚¡ç¥¨ç³»ç»Ÿåˆå§‹åŒ–ä»»åŠ¡å·²è§¦å‘: {action_desc}',
                'task_type': task_type,
                'mode': mode
            }
        elif task_type == 'clear_refetch':
            threading.Thread(target=clear_and_refetch_kline_data, daemon=True).start()
            return {
                'success': True,
                'message': 'Kçº¿æ•°æ®å…¨é‡åˆ·æ–°ä»»åŠ¡å·²è§¦å‘',
                'task_type': task_type
            }
        elif task_type == 'calc_signals':
            threading.Thread(target=calculate_strategy_signals, daemon=True).start()
            return {
                'success': True,
                'message': 'ç­–ç•¥ä¿¡å·è®¡ç®—ä»»åŠ¡å·²è§¦å‘',
                'task_type': task_type
            }
        elif task_type == 'update_realtime':
            # ä½¿ç”¨å‚æ•°æ§åˆ¶æ˜¯å¦ä¸ºæ”¶ç›˜æ›´æ–°
            threading.Thread(
                target=lambda: update_realtime_stock_data(force_update=True, is_closing_update=is_closing_update), 
                daemon=True
            ).start()
            
            update_type = "æ”¶ç›˜ä»·æ ¼" if is_closing_update else "å®æ—¶ä»·æ ¼"
            return {
                'success': True,
                'message': f'{update_type}æ›´æ–°ä»»åŠ¡å·²è§¦å‘',
                'task_type': task_type,
                'is_closing_update': is_closing_update
            }
        else:
            return {
                'success': False,
                'message': f'æœªçŸ¥ä»»åŠ¡ç±»å‹: {task_type}',
                'task_type': task_type
            }
            
    except Exception as e:
        logger.error(f'è‚¡ç¥¨ä»»åŠ¡è§¦å‘å¤±è´¥: {str(e)}')
        return {'success': False, 'message': f'è‚¡ç¥¨ä»»åŠ¡è§¦å‘å¤±è´¥: {str(e)}', 'data': None}

def refresh_stock_list() -> Dict[str, Any]:
    """åˆ·æ–°è‚¡ç¥¨åˆ—è¡¨ï¼ˆä½¿ç”¨å®æ—¶APIè·å–å®Œæ•´åˆ—è¡¨ï¼‰"""
    start_time = datetime.now()
    
    try:
        logger.info("ğŸ“¡ å¼€å§‹åˆ·æ–°è‚¡ç¥¨åˆ—è¡¨ï¼ˆå®æ—¶APIï¼‰...")
        
        # ä½¿ç”¨å®æ—¶APIè·å–æœ€æ–°è‚¡ç¥¨åˆ—è¡¨
        df = ak.stock_zh_a_spot_em()
        
        if df.empty:
            raise Exception("è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥")
        
        # è½¬æ¢æ•°æ®æ ¼å¼
        stock_codes = []
        for _, row in df.iterrows():
            code = row['ä»£ç ']
            # åˆ¤æ–­å¸‚åœº
            if code.startswith('6'):
                market = 'SH'
                ts_code = f"{code}.SH"
            elif code.startswith(('43', '83', '87', '88')):
                market = 'BJ'
                ts_code = f"{code}.BJ"
            else:
                market = 'SZ'
                ts_code = f"{code}.SZ"
            
            stock_data = {
                'code': code,
                'name': row['åç§°'],
                'ts_code': ts_code,
                'market': market
            }
            stock_codes.append(stock_data)
        
        # å­˜å‚¨åˆ°Redis
        redis_cache.set_cache(STOCK_KEYS['stock_codes'], stock_codes, ttl=86400)
        
        execution_time = (datetime.now() - start_time).total_seconds()
        logger.info(f" è‚¡ç¥¨åˆ—è¡¨åˆ·æ–°æˆåŠŸ: {len(stock_codes)}åªè‚¡ç¥¨ï¼Œè€—æ—¶ {execution_time:.2f}ç§’")
        
        add_stock_job_log('refresh_stocks', 'success', f'è‚¡ç¥¨åˆ—è¡¨åˆ·æ–°æˆåŠŸ: {len(stock_codes)}åª', len(stock_codes), execution_time)
        
        return {
            'success': True,
            'message': f'è‚¡ç¥¨åˆ—è¡¨åˆ·æ–°æˆåŠŸ: {len(stock_codes)}åªè‚¡ç¥¨',
            'data': {
                'count': len(stock_codes),
                'execution_time': execution_time,
                'updated_at': datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        execution_time = (datetime.now() - start_time).total_seconds()
        error_msg = f'è‚¡ç¥¨åˆ—è¡¨åˆ·æ–°å¤±è´¥: {str(e)}'
        logger.error(f" {error_msg}")
        
        add_stock_job_log('refresh_stocks', 'failed', error_msg, 0, execution_time)
        
        return {
            'success': False,
            'message': error_msg,
            'data': None
        } 