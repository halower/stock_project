# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ•°æ®ç®¡ç†æœåŠ¡
å®ç°è‚¡ç¥¨æ¸…å•å’Œè‚¡ç¥¨èµ°åŠ¿æ•°æ®çš„ç®¡ç†å’Œæ£€æŸ¥åŠŸèƒ½

ä¸»è¦åŠŸèƒ½:
1. è‚¡ç¥¨æ¸…å•ç®¡ç†: åˆå§‹åŒ–å’Œæ›´æ–°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
2. è‚¡ç¥¨èµ°åŠ¿æ•°æ®ç®¡ç†: è·å–ã€å­˜å‚¨å’Œæ›´æ–°è‚¡ç¥¨å†å²äº¤æ˜“æ•°æ®
3. APIé¢‘ç‡é™åˆ¶æ§åˆ¶: é€šè¿‡TushareRateLimiterç±»ç¡®ä¿APIè°ƒç”¨ä¸è¶…è¿‡é™åˆ¶
4. æ•°æ®è¡¥å¿æœºåˆ¶: å¯¹è·å–å¤±è´¥çš„è‚¡ç¥¨æ•°æ®è¿›è¡Œè¡¥å¿å¤„ç†
5. ç³»ç»Ÿå¯åŠ¨æ£€æŸ¥: åœ¨ç³»ç»Ÿå¯åŠ¨æ—¶æ£€æŸ¥æ•°æ®å®Œæ•´æ€§

æ¶æ„è¯´æ˜:
çº¯å¼‚æ­¥IOæ¨¡å¼ - ä½¿ç”¨asyncioå®ç°é«˜æ•ˆå¹¶å‘ï¼Œæ— éœ€å¤šçº¿ç¨‹/å¤šè¿›ç¨‹
"""
import asyncio
import logging
from datetime import datetime, time, timedelta
from typing import Dict, List, Optional, Tuple
import json
import redis.asyncio as redis
import tushare as ts
import pandas as pd
from app.core.config import settings
import time as time_module
from collections import defaultdict
import threading

logger = logging.getLogger(__name__)

class TushareRateLimiter:
    """Tushare APIé¢‘ç‡é™åˆ¶å™¨ - çº¯å¼‚æ­¥IOæ¨¡å¼"""
    
    def __init__(self, max_calls_per_minute=240):
        self.call_times = []  # è®°å½•è°ƒç”¨æ—¶é—´
        self.max_calls_per_minute = max_calls_per_minute  # Tushareé™åˆ¶ï¼ˆå®é™…250æ¬¡/åˆ†é’Ÿï¼Œè®¾ç½®240ç•™ä½™é‡ï¼‰
        self.daily_limit_reached = False
        self.daily_limit_check_time = None
        self.lock = threading.Lock()
        self.async_lock = None  # å¼‚æ­¥é”ï¼Œç”¨äºå¹¶å‘æ§åˆ¶
        
    def _record_call(self):
        """è®°å½•APIè°ƒç”¨"""
        with self.lock:
            current_time = time_module.time()
            self.call_times.append(current_time)
            # åªä¿ç•™æœ€è¿‘1åˆ†é’Ÿçš„è®°å½•
            cutoff_time = current_time - 60
            self.call_times = [t for t in self.call_times if t > cutoff_time]
    
    def _check_rate_limit(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è§¦å‘é¢‘ç‡é™åˆ¶"""
        with self.lock:
            current_time = time_module.time()
            # æ¸…ç†è¿‡æœŸè®°å½•
            cutoff_time = current_time - 60
            self.call_times = [t for t in self.call_times if t > cutoff_time]
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            return len(self.call_times) >= self.max_calls_per_minute
    
    async def wait_for_rate_limit(self):
        """ç­‰å¾…é¢‘ç‡é™åˆ¶è§£é™¤ - çº¯å¼‚æ­¥æ¨¡å¼ï¼Œæ”¯æŒå¹¶å‘å®‰å…¨"""
        # åˆå§‹åŒ–å¼‚æ­¥é”ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œç¡®ä¿åœ¨æ­£ç¡®çš„äº‹ä»¶å¾ªç¯ä¸­ï¼‰
        if self.async_lock is None:
            self.async_lock = asyncio.Lock()
        
        # ä½¿ç”¨å¼‚æ­¥é”ä¿æŠ¤ï¼Œé¿å…å¹¶å‘ç«æ€æ¡ä»¶
        async with self.async_lock:
            # åœ¨é”å†…å†æ¬¡æ£€æŸ¥ï¼Œç¡®ä¿å¹¶å‘å®‰å…¨
            if self._check_rate_limit():
                # è®¡ç®—ç­‰å¾…æ—¶é—´ï¼šç­‰å¾…æœ€æ—©çš„APIè°ƒç”¨è¿‡æœŸï¼ˆ60ç§’åï¼‰
                with self.lock:
                    if self.call_times:
                        oldest_call = min(self.call_times)
                        elapsed = time_module.time() - oldest_call
                        wait_seconds = max(5.0, 60 - elapsed + 1.0)  # æœ€å°‘ç­‰5ç§’ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿæ¢å¤æ—¶é—´
                    else:
                        wait_seconds = 5.0
                
                logger.warning(f"è§¦å‘Tushareé¢‘ç‡é™åˆ¶ï¼ˆ{len(self.call_times)}/{self.max_calls_per_minute}ï¼‰ï¼Œç­‰å¾… {wait_seconds:.1f} ç§’...")
                
                await asyncio.sleep(wait_seconds)
                
                # ç­‰å¾…åç«‹å³æ¸…ç†è¿‡æœŸè®°å½•
                with self.lock:
                    current_time = time_module.time()
                    cutoff_time = current_time - 60
                    old_count = len(self.call_times)
                    self.call_times = [t for t in self.call_times if t > cutoff_time]
                    new_count = len(self.call_times)
                    if old_count > new_count:
                        logger.info(f"æ¸…ç†è¿‡æœŸAPIè®°å½•: {old_count} â†’ {new_count}")
                
                logger.info("é¢‘ç‡é™åˆ¶è§£é™¤ï¼Œç»§ç»­æ•°æ®è·å–...")
    
    def handle_daily_limit_error(self, ts_code: str, days: int):
        """å¤„ç†æ¯æ—¥é™åˆ¶é”™è¯¯"""
        self.daily_limit_reached = True
        self.daily_limit_check_time = datetime.now()
        logger.error("Tushareæ¯æ—¥è°ƒç”¨é‡å·²è¾¾ä¸Šé™ï¼")
        logger.info("æ¯æ—¥é™åˆ¶å°†åœ¨æ˜å¤©0ç‚¹é‡ç½®")
    
    def check_daily_limit_reset(self):
        """æ£€æŸ¥æ¯æ—¥é™åˆ¶æ˜¯å¦é‡ç½®"""
        if (self.daily_limit_reached and self.daily_limit_check_time and 
            datetime.now().date() > self.daily_limit_check_time.date()):
            self.daily_limit_reached = False
            self.daily_limit_check_time = None
            logger.info("Tushareæ¯æ—¥é™åˆ¶å·²é‡ç½®")

class StockDataManager:
    """
    è‚¡ç¥¨æ•°æ®ç®¡ç†å™¨
    
    ä¸»è¦åŠŸèƒ½:
    1. è‚¡ç¥¨æ¸…å•ç®¡ç†: åˆå§‹åŒ–å’Œæ›´æ–°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
    2. è‚¡ç¥¨èµ°åŠ¿æ•°æ®ç®¡ç†: è·å–ã€å­˜å‚¨å’Œæ›´æ–°è‚¡ç¥¨å†å²äº¤æ˜“æ•°æ®
    3. APIé¢‘ç‡é™åˆ¶æ§åˆ¶: é€šè¿‡TushareRateLimiterç±»ç¡®ä¿APIè°ƒç”¨ä¸è¶…è¿‡é™åˆ¶
    4. æ•°æ®è¡¥å¿æœºåˆ¶: å¯¹è·å–å¤±è´¥çš„è‚¡ç¥¨æ•°æ®è¿›è¡Œè¡¥å¿å¤„ç†
    5. ç³»ç»Ÿå¯åŠ¨æ£€æŸ¥: åœ¨ç³»ç»Ÿå¯åŠ¨æ—¶æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    
    å‚æ•°:
        batch_size: å¸¸è§„æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤30
        small_batch_size: å°æ‰¹é‡å¤„ç†å¤§å°ï¼Œé»˜è®¤15
        max_calls_per_minute: æ¯åˆ†é’Ÿæœ€å¤§APIè°ƒç”¨æ¬¡æ•°ï¼Œé»˜è®¤240ï¼ˆå®é™…250æ¬¡/åˆ†é’Ÿï¼Œç•™10æ¬¡ä½™é‡ï¼‰
    
    æ¶æ„: çº¯å¼‚æ­¥IOæ¨¡å¼ï¼Œä¸ä½¿ç”¨å¤šçº¿ç¨‹
    """
    
    def __init__(self, batch_size=30, small_batch_size=15, max_calls_per_minute=240):
        self.redis_client = None
        
        # å•Tokené…ç½®ï¼ˆå®é™…250æ¬¡/åˆ†é’Ÿï¼Œè®¾ç½®240æ¬¡ç•™ä½™é‡ï¼‰
        self.tushare_token = settings.TUSHARE_TOKEN
        
        # åˆå§‹åŒ–Tushare API
        if self.tushare_token:
            ts.set_token(self.tushare_token)
            self.pro = ts.pro_api()
            logger.info(f"åˆå§‹åŒ–Tushare Token: {self.tushare_token[:20]}...")
            logger.info(f"âœ… Tokenå·²é…ç½®ï¼ˆæ¯åˆ†é’Ÿ240æ¬¡è¯·æ±‚ï¼Œçº¯å¼‚æ­¥IOæ¨¡å¼ï¼‰")
        else:
            self.pro = None
            logger.warning("æœªé…ç½®Tushare Token")
        
        # æ‰¹å¤„ç†å‚æ•°
        self.batch_size = batch_size  # å¸¸è§„æ‰¹å¤„ç†å¤§å°
        self.small_batch_size = small_batch_size  # å°æ‰¹é‡å¤„ç†å¤§å°
        
        # é¢‘ç‡é™åˆ¶å™¨ï¼ˆçº¯å¼‚æ­¥æ¨¡å¼ï¼‰
        self.rate_limiter = TushareRateLimiter(max_calls_per_minute=max_calls_per_minute)
        self.failed_stocks = []  # è®°å½•å¤±è´¥çš„è‚¡ç¥¨ï¼Œç”¨äºåç»­è¡¥å¿
        
        logger.info(f"ğŸ“Š æ•°æ®ç®¡ç†å™¨é…ç½®: æ¯åˆ†é’Ÿ{max_calls_per_minute}æ¬¡è°ƒç”¨ï¼Œçº¯å¼‚æ­¥IOæ¨¡å¼")
    
        
    async def initialize(self):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        if not self.redis_client:
            from app.core.redis_client import get_redis_client
            self.redis_client = await get_redis_client()
        return self.redis_client is not None

    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.redis_client:
            await self.redis_client.close()
            self.redis_client = None
            
    async def update_processing_parameters(self, batch_size=None, small_batch_size=None, max_calls_per_minute=None):
        """
        åŠ¨æ€æ›´æ–°å¤„ç†å‚æ•°
        
        å‚æ•°:
            batch_size: å¸¸è§„æ‰¹å¤„ç†å¤§å°
            small_batch_size: å°æ‰¹é‡å¤„ç†å¤§å°
            max_calls_per_minute: æ¯åˆ†é’Ÿæœ€å¤§APIè°ƒç”¨æ¬¡æ•°
        """
        if batch_size is not None:
            self.batch_size = batch_size
            logger.info(f"æ›´æ–°å¸¸è§„æ‰¹å¤„ç†å¤§å°ä¸º: {batch_size}")
            
        if small_batch_size is not None:
            self.small_batch_size = small_batch_size
            logger.info(f"æ›´æ–°å°æ‰¹é‡å¤„ç†å¤§å°ä¸º: {small_batch_size}")
            
        if max_calls_per_minute is not None:
            self.rate_limiter.max_calls_per_minute = max_calls_per_minute
            logger.info(f"æ›´æ–°æ¯åˆ†é’Ÿæœ€å¤§APIè°ƒç”¨æ¬¡æ•°ä¸º: {max_calls_per_minute}")
            
        return {
            "batch_size": self.batch_size,
            "small_batch_size": self.small_batch_size,
            "max_calls_per_minute": self.rate_limiter.max_calls_per_minute
        }

    async def get_processing_status(self) -> Dict:
        """
        è·å–å½“å‰å¤„ç†çŠ¶æ€
        
        è¿”å›:
            åŒ…å«å½“å‰å¤„ç†çŠ¶æ€çš„å­—å…¸
        """
        # è·å–APIè°ƒç”¨ç»Ÿè®¡
        current_minute_calls = len([t for t in self.rate_limiter.call_times if time_module.time() - t < 60])
        
        # è·å–æ•°æ®ç»Ÿè®¡
        stock_list_count = await self.get_stock_list_count()
        trend_data_count = await self.get_stock_trend_data_count()
        
        return {
            "processing_parameters": {
                "batch_size": self.batch_size,
                "small_batch_size": self.small_batch_size,
                "max_calls_per_minute": self.rate_limiter.max_calls_per_minute,
                "architecture": "çº¯å¼‚æ­¥IOæ¨¡å¼"
            },
            "api_status": {
                "current_minute_calls": current_minute_calls,
                "daily_limit_reached": self.rate_limiter.daily_limit_reached,
                "api_utilization_percentage": (current_minute_calls / self.rate_limiter.max_calls_per_minute) * 100 if self.rate_limiter.max_calls_per_minute > 0 else 0
            },
            "data_status": {
                "stock_list_count": stock_list_count,
                "trend_data_count": trend_data_count,
                "trend_data_coverage_percentage": (trend_data_count / stock_list_count) * 100 if stock_list_count > 0 else 0
            },
            "timestamp": datetime.now().isoformat()
        }
    
    # ===================== è‚¡ç¥¨æ¸…å•ç®¡ç† =====================
    
    async def get_stock_list_count(self) -> int:
        """è·å–è‚¡ç¥¨æ¸…å•æ•°é‡ï¼ˆå…¼å®¹æ—§ç³»ç»Ÿæ ¼å¼ï¼‰"""
        try:
            # ä¼˜å…ˆä½¿ç”¨æ–°æ ¼å¼
            count = await self.redis_client.hlen("stock_list")
            if count > 0:
                return count
            
            # å…¼å®¹æ—§æ ¼å¼
            old_format_stocks = await self.redis_client.get("stocks:codes:all")
            if old_format_stocks:
                import json
                stocks_data = json.loads(old_format_stocks)
                if isinstance(stocks_data, list):
                    return len(stocks_data)
                    
            return 0
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨æ¸…å•æ•°é‡å¤±è´¥: {e}")
            return 0
    
    async def initialize_stock_list(self) -> bool:
        """åˆå§‹åŒ–è‚¡ç¥¨æ¸…å•"""
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–è‚¡ç¥¨æ¸…å•...")
            
            # è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            stock_list = await self._fetch_stock_basic_info()
            
            if not stock_list:
                logger.error("è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥")
                return False
            
            # å­˜å‚¨åˆ°Redisï¼ˆåŒæ—¶æ”¯æŒæ–°æ—§æ ¼å¼ï¼‰
            pipe = self.redis_client.pipeline()
            pipe.delete("stock_list")  # æ¸…ç©ºç°æœ‰æ•°æ®
            
            # æ–°æ ¼å¼ï¼šHashå­˜å‚¨
            for stock in stock_list:
                stock_key = stock['ts_code']
                stock_data = {
                    'ts_code': stock['ts_code'],
                    'symbol': stock['symbol'],
                    'name': stock['name'],
                    'area': stock.get('area', ''),
                    'industry': stock.get('industry', ''),
                    'market': stock.get('market', ''),
                    'list_date': stock.get('list_date', ''),
                    'updated_at': datetime.now().isoformat()
                }
                pipe.hset("stock_list", stock_key, json.dumps(stock_data))
            
            # æ—§æ ¼å¼ï¼šå…¼å®¹æ€§å­˜å‚¨ï¼ˆä¸ºå…¶ä»–æœåŠ¡æä¾›æ”¯æŒï¼‰
            old_format_list = []
            for stock in stock_list:
                old_format_stock = {
                    'ts_code': stock['ts_code'],
                    'symbol': stock['symbol'],
                    'name': stock['name'],
                    'area': stock.get('area', ''),
                    'industry': stock.get('industry', ''),
                    'market': stock.get('market', '')
                }
                old_format_list.append(old_format_stock)
            
            # å­˜å‚¨æ—§æ ¼å¼æ•°æ®
            pipe.set("stocks:codes:all", json.dumps(old_format_list))
            
            await pipe.execute()
            
            count = len(stock_list)
            logger.info(f"è‚¡ç¥¨æ¸…å•åˆå§‹åŒ–å®Œæˆï¼Œå…±{count}åªè‚¡ç¥¨")
            logger.info(f"åŒæ—¶å­˜å‚¨äº†æ–°æ ¼å¼(stock_list)å’Œæ—§æ ¼å¼(stocks:codes:all)ä»¥ç¡®ä¿å…¼å®¹æ€§")
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–è‚¡ç¥¨æ¸…å•å¤±è´¥: {e}")
            return False
    
    async def _fetch_stock_basic_info(self) -> List[Dict]:
        """è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
        try:
            # ä½¿ç”¨tushareè·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            if self.pro:
                try:
                    df = self.pro.stock_basic(exchange='', list_status='L', fields='ts_code,symbol,name,area,industry,market,list_date')
                    return df.to_dict('records')
                except Exception as e:
                    logger.warning(f"tushareè·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            
            logger.error("æœªé…ç½®Tushare APIï¼Œæ— æ³•è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯")
            return []
            
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    async def initialize_etf_list(self, clear_existing: bool = True) -> bool:
        """
        åˆå§‹åŒ– ETF æ¸…å•
        
        Args:
            clear_existing: æ˜¯å¦æ¸…ç©ºç°æœ‰çš„ ETF æ•°æ®ï¼ˆé»˜è®¤ Trueï¼‰
        """
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ– ETF æ¸…å•...")
            
            # ç›´æ¥ä»é…ç½®æ–‡ä»¶è·å– ETF åˆ—è¡¨ï¼ˆ121ä¸ªç²¾é€‰ETFï¼‰
            from app.core.etf_config import get_etf_list
            etf_list = get_etf_list()
            
            if not etf_list:
                logger.error("è·å– ETF åŸºæœ¬ä¿¡æ¯å¤±è´¥")
                return False
            
            logger.info(f"ä»é…ç½®æ–‡ä»¶è·å–åˆ° {len(etf_list)} ä¸ª ETF")
            
            # æ¸…ç©ºç°æœ‰çš„ ETF æ•°æ®
            if clear_existing:
                logger.info("æ¸…ç©ºç°æœ‰çš„ ETF æ•°æ®...")
                
                # 1. ä» stock_list ä¸­åˆ é™¤æ‰€æœ‰ ETF
                all_keys = await self.redis_client.hkeys("stock_list")
                etf_keys_to_delete = []
                
                for key in all_keys:
                    stock_data = await self.redis_client.hget("stock_list", key)
                    if stock_data:
                        try:
                            stock_info = json.loads(stock_data)
                            if stock_info.get('market') == 'ETF':
                                etf_keys_to_delete.append(key)
                        except:
                            pass
                
                if etf_keys_to_delete:
                    pipe = self.redis_client.pipeline()
                    for key in etf_keys_to_delete:
                        pipe.hdel("stock_list", key)
                    await pipe.execute()
                    logger.info(f"å·²ä» stock_list åˆ é™¤ {len(etf_keys_to_delete)} ä¸ªæ—§ ETF")
                
                # 2. åˆ é™¤ ETF Kçº¿æ•°æ®ï¼ˆETFä½¿ç”¨etf_trend:å‰ç¼€ï¼‰
                deleted_kline_count = 0
                for key in etf_keys_to_delete:
                    kline_key = f"etf_trend:{key}"
                    if await self.redis_client.delete(kline_key):
                        deleted_kline_count += 1
                
                if deleted_kline_count > 0:
                    logger.info(f"å·²åˆ é™¤ {deleted_kline_count} ä¸ª ETF çš„ Kçº¿æ•°æ®")
                
                # 3. åˆ é™¤ä¸“é—¨çš„ ETF åˆ—è¡¨
                await self.redis_client.delete("etf:list:all")
                logger.info("å·²æ¸…ç©º ETF ä¸“ç”¨åˆ—è¡¨")
            
            # å­˜å‚¨æ–°çš„ ETF æ•°æ®åˆ° Redis
            pipe = self.redis_client.pipeline()
            
            # æ–°æ ¼å¼ï¼šHash å­˜å‚¨åˆ° stock_listï¼ˆETF å’Œè‚¡ç¥¨æ··åˆå­˜å‚¨ï¼‰
            for etf in etf_list:
                etf_key = etf['ts_code']
                etf_data = {
                    'ts_code': etf['ts_code'],
                    'symbol': etf['symbol'],
                    'name': etf['name'],
                    'area': etf.get('area', ''),
                    'industry': etf.get('industry', 'T+0äº¤æ˜“'),  # T+0äº¤æ˜“ æˆ– T+1äº¤æ˜“
                    'market': etf.get('market', 'ETF'),  # è™šæ‹Ÿçš„ ETF æ¿å—
                    'list_date': etf.get('list_date', ''),
                    'updated_at': datetime.now().isoformat()
                }
                pipe.hset("stock_list", etf_key, json.dumps(etf_data))
            
            # åŒæ—¶å­˜å‚¨åˆ°ä¸“é—¨çš„ ETF åˆ—è¡¨ï¼ˆæ–¹ä¾¿å•ç‹¬æŸ¥è¯¢ï¼‰
            pipe.set("etf:list:all", json.dumps(etf_list))
            
            await pipe.execute()
            
            count = len(etf_list)
            logger.info(f"âœ… ETF æ¸…å•åˆå§‹åŒ–å®Œæˆï¼Œå…± {count} ä¸ªå¯äº¤æ˜“ ETF")
            logger.info(f"ETF å·²å­˜å‚¨åˆ° stock_listï¼ˆä¸è‚¡ç¥¨æ··åˆï¼‰å’Œ etf:list:allï¼ˆå•ç‹¬åˆ—è¡¨ï¼‰")
            
            # æ›´æ–° stocks:codes:all åŒ…å« ETF
            await self._update_stocks_codes_all()
            
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ– ETF æ¸…å•å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
    
    async def initialize_combined_list(self) -> bool:
        """åˆå§‹åŒ–è‚¡ç¥¨å’Œ ETF çš„ç»„åˆæ¸…å•"""
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–è‚¡ç¥¨å’Œ ETF ç»„åˆæ¸…å•...")
            
            # å…ˆåˆå§‹åŒ–è‚¡ç¥¨
            stock_success = await self.initialize_stock_list()
            if not stock_success:
                logger.error("è‚¡ç¥¨æ¸…å•åˆå§‹åŒ–å¤±è´¥")
                return False
            
            # å†åˆå§‹åŒ– ETF
            etf_success = await self.initialize_etf_list()
            if not etf_success:
                logger.warning("ETF æ¸…å•åˆå§‹åŒ–å¤±è´¥ï¼Œä½†è‚¡ç¥¨æ¸…å•å·²æˆåŠŸ")
                return True  # è‚¡ç¥¨æˆåŠŸå°±ç®—éƒ¨åˆ†æˆåŠŸ
            
            # æ›´æ–° stocks:codes:all åŒ…å«è‚¡ç¥¨å’Œ ETF
            await self._update_stocks_codes_all()
            
            logger.info("âœ… è‚¡ç¥¨å’Œ ETF ç»„åˆæ¸…å•åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–ç»„åˆæ¸…å•å¤±è´¥: {e}")
            return False
    
    async def _update_stocks_codes_all(self):
        """
        æ›´æ–° stocks:codes:allï¼ŒåŒ…å«è‚¡ç¥¨å’Œ ETF
        ç”¨äºå…¼å®¹æ—§çš„ APIï¼ˆå¦‚ chart APIï¼‰
        """
        try:
            # ä» stock_list è¯»å–æ‰€æœ‰æ•°æ®ï¼ˆåŒ…æ‹¬è‚¡ç¥¨å’Œ ETFï¼‰
            all_data = await self.redis_client.hgetall("stock_list")
            
            combined_list = []
            for ts_code, data_str in all_data.items():
                try:
                    stock_data = json.loads(data_str)
                    combined_list.append({
                        'ts_code': stock_data['ts_code'],
                        'symbol': stock_data['symbol'],
                        'name': stock_data['name'],
                        'area': stock_data.get('area', ''),
                        'industry': stock_data.get('industry', ''),
                        'market': stock_data.get('market', '')
                    })
                except:
                    pass
            
            # æ›´æ–° stocks:codes:all
            await self.redis_client.set("stocks:codes:all", json.dumps(combined_list))
            logger.info(f"âœ… å·²æ›´æ–° stocks:codes:allï¼ŒåŒ…å« {len(combined_list)} ä¸ªæ ‡çš„ï¼ˆè‚¡ç¥¨+ETFï¼‰")
            
        except Exception as e:
            logger.error(f"æ›´æ–° stocks:codes:all å¤±è´¥: {e}")
    
    async def check_stock_list_status(self) -> Tuple[bool, int]:
        """æ£€æŸ¥è‚¡ç¥¨æ¸…å•çŠ¶æ€"""
        count = await self.get_stock_list_count()
        is_sufficient = count >= 5000
        
        logger.info(f"è‚¡ç¥¨æ¸…å•æ£€æŸ¥ç»“æœ: {count}åªè‚¡ç¥¨, {'å……è¶³' if is_sufficient else 'ä¸è¶³'}")
        return is_sufficient, count
    
    # ===================== è‚¡ç¥¨èµ°åŠ¿æ•°æ®ç®¡ç† =====================
    
    async def get_stock_trend_data_count(self) -> int:
        """è·å–æœ‰èµ°åŠ¿æ•°æ®çš„è‚¡ç¥¨æ•°é‡"""
        try:
            # ç¡®ä¿æœ‰æ´»è·ƒçš„è¿æ¥
            if not self.redis_client:
                await self.initialize()
            
            # æ‰«ææ‰€æœ‰è‚¡ç¥¨èµ°åŠ¿æ•°æ®key
            keys = []
            async for key in self.redis_client.scan_iter(match="stock_trend:*"):
                keys.append(key)
            return len(keys)
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨èµ°åŠ¿æ•°æ®æ•°é‡å¤±è´¥: {e}")
            return 0
    
    async def initialize_all_stock_trend_data(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰è‚¡ç¥¨èµ°åŠ¿æ•°æ® - ç®€åŒ–ä¸ºå•çº¿ç¨‹ä¸²è¡Œå¤„ç†"""
        try:
            logger.info("=" * 70)
            logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ‰€æœ‰è‚¡ç¥¨èµ°åŠ¿æ•°æ®...")
            logger.info("=" * 70)
            
            # è®¾ç½®åˆå§‹åŒ–çŠ¶æ€
            await self.redis_client.set("stock_data_init_status", "æ­£åœ¨åˆå§‹åŒ–è‚¡ç¥¨æ•°æ®...")
            
            # è·å–è‚¡ç¥¨æ¸…å•
            stock_list = await self._get_all_stocks()
            if not stock_list:
                logger.error("âŒ è·å–è‚¡ç¥¨æ¸…å•å¤±è´¥")
                return False
            
            total_count = len(stock_list)
            logger.info(f"ğŸ“Š å…±éœ€è¦åˆå§‹åŒ– {total_count} åªè‚¡ç¥¨çš„èµ°åŠ¿æ•°æ®")
            logger.info(f"ğŸ“ˆ æ¯åªè‚¡ç¥¨è·å–180å¤©Kçº¿æ•°æ®ï¼ˆæ»¡è¶³EMA169éœ€æ±‚ï¼‰")
            logger.info(f"âš¡ APIé…ç½®: å•Token, æ¯åˆ†é’Ÿ{self.rate_limiter.max_calls_per_minute}æ¬¡è°ƒç”¨")
            logger.info(f"ğŸ”„ å¤„ç†æ¨¡å¼: å•çº¿ç¨‹ä¸²è¡Œï¼ˆç®€å•å¯é ï¼‰")
            
            start_time = datetime.now()
            success_count = 0
            failed_count = 0
            
            # ä¸²è¡Œå¤„ç†æ‰€æœ‰è‚¡ç¥¨
            for i, stock in enumerate(stock_list, 1):
                ts_code = stock.get('ts_code')
                stock_name = stock.get('name', ts_code)
                
                try:
                    # æ¯100åªè‚¡ç¥¨æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    if i % 100 == 0 or i == 1:
                        elapsed = (datetime.now() - start_time).total_seconds()
                        speed = i / elapsed * 60 if elapsed > 0 else 0
                        remaining = (total_count - i) / speed if speed > 0 else 0
                        logger.info(f"ğŸ“ è¿›åº¦: {i}/{total_count} ({i/total_count*100:.1f}%) | "
                                  f"æˆåŠŸ: {success_count} | å¤±è´¥: {failed_count} | "
                                  f"é€Ÿåº¦: {speed:.1f}åª/åˆ†é’Ÿ | é¢„è®¡å‰©ä½™: {remaining:.1f}åˆ†é’Ÿ")
                    
                    # è·å–180å¤©æ•°æ®
                    success = await self._fetch_with_tushare(ts_code, 180)
                    
                    if success:
                        success_count += 1
                        if i % 50 == 0:  # æ¯50åªè¯¦ç»†è®°å½•ä¸€æ¬¡
                            logger.debug(f"âœ… [{i}/{total_count}] {stock_name}({ts_code}) - æˆåŠŸ")
                    else:
                        failed_count += 1
                        logger.warning(f"âŒ [{i}/{total_count}] {stock_name}({ts_code}) - å¤±è´¥")
                    
                except Exception as e:
                    failed_count += 1
                    logger.error(f"âŒ [{i}/{total_count}] {stock_name}({ts_code}) - å¼‚å¸¸: {e}")
            
            # æœ€ç»ˆç»Ÿè®¡
            total_elapsed = (datetime.now() - start_time).total_seconds()
            success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
            avg_speed = total_count / total_elapsed * 60 if total_elapsed > 0 else 0
            
            # éªŒè¯å®é™…å­˜å‚¨çš„æ•°æ®
            actual_count = await self.get_stock_trend_data_count()
            
            logger.info("=" * 70)
            logger.info("âœ¨ è‚¡ç¥¨èµ°åŠ¿æ•°æ®åˆå§‹åŒ–å®Œæˆ!")
            logger.info("=" * 70)
            logger.info("ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
            logger.info(f"  â€¢ æ€»è‚¡ç¥¨æ•°é‡: {total_count}")
            logger.info(f"  â€¢ æˆåŠŸ: {success_count} åª ({success_rate:.1f}%)")
            logger.info(f"  â€¢ å¤±è´¥: {failed_count} åª")
            logger.info(f"  â€¢ å®é™…å­˜å‚¨: {actual_count} åª")
            logger.info(f"  â€¢ æ€»è€—æ—¶: {total_elapsed/60:.1f}åˆ†é’Ÿ")
            logger.info(f"  â€¢ å¹³å‡é€Ÿåº¦: {avg_speed:.1f}åª/åˆ†é’Ÿ")
            
            # APIä½¿ç”¨ç»Ÿè®¡
            if hasattr(self.rate_limiter, 'call_times'):
                tushare_calls = len(self.rate_limiter.call_times)
                logger.info("âš¡ APIä½¿ç”¨ç»Ÿè®¡:")
                logger.info(f"  â€¢ Tushareè°ƒç”¨æ¬¡æ•°: {tushare_calls}")
                logger.info(f"  â€¢ æ¯æ—¥é™åˆ¶çŠ¶æ€: {'å·²è¾¾ä¸Šé™' if self.rate_limiter.daily_limit_reached else 'æ­£å¸¸'}")
            
            logger.info("=" * 70)
            
            # æ›´æ–°åˆå§‹åŒ–çŠ¶æ€
            if success_rate >= 95:
                status_msg = f"âœ… åˆå§‹åŒ–å®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.1f}%"
            elif success_rate >= 80:
                status_msg = f"âš ï¸ åˆå§‹åŒ–åŸºæœ¬å®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.1f}%"
            else:
                status_msg = f"âŒ åˆå§‹åŒ–æœªå®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.1f}%"
            
            await self.redis_client.set("stock_data_init_status", status_msg)
            
            # å¦‚æœæˆåŠŸç‡ä½äº80%ï¼Œè®°å½•è­¦å‘Š
            if success_rate < 80:
                logger.warning(f"âš ï¸ æ•°æ®åˆå§‹åŒ–æˆåŠŸç‡è¾ƒä½ ({success_rate:.1f}%)ï¼Œè¯·æ£€æŸ¥:")
                logger.warning("  1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
                logger.warning("  2. Tushare Tokenæ˜¯å¦æœ‰æ•ˆ")
                logger.warning("  3. æ˜¯å¦è¾¾åˆ°APIæ¯æ—¥é™é¢")
            
            return success_rate >= 80  # è‡³å°‘80%æˆåŠŸæ‰ç®—åˆå§‹åŒ–æˆåŠŸ
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–æ‰€æœ‰è‚¡ç¥¨èµ°åŠ¿æ•°æ®å¤±è´¥: {e}")
            # è®¾ç½®å¤±è´¥çŠ¶æ€
            try:
                await self.redis_client.set("stock_data_init_status", f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            except:
                pass
            return False
    
    # å·²åˆ é™¤å¤æ‚çš„å¹¶è¡Œå¤„ç†å‡½æ•°ï¼Œé‡‡ç”¨ç®€å•çš„ä¸²è¡Œå¤„ç†
    
    async def _is_etf(self, ts_code: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸º ETF"""
        try:
            # ä» Redis è·å–è‚¡ç¥¨ä¿¡æ¯
            stock_data = await self.redis_client.hget("stock_list", ts_code)
            if stock_data:
                stock_info = json.loads(stock_data)
                return stock_info.get('market') == 'ETF'
            return False
        except:
            return False
    
    async def _fetch_with_tushare(self, ts_code: str, days: int) -> bool:
        """
        ä½¿ç”¨ tushare è·å–è‚¡ç¥¨/ETF æ•°æ® - çº¯å¼‚æ­¥IOæ¨¡å¼
        
        è‡ªåŠ¨è¯†åˆ« ETF å¹¶ä½¿ç”¨æ­£ç¡®çš„æ¥å£ï¼š
        - è‚¡ç¥¨ï¼šä½¿ç”¨ daily æ¥å£
        - ETFï¼šä½¿ç”¨ fund_daily æ¥å£
        """
        try:
            # æ£€æŸ¥å¹¶ç­‰å¾…APIè°ƒç”¨é™åˆ¶
            await self.rate_limiter.wait_for_rate_limit()
            
            # è®°å½•APIè°ƒç”¨
            self.rate_limiter._record_call()
            
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=days * 2)).strftime('%Y%m%d')
            
            # åˆ¤æ–­æ˜¯å¦ä¸º ETF
            is_etf = await self._is_etf(ts_code)
            
            # ä½¿ç”¨å¯¹åº”çš„ Tushare API
            if is_etf:
                # ETF ä½¿ç”¨ fund_daily æ¥å£
                logger.debug(f"ä½¿ç”¨ fund_daily æ¥å£è·å– ETF {ts_code} æ•°æ®")
                df = self.pro.fund_daily(ts_code=ts_code, start_date=start_date, end_date=end_date)
            else:
                # è‚¡ç¥¨ä½¿ç”¨ daily æ¥å£
                df = self.pro.daily(ts_code=ts_code, start_date=start_date, end_date=end_date)
            
            if not df.empty:
                df = df.sort_values('trade_date').tail(days)
                source = 'tushare_fund' if is_etf else 'tushare'
                return await self._store_stock_data(ts_code, df, source)
            return False
            
        except Exception as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¯æ—¥é™åˆ¶é”™è¯¯
            if "æ¯æ—¥è°ƒç”¨é‡è¶…é™" in str(e) or "daily calling limit" in str(e).lower():
                self.rate_limiter.handle_daily_limit_error(ts_code, days)
            raise e
    
    # Akshareç›¸å…³å‡½æ•°å·²åˆ é™¤ï¼Œä»…ä½¿ç”¨Tushare
    
    async def _store_stock_data(self, ts_code: str, df: pd.DataFrame, source: str) -> bool:
        """å­˜å‚¨è‚¡ç¥¨æ•°æ®åˆ°Redis"""
        try:
            # è½¬æ¢DataFrameï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å¯ä»¥JSONåºåˆ—åŒ–
            data_records = df.to_dict('records')
            
            # å¤„ç†æ—¥æœŸç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            for record in data_records:
                for key, value in record.items():
                    if pd.isna(value):
                        record[key] = None
                    elif hasattr(value, 'strftime'):  # å¤„ç†æ—¥æœŸç±»å‹
                        record[key] = value.strftime('%Y-%m-%d') if hasattr(value, 'date') else str(value)
                    elif isinstance(value, (pd.Timestamp, datetime)):
                        record[key] = value.strftime('%Y-%m-%d')
            
            trend_data = {
                'ts_code': ts_code,
                'data': data_records,
                'updated_at': datetime.now().isoformat(),
                'data_count': len(df),
                'source': source
            }
            
            # æ ¹æ®sourceåˆ¤æ–­æ˜¯å¦ä¸ºETFï¼Œä½¿ç”¨ä¸åŒçš„Redis key
            if source == 'tushare_fund':
                key = f"etf_trend:{ts_code}"
            else:
                key = f"stock_trend:{ts_code}"
            
            await self.redis_client.set(key, json.dumps(trend_data, default=str))
            return True
            
        except Exception as e:
            logger.error(f"å­˜å‚¨è‚¡ç¥¨ {ts_code} æ•°æ®å¤±è´¥: {e}")
            return False
    
    async def update_stock_trend_data(self, ts_code: str, days: int = 180) -> bool:
        """æ›´æ–°å•åªè‚¡ç¥¨çš„èµ°åŠ¿æ•°æ®ï¼ˆé»˜è®¤180å¤©ä»¥æ”¯æŒEMA169ï¼‰"""
        try:
            # è·å–å†å²æ•°æ®
            df = await self._fetch_stock_history(ts_code, days)
            if df is None or df.empty:
                logger.debug(f"è‚¡ç¥¨ {ts_code} æ— æ³•è·å–å†å²æ•°æ®")
                return False
            
            # ä½¿ç”¨æ ‡å‡†å­˜å‚¨æ–¹æ³•ï¼ˆå·²ä¿®å¤JSONåºåˆ—åŒ–é—®é¢˜ï¼‰
            success = await self._store_stock_data(ts_code, df, 'manual_update')
            
            if success:
                logger.debug(f"è‚¡ç¥¨ {ts_code} èµ°åŠ¿æ•°æ®æ›´æ–°æˆåŠŸï¼Œè·å– {len(df)} æ¡è®°å½•")
                return True
            else:
                logger.debug(f"è‚¡ç¥¨ {ts_code} æ•°æ®å­˜å‚¨å¤±è´¥")
                return False
            
        except Exception as e:
            logger.debug(f"æ›´æ–°è‚¡ç¥¨ {ts_code} èµ°åŠ¿æ•°æ®å¤±è´¥: {e}")
            return False
    
    async def _fetch_stock_history(self, ts_code: str, days: int = 180) -> Optional[pd.DataFrame]:
        """è·å–è‚¡ç¥¨/ETFå†å²æ•°æ®ï¼ˆæ”¯æŒé¢‘ç‡æ§åˆ¶å’Œå¤±è´¥é‡è¯•ï¼Œé»˜è®¤180å¤©ä»¥æ”¯æŒEMA169ï¼‰"""
        try:
            # è®¡ç®—å¼€å§‹æ—¥æœŸ
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=days * 2)).strftime('%Y%m%d')
            
            # æ£€æŸ¥æ¯æ—¥é™åˆ¶æ˜¯å¦é‡ç½®
            self.rate_limiter.check_daily_limit_reset()
            
            # ä½¿ç”¨tushareè·å–æ•°æ®
            if self.pro and not self.rate_limiter.daily_limit_reached:
                try:
                    # æ£€æŸ¥å¹¶ç­‰å¾…APIè°ƒç”¨é™åˆ¶
                    await self.rate_limiter.wait_for_rate_limit()
                    
                    # è®°å½•APIè°ƒç”¨
                    self.rate_limiter._record_call()
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸º ETF
                    is_etf = await self._is_etf(ts_code)
                    
                    # ä½¿ç”¨å¯¹åº”çš„æ¥å£
                    if is_etf:
                        logger.debug(f"ä½¿ç”¨ fund_daily æ¥å£è·å– ETF {ts_code} æ•°æ®...")
                        df = self.pro.fund_daily(ts_code=ts_code, start_date=start_date, end_date=end_date)
                    else:
                        logger.debug(f"ä½¿ç”¨ daily æ¥å£è·å–è‚¡ç¥¨ {ts_code} æ•°æ®...")
                        df = self.pro.daily(ts_code=ts_code, start_date=start_date, end_date=end_date)
                    if not df.empty:
                        df = df.sort_values('trade_date').tail(days)
                        # æ·»åŠ å®é™…äº¤æ˜“æ—¥æœŸå­—æ®µ
                        df['actual_trade_date'] = pd.to_datetime(df['trade_date'].astype(str))
                        logger.debug(f"tushareè·å– {ts_code} æˆåŠŸï¼Œ{len(df)}æ¡æ•°æ®")
                        return df
                    else:
                        logger.debug(f"tushareè·å– {ts_code} è¿”å›ç©ºæ•°æ®")
                except Exception as e:
                    logger.warning(f"tushareè·å– {ts_code} æ•°æ®å¤±è´¥: {e}")
                    error_msg = str(e)
                    if ("æ¯åˆ†é’Ÿæœ€å¤šè®¿é—®" in error_msg or "500æ¬¡" in error_msg or 
                        "æ¯å¤©æœ€å¤šè®¿é—®" in error_msg or "20000æ¬¡" in error_msg):
                        
                        # å¦‚æœæ˜¯æ¯æ—¥é™åˆ¶é”™è¯¯ï¼Œç‰¹æ®Šå¤„ç†
                        if "æ¯å¤©æœ€å¤šè®¿é—®" in error_msg:
                            self.rate_limiter.handle_daily_limit_error(ts_code, days)
                        else:
                            # åˆ†é’Ÿé™åˆ¶é”™è¯¯ - ä¸è·³è¿‡ï¼Œè€Œæ˜¯æš‚åœç­‰å¾…
                            logger.info(f"{ts_code} è§¦å‘åˆ†é’Ÿé™åˆ¶ï¼Œç­‰å¾…æ¢å¤...")
                            await self.rate_limiter.wait_for_rate_limit()
                            # é‡è¯•ä¸€æ¬¡
                            return await self._fetch_stock_history(ts_code, days)
            elif self.rate_limiter.daily_limit_reached:
                logger.debug(f"tushareæ¯æ—¥é™åˆ¶å·²è¾¾ä¸Šé™ï¼Œæ— æ³•è·å– {ts_code} æ•°æ®")
            elif not hasattr(self, 'pro') or not self.tushare_token:
                logger.debug(f"tushareæœªé…ç½®ï¼Œæ— æ³•è·å– {ts_code} æ•°æ®")
            
            # å¦‚æœtushareå¤±è´¥ï¼Œè¿”å›None
            logger.warning(f"è·å– {ts_code} æ•°æ®å¤±è´¥")
            return None
            
        except Exception as e:
            logger.error(f"è·å– {ts_code} å†å²æ•°æ®å¼‚å¸¸: {e}")
            return None
    
    async def _get_all_stocks(self) -> List[Dict]:
        """è·å–æ‰€æœ‰è‚¡ç¥¨åˆ—è¡¨ï¼ˆå…¼å®¹æ—§ç³»ç»Ÿæ ¼å¼ï¼‰- ä½¿ç”¨åŒæ­¥Redisé¿å…äº‹ä»¶å¾ªç¯å†²çª"""
        try:
            # ä½¿ç”¨åŒæ­¥Rediså®¢æˆ·ç«¯ï¼Œé¿å…åœ¨ä¸åŒäº‹ä»¶å¾ªç¯ä¸­è°ƒç”¨å¼‚æ­¥Redis
            from app.core.sync_redis_client import get_sync_redis_client
            sync_redis = get_sync_redis_client()
            
            # ä¼˜å…ˆä½¿ç”¨æ–°æ ¼å¼
            stocks = sync_redis.hgetall("stock_list")
            if stocks:
                result = []
                for key, data in stocks.items():
                    try:
                        # ç¡®ä¿dataæ˜¯å­—ç¬¦ä¸²å†è§£æ
                        if isinstance(data, bytes):
                            data = data.decode('utf-8')
                        if isinstance(data, str):
                            stock_dict = json.loads(data)
                            # ç¡®ä¿è§£æåæ˜¯å­—å…¸
                            if isinstance(stock_dict, dict):
                                result.append(stock_dict)
                            else:
                                logger.warning(f"è‚¡ç¥¨æ•°æ®æ ¼å¼é”™è¯¯ {key}: {type(stock_dict)}, æ•°æ®: {str(stock_dict)[:200]}")
                        else:
                            logger.warning(f"è‚¡ç¥¨æ•°æ®ä¸æ˜¯å­—ç¬¦ä¸² {key}: {type(data)}, æ•°æ®: {str(data)[:200]}")
                    except json.JSONDecodeError as e:
                        logger.warning(f"è§£æè‚¡ç¥¨æ•°æ®JSONå¤±è´¥ {key}: {e}, åŸå§‹æ•°æ®: {str(data)[:200]}")
                        continue
                    except Exception as e:
                        logger.warning(f"è§£æè‚¡ç¥¨æ•°æ®å¤±è´¥ {key}: {e}")
                        continue
                
                logger.info(f"ä» stock_list è·å–åˆ° {len(result)} æ¡æœ‰æ•ˆæ•°æ®ï¼ˆæ€»å…± {len(stocks)} æ¡ï¼‰")
                return result
            
            # å…¼å®¹æ—§æ ¼å¼
            old_format_stocks = sync_redis.get("stocks:codes:all")
            if old_format_stocks:
                # åŒæ­¥Redisè¿”å›çš„æ˜¯å­—ç¬¦ä¸²æˆ–bytesï¼Œéœ€è¦è§£æ
                if isinstance(old_format_stocks, bytes):
                    old_format_stocks = old_format_stocks.decode('utf-8')
                if isinstance(old_format_stocks, str):
                    stocks_data = json.loads(old_format_stocks)
                else:
                    stocks_data = old_format_stocks
                    
                if isinstance(stocks_data, list):
                    # ç¡®ä¿åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯å­—å…¸
                    return [item for item in stocks_data if isinstance(item, dict)]
                    
            return []
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    async def check_stock_trend_data_status(self) -> Tuple[bool, int]:
        """æ£€æŸ¥è‚¡ç¥¨èµ°åŠ¿æ•°æ®çŠ¶æ€ï¼ˆç®€æ´ç‰ˆ - åªæ£€æŸ¥æ•°é‡ï¼‰"""
        count = await self.get_stock_trend_data_count()
        
        # ç®€å•æ£€æŸ¥æ•°é‡æ˜¯å¦å……è¶³ï¼ˆ5000åªè‚¡ç¥¨ä½œä¸ºå……è¶³æ ‡å‡†ï¼‰
        is_sufficient = count >= 5000
        
        logger.info(f"è‚¡ç¥¨èµ°åŠ¿æ•°æ®æ£€æŸ¥ç»“æœ: {count}åªè‚¡ç¥¨æœ‰æ•°æ®, {'æ•°é‡å……è¶³' if is_sufficient else 'æ•°é‡ä¸è¶³'}")
        return is_sufficient, count
    
    # ===================== æ™ºèƒ½æ›´æ–°æœºåˆ¶ =====================
    
    async def is_force_update_day(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¼ºåˆ¶æ›´æ–°æ—¥ï¼ˆå‘¨å…­ï¼‰"""
        return datetime.now().weekday() == 5  # å‘¨å…­
    
    async def smart_update_trend_data(self) -> Tuple[int, int]:
        """ç®€åŒ–çš„è‚¡ç¥¨èµ°åŠ¿æ•°æ®æ›´æ–°"""
        try:
            logger.info("å¼€å§‹æ›´æ–°è‚¡ç¥¨èµ°åŠ¿æ•°æ®...")
            
            # è·å–æ‰€æœ‰è‚¡ç¥¨
            stock_list = await self._get_all_stocks()
            if not stock_list:
                logger.error("è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥")
                return 0, 0
            
            total_count = len(stock_list)
            logger.info(f"å°†æ›´æ–° {total_count} åªè‚¡ç¥¨çš„æ•°æ®")
            
            # è°ƒç”¨å·²æœ‰çš„åˆå§‹åŒ–å‡½æ•°
            success = await self.initialize_all_stock_trend_data()
            
            if success:
                final_count = await self.get_stock_trend_data_count()
                logger.info(f"è‚¡ç¥¨æ•°æ®æ›´æ–°å®Œæˆ: {final_count} åªè‚¡ç¥¨")
                return final_count, 0
            else:
                logger.error("è‚¡ç¥¨æ•°æ®æ›´æ–°å¤±è´¥")
                return 0, total_count
                
        except Exception as e:
            logger.error(f"è‚¡ç¥¨æ•°æ®æ›´æ–°å¼‚å¸¸: {e}")
            return 0, len(stock_list) if 'stock_list' in locals() else 0
    
    # ===================== å¯åŠ¨æ£€æŸ¥ =====================
    
    async def startup_check(self) -> Dict[str, any]:
        """å¯åŠ¨æ—¶æ•°æ®æ£€æŸ¥"""
        logger.info("å¼€å§‹å¯åŠ¨æ•°æ®æ£€æŸ¥...")
        
        result = {
            'stock_list_check': False,
            'stock_list_count': 0,
            'stock_list_initialized': False,
            'trend_data_check': False,
            'trend_data_count': 0,
            'trend_data_initialized': False,
            'buy_signals_check': False,
            'buy_signals_count': 0,
            'buy_signals_initialized': False,
            'success': False
        }
        
        try:
            # æ£€æŸ¥1: è‚¡ç¥¨æ¸…å•
            list_sufficient, list_count = await self.check_stock_list_status()
            result['stock_list_check'] = list_sufficient
            result['stock_list_count'] = list_count
            
            if not list_sufficient:
                logger.info("è‚¡ç¥¨æ¸…å•ä¸è¶³ï¼Œå¼€å§‹åˆå§‹åŒ–...")
                list_init_success = await self.initialize_stock_list()
                result['stock_list_initialized'] = list_init_success
                
                if list_init_success:
                    _, result['stock_list_count'] = await self.check_stock_list_status()
            
            # æ£€æŸ¥2: è‚¡ç¥¨èµ°åŠ¿æ•°æ®
            trend_sufficient, trend_count = await self.check_stock_trend_data_status()
            result['trend_data_check'] = trend_sufficient
            result['trend_data_count'] = trend_count
            
            if not trend_sufficient:
                logger.info("è‚¡ç¥¨èµ°åŠ¿æ•°æ®ä¸è¶³ï¼Œå¼€å§‹åˆå§‹åŒ–...")
                trend_init_success = await self.initialize_all_stock_trend_data()
                result['trend_data_initialized'] = trend_init_success
                
                if trend_init_success:
                    _, result['trend_data_count'] = await self.check_stock_trend_data_status()
            else:
                # å³ä½¿æ•°æ®å……è¶³ï¼Œä¹Ÿè¦æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„è‚¡ç¥¨éœ€è¦è¡¥å¿
                logger.info("è‚¡ç¥¨èµ°åŠ¿æ•°æ®å……è¶³ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é—æ¼è‚¡ç¥¨éœ€è¦è¡¥å¿...")
                missing_count = await self._check_and_compensate_missing_stocks()
                if missing_count > 0:
                    logger.info(f"å·²è¡¥å¿ {missing_count} åªé—æ¼çš„è‚¡ç¥¨")
                    _, result['trend_data_count'] = await self.check_stock_trend_data_status()
            
            # æ£€æŸ¥3: ä¹°å…¥ä¿¡å·ï¼ˆéœ€è¦ä¾èµ–å‰é¢çš„æ•°æ®ï¼‰
            if result['stock_list_count'] > 0 and result['trend_data_count'] > 0:
                from app.services.signal.signal_manager import signal_manager
                await signal_manager.initialize()
                
                try:
                    signals_sufficient, signals_count = await signal_manager.check_buy_signals_status()
                    result['buy_signals_check'] = signals_sufficient
                    result['buy_signals_count'] = signals_count
                    
                    if not signals_sufficient:
                        logger.info("ä¹°å…¥ä¿¡å·ä¸è¶³ï¼Œå¼€å§‹åˆå§‹åŒ–...")
                        signals_init_success = await signal_manager.initialize_all_buy_signals()
                        result['buy_signals_initialized'] = signals_init_success
                        
                        if signals_init_success:
                            _, result['buy_signals_count'] = await signal_manager.check_buy_signals_status()
                finally:
                    await signal_manager.close()
            else:
                logger.warning("è‚¡ç¥¨æ¸…å•æˆ–èµ°åŠ¿æ•°æ®ä¸è¶³ï¼Œè·³è¿‡ä¹°å…¥ä¿¡å·æ£€æŸ¥")
            
            result['success'] = True
            logger.info("å¯åŠ¨æ•°æ®æ£€æŸ¥å®Œæˆ")
            
        except Exception as e:
            logger.error(f"å¯åŠ¨æ•°æ®æ£€æŸ¥å¤±è´¥: {e}")
            result['error'] = str(e)
        
        return result
    
    async def _check_and_compensate_missing_stocks(self) -> int:
        """æ£€æŸ¥å¹¶è¡¥å¿é—æ¼çš„è‚¡ç¥¨æ•°æ®"""
        try:
            # è·å–æ‰€æœ‰è‚¡ç¥¨æ¸…å•
            all_stocks = await self._get_all_stocks()
            if not all_stocks:
                logger.warning("æ— æ³•è·å–è‚¡ç¥¨æ¸…å•ï¼Œè·³è¿‡é—æ¼æ£€æŸ¥")
                return 0
            
            # æ£€æŸ¥å“ªäº›è‚¡ç¥¨æ²¡æœ‰èµ°åŠ¿æ•°æ®
            missing_stocks = []
            
            for stock in all_stocks:
                ts_code = stock['ts_code']
                market = stock.get('market', '')
                
                # æ ¹æ®marketå­—æ®µåˆ¤æ–­ä½¿ç”¨å“ªä¸ªRedis key
                if market == 'ETF':
                    key = f"etf_trend:{ts_code}"
                else:
                    key = f"stock_trend:{ts_code}"
                
                # æ£€æŸ¥Redisä¸­æ˜¯å¦å­˜åœ¨è¯¥è‚¡ç¥¨çš„æ•°æ®
                exists = await self.redis_client.exists(key)
                if not exists:
                    missing_stocks.append(stock)
            
            if not missing_stocks:
                logger.info("æ‰€æœ‰è‚¡ç¥¨éƒ½æœ‰èµ°åŠ¿æ•°æ®ï¼Œæ— éœ€è¡¥å¿")
                return 0
            
            logger.info(f"å‘ç° {len(missing_stocks)} åªè‚¡ç¥¨ç¼ºå°‘èµ°åŠ¿æ•°æ®ï¼Œå¼€å§‹è¡¥å¿...")
            
            # å¦‚æœé—æ¼è‚¡ç¥¨æ•°é‡è¾ƒå°‘ï¼Œä½¿ç”¨ä¸²è¡Œè¡¥å¿
            if len(missing_stocks) <= 100:
                logger.info("é—æ¼è‚¡ç¥¨æ•°é‡è¾ƒå°‘ï¼Œä½¿ç”¨ä¸²è¡Œè¡¥å¿æ¨¡å¼")
                compensated = await self._serial_compensate_missing_stocks(missing_stocks)
            else:
                logger.info("é—æ¼è‚¡ç¥¨æ•°é‡è¾ƒå¤šï¼Œä½¿ç”¨æ··åˆè¡¥å¿æ¨¡å¼")
                # å…ˆå°è¯•å°æ‰¹é‡å¹¶è¡Œï¼Œå¤±è´¥çš„å†ä¸²è¡Œè¡¥å¿
                compensated = await self._hybrid_compensate_missing_stocks(missing_stocks)
            
            logger.info(f"é—æ¼è‚¡ç¥¨è¡¥å¿å®Œæˆ: æˆåŠŸ {compensated}/{len(missing_stocks)} åª")
            return compensated
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥å’Œè¡¥å¿é—æ¼è‚¡ç¥¨å¤±è´¥: {e}")
            return 0
    
    async def _serial_compensate_missing_stocks(self, missing_stocks: List[Dict]) -> int:
        """ä¸²è¡Œè¡¥å¿é—æ¼çš„è‚¡ç¥¨"""
        compensated = 0
        total = len(missing_stocks)
        
        logger.info(f"å¼€å§‹ä¸²è¡Œè¡¥å¿ {total} åªé—æ¼è‚¡ç¥¨...")
        
        for i, stock in enumerate(missing_stocks, 1):
            ts_code = stock['ts_code']
            stock_name = stock.get('name', '')
            
            try:
                logger.debug(f"[{i}/{total}] è¡¥å¿: {ts_code} ({stock_name})")
                
                # ä½¿ç”¨ç»¼åˆè·å–æ–¹æ³•
                success = await self._comprehensive_fetch_single_stock(ts_code)
                
                if success:
                    compensated += 1
                    logger.debug(f"[{i}/{total}] è¡¥å¿æˆåŠŸ: {ts_code}")
                else:
                    logger.debug(f"[{i}/{total}] è¡¥å¿å¤±è´¥: {ts_code}")
                
                # æ§åˆ¶è¡¥å¿é€Ÿåº¦ï¼Œé¿å…è¿‡äºé¢‘ç¹
                await asyncio.sleep(0.3)
                
                # æ¯20åªè‚¡ç¥¨æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
                if i % 20 == 0:
                    progress = (i / total) * 100
                    success_rate = (compensated / i) * 100
                    logger.info(f"ä¸²è¡Œè¡¥å¿è¿›åº¦: {i}/{total} ({progress:.1f}%), æˆåŠŸç‡: {success_rate:.1f}%")
                
            except Exception as e:
                logger.debug(f"[{i}/{total}] è¡¥å¿å¼‚å¸¸: {ts_code} - {e}")
        
        return compensated
    
    async def _hybrid_compensate_missing_stocks(self, missing_stocks: List[Dict]) -> int:
        """æ··åˆè¡¥å¿é—æ¼çš„è‚¡ç¥¨ - å…ˆå°æ‰¹é‡å¹¶è¡Œ(ä½¿ç”¨çº¿ç¨‹æ§åˆ¶)ï¼Œå¤±è´¥çš„å†ä¸²è¡Œ"""
        total = len(missing_stocks)
        logger.info(f"å¼€å§‹æ··åˆè¡¥å¿ {total} åªé—æ¼è‚¡ç¥¨...")
        
        # ç¬¬ä¸€æ­¥ï¼šå°æ‰¹é‡å¹¶è¡Œå¤„ç†(ä½¿ç”¨çº¿ç¨‹æ§åˆ¶)
        logger.info("ç¬¬ä¸€æ­¥: å°æ‰¹é‡å¹¶è¡Œå¤„ç†(ä½¿ç”¨çº¿ç¨‹æ§åˆ¶)...")
        parallel_success, parallel_failed = await self._small_batch_parallel_compensate(missing_stocks)
        
        # ç¬¬äºŒæ­¥ï¼šä¸²è¡Œå¤„ç†å¤±è´¥çš„è‚¡ç¥¨
        serial_success = 0
        if parallel_failed:
            logger.info(f"ç¬¬äºŒæ­¥: ä¸²è¡Œå¤„ç† {len(parallel_failed)} åªå¤±è´¥è‚¡ç¥¨...")
            serial_success = await self._serial_compensate_missing_stocks(parallel_failed)
        
        total_success = parallel_success + serial_success
        logger.info(f"æ··åˆè¡¥å¿å®Œæˆ: å¹¶è¡ŒæˆåŠŸ {parallel_success}, ä¸²è¡ŒæˆåŠŸ {serial_success}, æ€»æˆåŠŸ {total_success}")
        
        return total_success
    
    async def _small_batch_parallel_compensate(self, missing_stocks: List[Dict]) -> Tuple[int, List[Dict]]:
        """å°æ‰¹é‡è¡¥å¿ - å¼‚æ­¥ä¸²è¡Œå¤„ç†"""
        success_count = 0
        failed_stocks = []
        
        # ä½¿ç”¨ç±»ä¸­å®šä¹‰çš„å°æ‰¹é‡å¤§å°
        batch_size = self.small_batch_size
        total = len(missing_stocks)
        
        for i in range(0, total, batch_size):
            batch = missing_stocks[i:i + batch_size]
            
            logger.info(f"å°æ‰¹é‡å¤„ç† ç¬¬ {i//batch_size + 1} æ‰¹ ({i+1}-{min(i + batch_size, total)}/{total})")
            
            # å¼‚æ­¥ä¸²è¡Œå¤„ç†
            batch_results = []
            for stock in batch:
                try:
                    result = await self.update_stock_trend_data(stock['ts_code'])
                    batch_results.append(result)
                except Exception as e:
                    logger.error(f"å°æ‰¹é‡å¤„ç†è‚¡ç¥¨ {stock['ts_code']} å¼‚å¸¸: {e}")
                    batch_results.append(False)
            
            # ç»Ÿè®¡ç»“æœ
            for idx, result in enumerate(batch_results):
                stock = batch[idx]
                if isinstance(result, bool) and result:
                    success_count += 1
                else:
                    failed_stocks.append(stock)
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            await asyncio.sleep(1.0)
        
        return success_count, failed_stocks

# åˆ›å»ºè‚¡ç¥¨æ•°æ®ç®¡ç†å™¨å·¥å‚å‡½æ•°ï¼Œé¿å…å•ä¾‹é—®é¢˜
def create_stock_data_manager(batch_size=10, small_batch_size=5, max_calls_per_minute=50):
    """
    åˆ›å»ºæ–°çš„è‚¡ç¥¨æ•°æ®ç®¡ç†å™¨å®ä¾‹
    
    å‚æ•°:
        batch_size: å¸¸è§„æ‰¹å¤„ç†å¤§å°
        small_batch_size: å°æ‰¹é‡å¤„ç†å¤§å°
        max_calls_per_minute: æ¯åˆ†é’Ÿæœ€å¤§APIè°ƒç”¨æ¬¡æ•°
    """
    return StockDataManager(
        batch_size=batch_size,
        small_batch_size=small_batch_size,
        max_calls_per_minute=max_calls_per_minute
    )

# å…¨å±€å®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰
stock_data_manager = StockDataManager() 