# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨æ•°æ®åŸå­æœåŠ¡
æä¾›DDDé£æ ¼çš„åŸå­èƒ½åŠ›æ–¹æ³•ï¼Œä¾¿äºç»´æŠ¤å’Œç»„ç»‡
"""

import asyncio
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd

from app.core.logging import logger
from app.core.config import settings
from app.core.etf_config import get_etf_list
from app.core.invalid_stock_codes import filter_valid_stocks
from app.db.session import RedisCache


class StockAtomicService:
    """è‚¡ç¥¨æ•°æ®åŸå­æœåŠ¡ç±»"""
    
    def __init__(self):
        self.redis_cache = RedisCache()
        self.stock_keys = {
            'stock_codes': 'stocks:codes:all',
            'stock_kline': 'stock_trend:{}',
        }
    
    # ==================== 1.1 è·å–æœ‰æ•ˆè‚¡ç¥¨ä»£ç åˆ—è¡¨æ–¹æ³• ====================
    
    async def get_valid_stock_codes(self, include_etf: bool = True) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰æœ‰æ•ˆçš„è‚¡ç¥¨å’ŒETFä»£ç åˆ—è¡¨ï¼ˆç»Ÿä¸€å°è£…æ–¹æ³•ï¼‰
        
        åŠŸèƒ½ï¼š
        1. è·å–Aè‚¡è‚¡ç¥¨ä»£ç ï¼ˆæ²ªæ·±åŒ—ä¸‰å¸‚åœºï¼‰
        2. è·å–ETFä»£ç 
        3. è‡ªåŠ¨è¿‡æ»¤æ— æ•ˆä»£ç ï¼ˆåŒ—äº¤æ‰€åºŸå¼ƒä»£ç ã€é€€å¸‚è‚¡ç¥¨ç­‰ï¼‰
        4. ç»Ÿä¸€æ±‡æ€»æ—¥å¿—è¾“å‡º
        
        Args:
            include_etf: æ˜¯å¦åŒ…å«ETFï¼Œé»˜è®¤Trueï¼ˆå°†ETFä½œä¸ºç‰¹æ®Šè‚¡ç¥¨å¤„ç†ï¼‰
            
        Returns:
            æœ‰æ•ˆè‚¡ç¥¨åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«: ts_code, symbol, name, area, industry, market, list_date
        """
        logger.info("=" * 80)
        logger.info("å¼€å§‹è·å–æ‰€æœ‰æœ‰æ•ˆè‚¡ç¥¨å’ŒETFä»£ç ...")
        logger.info("=" * 80)
        
        start_time = datetime.now()
        
        try:
            # 1. è·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨
            logger.info("æ­¥éª¤1: è·å–Aè‚¡è‚¡ç¥¨ä»£ç ï¼ˆæ²ªæ·±åŒ—ä¸‰å¸‚åœºï¼‰...")
            stock_list = await self._fetch_a_stock_list()
            stock_count = len(stock_list)
            logger.info(f"âœ“ è·å–åˆ°Aè‚¡è‚¡ç¥¨ä»£ç : {stock_count} åª")
            
            # ç»Ÿè®¡å„å¸‚åœºè‚¡ç¥¨æ•°é‡
            sh_count = sum(1 for s in stock_list if s.get('ts_code', '').endswith('.SH'))
            sz_count = sum(1 for s in stock_list if s.get('ts_code', '').endswith('.SZ'))
            bj_count = sum(1 for s in stock_list if s.get('ts_code', '').endswith('.BJ'))
            logger.info(f"  - ä¸Šæµ·å¸‚åœº(SH): {sh_count} åª")
            logger.info(f"  - æ·±åœ³å¸‚åœº(SZ): {sz_count} åª")
            logger.info(f"  - åŒ—äº¬å¸‚åœº(BJ): {bj_count} åª")
            
            # 2. å¦‚æœåŒ…å«ETFï¼Œæ·»åŠ ETFåˆ—è¡¨
            etf_count = 0
            etf_sh_count = 0
            etf_sz_count = 0
            if include_etf:
                logger.info("æ­¥éª¤2: è·å–ETFä»£ç ...")
                etf_list = get_etf_list()
                etf_count = len(etf_list)
                logger.info(f"âœ“ è·å–åˆ°ETFä»£ç : {etf_count} åª")
                
                # ç»Ÿè®¡ETFå¸‚åœºåˆ†å¸ƒ
                etf_sh_count = sum(1 for e in etf_list if e.get('ts_code', '').endswith('.SH'))
                etf_sz_count = sum(1 for e in etf_list if e.get('ts_code', '').endswith('.SZ'))
                logger.info(f"  - ä¸Šæµ·ETF(SH): {etf_sh_count} åª")
                logger.info(f"  - æ·±åœ³ETF(SZ): {etf_sz_count} åª")
                
                stock_list.extend(etf_list)
            else:
                logger.info("æ­¥éª¤2: è·³è¿‡ETFä»£ç è·å–")
            
            # 3. åˆå¹¶æ‰€æœ‰ä»£ç 
            logger.info("æ­¥éª¤3: åˆå¹¶è‚¡ç¥¨å’ŒETFä»£ç ...")
            total_before_filter = len(stock_list)
            logger.info(f"âœ“ åˆå¹¶åæ€»ä»£ç æ•°: {total_before_filter} åª")
            
            # 4. è¿‡æ»¤æ— æ•ˆè‚¡ç¥¨ä»£ç ï¼ˆåŒ…æ‹¬åŒ—äº¤æ‰€åºŸå¼ƒä»£ç ï¼‰
            logger.info("æ­¥éª¤4: è¿‡æ»¤æ— æ•ˆä»£ç ...")
            from app.core.invalid_stock_codes import filter_valid_stocks, get_invalid_codes_summary
            
            # æ˜¾ç¤ºæ— æ•ˆä»£ç é…ç½®ç»Ÿè®¡
            invalid_summary = get_invalid_codes_summary()
            logger.info(f"  - æ— æ•ˆä»£ç é…ç½®:")
            logger.info(f"    Â· åŒ—äº¤æ‰€åºŸå¼ƒä»£ç : {invalid_summary['bj_codes']} åª")
            logger.info(f"    Â· é€€å¸‚è‚¡ç¥¨ä»£ç : {invalid_summary['delist_codes']} åª")
            logger.info(f"    Â· æš‚åœä¸Šå¸‚ä»£ç : {invalid_summary['suspend_codes']} åª")
            logger.info(f"    Â· æ€»è®¡: {invalid_summary['total']} åª")
            
            valid_stock_list = filter_valid_stocks(stock_list)
            filtered_count = total_before_filter - len(valid_stock_list)
            
            if filtered_count > 0:
                logger.warning(f"âœ— å®é™…è¿‡æ»¤æ‰: {filtered_count} åªæ— æ•ˆä»£ç ")
                # ç»Ÿè®¡è¢«è¿‡æ»¤çš„ä»£ç ç±»å‹
                filtered_codes = [s for s in stock_list if s not in valid_stock_list]
                filtered_stocks = [s for s in filtered_codes if s.get('market') != 'ETF']
                filtered_etfs = [s for s in filtered_codes if s.get('market') == 'ETF']
                if filtered_stocks:
                    logger.warning(f"  - è¢«è¿‡æ»¤çš„è‚¡ç¥¨: {len(filtered_stocks)} åª")
                if filtered_etfs:
                    logger.warning(f"  - è¢«è¿‡æ»¤çš„ETF: {len(filtered_etfs)} åª")
            else:
                logger.info(f"âœ“ æœªå‘ç°éœ€è¦è¿‡æ»¤çš„æ— æ•ˆä»£ç ")
            
            # 5. ç»Ÿè®¡æœ€ç»ˆç»“æœ
            final_stock_count = sum(1 for s in valid_stock_list if s.get('market') != 'ETF')
            final_etf_count = sum(1 for s in valid_stock_list if s.get('market') == 'ETF')
            
            logger.info("æ­¥éª¤5: å­˜å‚¨åˆ°Redis...")
            self.redis_cache.set_cache(
                self.stock_keys['stock_codes'],
                valid_stock_list,
                ttl=None  # æ°¸ä¹…ä¿å­˜
            )
            logger.info(f"âœ“ å·²å­˜å‚¨åˆ°Redis (æ°¸ä¹…ä¿å­˜)")
            
            # 6. è¾“å‡ºç»Ÿä¸€æ±‡æ€»æ—¥å¿—
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info("=" * 80)
            logger.info("ğŸ“Š è·å–è‚¡ç¥¨ä»£ç å®Œæˆ - æ±‡æ€»ç»Ÿè®¡")
            logger.info("=" * 80)
            logger.info(f"æ€»è€—æ—¶: {elapsed:.2f} ç§’")
            logger.info(f"")
            logger.info(f"ã€åŸå§‹æ•°æ®ã€‘")
            logger.info(f"  Aè‚¡è‚¡ç¥¨: {stock_count} åª (SH:{sh_count}, SZ:{sz_count}, BJ:{bj_count})")
            if include_etf:
                logger.info(f"  ETFåŸºé‡‘: {etf_count} åª (SH:{etf_sh_count}, SZ:{etf_sz_count})")
            logger.info(f"  åˆè®¡: {total_before_filter} åª")
            logger.info(f"")
            logger.info(f"ã€è¿‡æ»¤ç»“æœã€‘")
            logger.info(f"  æ— æ•ˆä»£ç é…ç½®: {invalid_summary['total']} åª")
            logger.info(f"  å®é™…è¿‡æ»¤: {filtered_count} åª")
            logger.info(f"")
            logger.info(f"ã€æœ€ç»ˆç»“æœã€‘")
            logger.info(f"  æœ‰æ•ˆè‚¡ç¥¨: {final_stock_count} åª")
            if include_etf:
                logger.info(f"  æœ‰æ•ˆETF: {final_etf_count} åª")
            logger.info(f"  æ€»è®¡: {len(valid_stock_list)} åª âœ“")
            logger.info("=" * 80)
            
            return valid_stock_list
            
        except Exception as e:
            logger.error("=" * 80)
            logger.error(f"âœ— è·å–æœ‰æ•ˆè‚¡ç¥¨ä»£ç å¤±è´¥: {e}")
            logger.error("=" * 80)
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    async def _fetch_a_stock_list(self) -> List[Dict[str, Any]]:
        """
        ä»Tushareè·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨
        
        Returns:
            Aè‚¡è‚¡ç¥¨åˆ—è¡¨
        """
        try:
            import tushare as ts
            
            # åˆå§‹åŒ–Tushare
            ts.set_token(settings.TUSHARE_TOKEN)
            pro = ts.pro_api()
            
            # è·å–è‚¡ç¥¨åˆ—è¡¨
            df = pro.stock_basic(
                exchange='',
                list_status='L',  # L=ä¸Šå¸‚ D=é€€å¸‚ P=æš‚åœä¸Šå¸‚
                fields='ts_code,symbol,name,area,industry,market,list_date'
            )
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            stock_list = df.to_dict('records')
            
            return stock_list
            
        except Exception as e:
            logger.error(f"ä»Tushareè·å–Aè‚¡åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    # ==================== 1.2 å…¨é‡æ›´æ–°æ‰€æœ‰è‚¡ç¥¨æ–¹æ³• ====================
    
    async def full_update_all_stocks(
        self,
        days: int = 180,
        batch_size: int = 50,
        max_concurrent: int = 10
    ) -> Dict[str, Any]:
        """
        å…¨é‡æ›´æ–°æ‰€æœ‰è‚¡ç¥¨çš„å†å²Kçº¿æ•°æ®ï¼ˆåŒ…æ‹¬ETFå’Œè‚¡ç¥¨ï¼‰
        æ¸…ç©ºå¹¶é‡æ–°è·å–æ‰€æœ‰è‚¡ç¥¨çš„å†å²Kçº¿æ•°æ®
        
        Args:
            days: è·å–å¤©æ•°ï¼Œé»˜è®¤180å¤©
            batch_size: æ¯æ‰¹å¤„ç†çš„è‚¡ç¥¨æ•°é‡
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            æ›´æ–°ç»“æœç»Ÿè®¡
        """
        logger.info(f"å¼€å§‹å…¨é‡æ›´æ–°æ‰€æœ‰è‚¡ç¥¨Kçº¿æ•°æ®ï¼Œå¤©æ•°={days}å¤©...")
        start_time = datetime.now()
        
        try:
            # 1. è·å–æœ‰æ•ˆè‚¡ç¥¨åˆ—è¡¨
            stock_list = self.redis_cache.get_cache(self.stock_keys['stock_codes'])
            if not stock_list:
                logger.warning("è‚¡ç¥¨ä»£ç åˆ—è¡¨ä¸ºç©ºï¼Œå…ˆè·å–è‚¡ç¥¨ä»£ç ")
                stock_list = await self.get_valid_stock_codes(include_etf=True)
            
            total_count = len(stock_list)
            logger.info(f"éœ€è¦æ›´æ–° {total_count} åªè‚¡ç¥¨çš„Kçº¿æ•°æ®")
            
            # 2. æ¸…ç©ºæ‰€æœ‰Kçº¿æ•°æ®
            await self._clear_all_kline_data(stock_list)
            
            # 3. æ‰¹é‡è·å–Kçº¿æ•°æ®
            result = await self._batch_fetch_kline_data(
                stock_list,
                days=days,
                batch_size=batch_size,
                max_concurrent=max_concurrent
            )
            
            # 4. å¤±è´¥è¡¥å¿ï¼šå¯¹å¤±è´¥çš„è‚¡ç¥¨é‡è¯•ä¸€æ¬¡
            if result['failed_count'] > 0 and result.get('failed_stocks'):
                logger.warning(f"æ£€æµ‹åˆ° {result['failed_count']} åªè‚¡ç¥¨è·å–å¤±è´¥ï¼Œå¼€å§‹è¡¥å¿é‡è¯•...")
                compensation_result = await self._compensate_failed_stocks(
                    result['failed_stocks'],
                    days=days,
                    max_concurrent=max_concurrent
                )
                
                # æ›´æ–°ç»Ÿè®¡
                result['success_count'] += compensation_result['success_count']
                result['failed_count'] = compensation_result['failed_count']
                result['compensation_attempted'] = compensation_result['total_count']
                result['compensation_success'] = compensation_result['success_count']
                
                logger.info(
                    f"è¡¥å¿å®Œæˆ: é‡è¯• {compensation_result['total_count']} åª, "
                    f"æˆåŠŸ {compensation_result['success_count']} åª, "
                    f"æœ€ç»ˆå¤±è´¥ {compensation_result['failed_count']} åª"
                )
            
            elapsed = (datetime.now() - start_time).total_seconds()
            result['elapsed_seconds'] = round(elapsed, 2)
            result['elapsed_minutes'] = round(elapsed / 60, 2)
            
            logger.info(
                f"å…¨é‡æ›´æ–°å®Œæˆ: æ€»è®¡={result['total_count']}, "
                f"æˆåŠŸ={result['success_count']}, "
                f"å¤±è´¥={result['failed_count']}, "
                f"æˆåŠŸç‡={result['success_rate']:.2f}%, "
                f"è€—æ—¶={result['elapsed_minutes']:.2f}åˆ†é’Ÿ"
            )
            
            return result
            
        except Exception as e:
            logger.error(f"å…¨é‡æ›´æ–°æ‰€æœ‰è‚¡ç¥¨å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success_count': 0,
                'failed_count': 0,
                'total_count': 0,
                'error': str(e)
            }
    
    async def _clear_all_kline_data(self, stock_list: List[Dict[str, Any]]):
        """æ¸…ç©ºæ‰€æœ‰è‚¡ç¥¨çš„Kçº¿æ•°æ®"""
        logger.info("å¼€å§‹æ¸…ç©ºæ‰€æœ‰è‚¡ç¥¨Kçº¿æ•°æ®...")
        cleared_count = 0
        
        for stock in stock_list:
            ts_code = stock.get('ts_code')
            if ts_code:
                key = self.stock_keys['stock_kline'].format(ts_code)
                self.redis_cache.delete_cache(key)
                cleared_count += 1
        
        logger.info(f"æ¸…ç©ºKçº¿æ•°æ®å®Œæˆï¼Œå…±æ¸…ç©º {cleared_count} åªè‚¡ç¥¨")
    
    async def _batch_fetch_kline_data(
        self,
        stock_list: List[Dict[str, Any]],
        days: int = 180,
        batch_size: int = 50,
        max_concurrent: int = 10
    ) -> Dict[str, Any]:
        """æ‰¹é‡è·å–Kçº¿æ•°æ®"""
        total_count = len(stock_list)
        success_count = 0
        failed_count = 0
        failed_stocks = []  # è®°å½•å¤±è´¥çš„è‚¡ç¥¨
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, total_count, batch_size):
            batch = stock_list[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total_count + batch_size - 1) // batch_size
            
            logger.info(f"å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ï¼Œå…± {len(batch)} åªè‚¡ç¥¨")
            
            # å¹¶å‘è·å–
            semaphore = asyncio.Semaphore(max_concurrent)
            tasks = [
                self._fetch_single_stock_kline(stock, days, semaphore)
                for stock in batch
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç»Ÿè®¡ç»“æœå¹¶è®°å½•å¤±è´¥çš„è‚¡ç¥¨
            batch_success = 0
            batch_failed = 0
            for idx, result in enumerate(results):
                if isinstance(result, Exception):
                    failed_count += 1
                    batch_failed += 1
                    failed_stocks.append(batch[idx])
                elif result:
                    success_count += 1
                    batch_success += 1
                else:
                    failed_count += 1
                    batch_failed += 1
                    failed_stocks.append(batch[idx])
            
            # è¾“å‡ºæ‰¹æ¬¡æ±‡æ€»æ—¥å¿—
            logger.info(f"ç¬¬ {batch_num} æ‰¹å®Œæˆ: æˆåŠŸ {batch_success}/{len(batch)}, å¤±è´¥ {batch_failed}/{len(batch)}, ç´¯è®¡æˆåŠŸ {success_count}/{total_count}")
            
            # é¿å…é¢‘ç¹è¯·æ±‚
            await asyncio.sleep(0.5)
        
        return {
            'total_count': total_count,
            'success_count': success_count,
            'failed_count': failed_count,
            'failed_stocks': failed_stocks,  # è¿”å›å¤±è´¥çš„è‚¡ç¥¨åˆ—è¡¨
            'success_rate': round(success_count / total_count * 100, 2) if total_count > 0 else 0
        }
    
    async def _fetch_single_stock_kline(
        self,
        stock: Dict[str, Any],
        days: int,
        semaphore: asyncio.Semaphore
    ) -> bool:
        """è·å–å•åªè‚¡ç¥¨çš„Kçº¿æ•°æ®"""
        async with semaphore:
            try:
                ts_code = stock.get('ts_code')
                if not ts_code:
                    return False
                
                # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒåŒæ­¥çš„Tushareè°ƒç”¨
                import concurrent.futures
                loop = asyncio.get_event_loop()
                
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    kline_data = await loop.run_in_executor(
                        executor,
                        self._sync_fetch_kline,
                        ts_code,
                        days
                    )
                
                if kline_data and len(kline_data) > 0:
                    # ç¼“å­˜åˆ°Redis
                    key = self.stock_keys['stock_kline'].format(ts_code)
                    cache_data = {
                        'data': kline_data,
                        'updated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'data_count': len(kline_data),
                        'source': 'tushare',
                        'last_update_type': 'full_update'
                    }
                    self.redis_cache.set_cache(key, cache_data, ttl=86400 * 30)  # 30å¤©
                    return True
                else:
                    # ä¸è¾“å‡ºæ¯æ¡å¤±è´¥æ—¥å¿—ï¼Œç”±æ‰¹æ¬¡æ±‡æ€»ç»Ÿè®¡
                    return False
                    
            except Exception as e:
                # ä¸è¾“å‡ºæ¯æ¡å¤±è´¥æ—¥å¿—ï¼Œç”±æ‰¹æ¬¡æ±‡æ€»ç»Ÿè®¡
                return False
    
    async def _compensate_failed_stocks(
        self,
        failed_stocks: List[Dict[str, Any]],
        days: int = 180,
        max_concurrent: int = 5
    ) -> Dict[str, Any]:
        """
        è¡¥å¿å¤±è´¥çš„è‚¡ç¥¨æ•°æ®è·å–
        
        Args:
            failed_stocks: å¤±è´¥çš„è‚¡ç¥¨åˆ—è¡¨
            days: è·å–å¤©æ•°
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆè¡¥å¿æ—¶ä½¿ç”¨è¾ƒå°çš„å¹¶å‘æ•°ï¼‰
            
        Returns:
            è¡¥å¿ç»“æœç»Ÿè®¡
        """
        total_count = len(failed_stocks)
        success_count = 0
        failed_count = 0
        
        logger.info(f"å¼€å§‹è¡¥å¿ {total_count} åªå¤±è´¥è‚¡ç¥¨...")
        
        # ç­‰å¾…5ç§’ï¼Œè®©APIé™åˆ¶æ¢å¤
        await asyncio.sleep(5)
        
        # å¹¶å‘é‡è¯•
        semaphore = asyncio.Semaphore(max_concurrent)
        tasks = [
            self._fetch_single_stock_kline(stock, days, semaphore)
            for stock in failed_stocks
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        for idx, result in enumerate(results):
            if isinstance(result, Exception):
                failed_count += 1
            elif result:
                success_count += 1
            else:
                failed_count += 1
                # è®°å½•æœ€ç»ˆå¤±è´¥çš„è‚¡ç¥¨ä»£ç 
                ts_code = failed_stocks[idx].get('ts_code', 'unknown')
                logger.warning(f"è¡¥å¿å¤±è´¥: {ts_code}")
        
        return {
            'total_count': total_count,
            'success_count': success_count,
            'failed_count': failed_count
        }
    
    def _sync_fetch_kline(self, ts_code: str, days: int) -> List[Dict[str, Any]]:
        """åŒæ­¥è·å–Kçº¿æ•°æ®ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€æ•°æ®æœåŠ¡
            from app.services.stock.unified_data_service import unified_data_service
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºETFï¼ˆä»£ç ä»¥5æˆ–1å¼€å¤´çš„6ä½æ•°å­—ï¼‰
            code = ts_code.split('.')[0]
            is_etf = len(code) == 6 and code[0] in ['5', '1']
            
            kline_data = unified_data_service.fetch_historical_data(
                ts_code=ts_code,
                days=days,
                is_etf=is_etf
            )
            
            return kline_data
            
        except Exception as e:
            logger.error(f"åŒæ­¥è·å–Kçº¿æ•°æ®å¤±è´¥ {ts_code}: {e}")
            return []
    
    # ==================== 1.3 å®æ—¶æ›´æ–°æ‰€æœ‰è‚¡ç¥¨æ•°æ®æ–¹æ³• ====================
    
    async def realtime_update_all_stocks(self) -> Dict[str, Any]:
        """
        å®æ—¶æ›´æ–°æ‰€æœ‰è‚¡ç¥¨æ•°æ®ï¼ˆåŒ…æ‹¬ETFï¼‰
        1. è·å–æ‰€æœ‰è‚¡ç¥¨å’ŒETFçš„å®æ—¶æ•°æ®
        2. æ›´æ–°åˆ°å†å²Kçº¿æ•°æ®ï¼ˆå½“æ—¥æœ‰åˆ™æ›´æ–°ï¼Œæ— åˆ™æ–°å¢ï¼‰
        
        Returns:
            æ›´æ–°ç»“æœç»Ÿè®¡
        """
        logger.info("å¼€å§‹å®æ—¶æ›´æ–°æ‰€æœ‰è‚¡ç¥¨æ•°æ®ï¼ˆåŒ…æ‹¬ETFï¼‰...")
        start_time = datetime.now()
        
        try:
            # 1. è·å–å®æ—¶æ•°æ®
            from app.services.stock.unified_data_service import unified_data_service
            
            realtime_result = await unified_data_service.async_fetch_all_realtime_data()
            
            if not realtime_result['success']:
                logger.error("è·å–å®æ—¶æ•°æ®å¤±è´¥")
                return {
                    'success': False,
                    'message': 'è·å–å®æ—¶æ•°æ®å¤±è´¥',
                    'stock_count': 0,
                    'etf_count': 0,
                    'total_count': 0
                }
            
            logger.info(
                f"æˆåŠŸè·å–å®æ—¶æ•°æ®: "
                f"è‚¡ç¥¨ {realtime_result['stock_count']} åª, "
                f"ETF {realtime_result['etf_count']} åª"
            )
            
            # 2. æ‰¹é‡æ›´æ–°Kçº¿æ•°æ®
            update_result = await unified_data_service.async_batch_update_klines_with_realtime(
                stock_df=realtime_result['stock_data'],
                etf_df=realtime_result['etf_data']
            )
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            result = {
                'success': True,
                'message': 'å®æ—¶æ›´æ–°å®Œæˆ',
                'stock_count': realtime_result['stock_count'],
                'etf_count': realtime_result['etf_count'],
                'total_count': realtime_result['total_count'],
                'stock_updated': update_result['stock_updated'],
                'stock_failed': update_result['stock_failed'],
                'etf_updated': update_result['etf_updated'],
                'etf_failed': update_result['etf_failed'],
                'total_updated': update_result['total_updated'],
                'total_failed': update_result['total_failed'],
                'elapsed_seconds': round(elapsed, 2),
                'update_time': realtime_result['update_time']
            }
            
            logger.info(
                f"å®æ—¶æ›´æ–°å®Œæˆ: "
                f"æˆåŠŸæ›´æ–° {result['total_updated']} åª, "
                f"å¤±è´¥ {result['total_failed']} åª, "
                f"è€—æ—¶ {elapsed:.2f}ç§’"
            )
            
            return result
            
        except Exception as e:
            logger.error(f"å®æ—¶æ›´æ–°æ‰€æœ‰è‚¡ç¥¨å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'message': f'å®æ—¶æ›´æ–°å¤±è´¥: {str(e)}',
                'error': str(e)
            }
    
    # ==================== 1.4 ç­–ç•¥ä¿¡å·è®¡ç®—æ–¹æ³• ====================
    
    async def calculate_strategy_signals(
        self,
        force_recalculate: bool = False
    ) -> Dict[str, Any]:
        """
        è®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„ç­–ç•¥ä¿¡å·
        
        Args:
            force_recalculate: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—
            
        Returns:
            è®¡ç®—ç»“æœç»Ÿè®¡
        """
        logger.info(f"å¼€å§‹è®¡ç®—ç­–ç•¥ä¿¡å·ï¼Œå¼ºåˆ¶é‡ç®—={force_recalculate}")
        start_time = datetime.now()
        
        try:
            # ä½¿ç”¨ç°æœ‰çš„signal_manager
            from app.services.signal.signal_manager import signal_manager
            
            # åˆå§‹åŒ–signal_manager
            await signal_manager.initialize()
            
            # è®¡ç®—ä¿¡å·
            result = await signal_manager.calculate_buy_signals(
                force_recalculate=force_recalculate
            )
            
            elapsed = (datetime.now() - start_time).total_seconds()
            result['elapsed_seconds'] = round(elapsed, 2)
            
            logger.info(f"ç­–ç•¥ä¿¡å·è®¡ç®—å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’")
            
            return result
            
        except Exception as e:
            logger.error(f"è®¡ç®—ç­–ç•¥ä¿¡å·å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'error': str(e)
            }
    
    # ==================== 1.5 æ–°é—»çˆ¬å–æ–¹æ³• ====================
    
    async def crawl_news(self, days: int = 1) -> Dict[str, Any]:
        """
        çˆ¬å–è´¢ç»æ–°é—»
        
        Args:
            days: çˆ¬å–å¤©æ•°
            
        Returns:
            çˆ¬å–ç»“æœç»Ÿè®¡
        """
        start_time = datetime.now()
        
        try:
            # ä½¿ç”¨ç°æœ‰çš„æ–°é—»æœåŠ¡
            from app.services.analysis.news_analysis_service import get_phoenix_finance_news
            
            # çˆ¬å–æ–°é—»
            news_list = get_phoenix_finance_news(days=days, skip_content=False, force_crawl=True)
            
            if not news_list or len(news_list) < 5:
                logger.warning(f"çˆ¬å–åˆ°çš„æ–°é—»æ•°æ®è´¨é‡ä¸ä½³ï¼Œæ•°é‡: {len(news_list)}")
                return {
                    'success': False,
                    'news_count': len(news_list),
                    'message': 'æ–°é—»æ•°æ®è´¨é‡ä¸ä½³'
                }
            
            # æ ¼å¼åŒ–å¹¶ç¼“å­˜
            formatted_news = []
            for news in news_list:
                formatted_news.append({
                    'title': news['title'],
                    'url': news['url'],
                    'datetime': news['datetime'],
                    'source': news['source'],
                    'summary': news.get('content', '')[:150] + '...' if news.get('content') and len(news.get('content')) > 150 else news.get('content', '')
                })
            
            # ç¼“å­˜åˆ°Redis
            cache_data = {
                'news': formatted_news,
                'count': len(formatted_news),
                'updated_at': start_time.strftime('%Y-%m-%d %H:%M:%S'),
                'data_source': 'phoenix_finance'
            }
            
            self.redis_cache.set_cache('news:latest', cache_data, ttl=7200)  # 2å°æ—¶
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ“ æ–°é—»çˆ¬å–å®Œæˆ: {len(formatted_news)}æ¡ï¼Œè€—æ—¶ {elapsed:.1f}ç§’")
            
            return {
                'success': True,
                'news_count': len(formatted_news),
                'elapsed_seconds': round(elapsed, 2)
            }
            
        except Exception as e:
            logger.error(f"çˆ¬å–æ–°é—»å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'news_count': 0,
                'error': str(e)
            }
    
    # ==================== 1.6 å›¾è¡¨æ–‡ä»¶æ¸…ç†æ–¹æ³• ====================
    
    async def cleanup_chart_files(self) -> Dict[str, Any]:
        """
        æ¸…ç†æ‰€æœ‰ç”Ÿæˆçš„å›¾è¡¨HTMLæ–‡ä»¶
        
        Returns:
            æ¸…ç†ç»“æœç»Ÿè®¡
        """
        logger.info("å¼€å§‹æ¸…ç†å›¾è¡¨æ–‡ä»¶...")
        start_time = datetime.now()
        
        try:
            import os
            import glob
            from app.core.config import CHART_DIR
            
            if not os.path.exists(CHART_DIR):
                logger.info("å›¾è¡¨ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†")
                return {
                    'success': True,
                    'deleted_count': 0,
                    'message': 'å›¾è¡¨ç›®å½•ä¸å­˜åœ¨'
                }
            
            # è·å–æ‰€æœ‰HTMLæ–‡ä»¶
            html_files = glob.glob(os.path.join(CHART_DIR, '*.html'))
            
            if not html_files:
                logger.info("æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„å›¾è¡¨æ–‡ä»¶")
                return {
                    'success': True,
                    'deleted_count': 0,
                    'message': 'æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ–‡ä»¶'
                }
            
            # åˆ é™¤æ‰€æœ‰HTMLæ–‡ä»¶
            deleted_count = 0
            for file_path in html_files:
                try:
                    os.remove(file_path)
                    deleted_count += 1
                except Exception as e:
                    logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f"å›¾è¡¨æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶ï¼Œè€—æ—¶ {elapsed:.2f}ç§’")
            
            return {
                'success': True,
                'deleted_count': deleted_count,
                'elapsed_seconds': round(elapsed, 2)
            }
            
        except Exception as e:
            logger.error(f"æ¸…ç†å›¾è¡¨æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'deleted_count': 0,
                'error': str(e)
            }


# å…¨å±€å•ä¾‹
stock_atomic_service = StockAtomicService()

